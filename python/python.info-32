This is python.info, produced by makeinfo version 6.0 from python.texi.

     Python 3.6.0a0, May 06, 2016

     Georg Brandl

     Copyright © 2001-2016, Python Software Foundation

INFO-DIR-SECTION Documentation tools
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY


   Generated by Sphinx 1.4.1.


File: python.info,  Node: Windows and Pads,  Next: Displaying Text,  Prev: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.3 Windows and Pads
-----------------------

Windows are the basic abstraction in curses.  A window object represents
a rectangular area of the screen, and supports methods to display text,
erase it, allow the user to input strings, and so forth.

The ‘stdscr’ object returned by the *note initscr(): 1abe. function is a
window object that covers the entire screen.  Many programs may need
only this single window, but you might wish to divide the screen into
smaller windows, in order to redraw or clear them separately.  The *note
newwin(): 1ad7. function creates a new window of a given size, returning
the new window object.

     begin_x = 20; begin_y = 7
     height = 5; width = 40
     win = curses.newwin(height, width, begin_y, begin_x)

Note that the coordinate system used in curses is unusual.  Coordinates
are always passed in the order `y,x', and the top-left corner of a
window is coordinate (0,0).  This breaks the normal convention for
handling coordinates where the `x' coordinate comes first.  This is an
unfortunate difference from most other computer applications, but it’s
been part of curses since it was first written, and it’s too late to
change things now.

Your application can determine the size of the screen by using the
‘curses.LINES’ and ‘curses.COLS’ variables to obtain the `y' and `x'
sizes.  Legal coordinates will then extend from ‘(0,0)’ to
‘(curses.LINES - 1, curses.COLS - 1)’.

When you call a method to display or erase text, the effect doesn’t
immediately show up on the display.  Instead you must call the *note
refresh(): 1b03. method of window objects to update the screen.

This is because curses was originally written with slow 300-baud
terminal connections in mind; with these terminals, minimizing the time
required to redraw the screen was very important.  Instead curses
accumulates changes to the screen and displays them in the most
efficient manner when you call ‘refresh()’.  For example, if your
program displays some text in a window and then clears the window,
there’s no need to send the original text because they’re never visible.

In practice, explicitly telling curses to redraw a window doesn’t really
complicate programming with curses much.  Most programs go into a flurry
of activity, and then pause waiting for a keypress or some other action
on the part of the user.  All you have to do is to be sure that the
screen has been redrawn before pausing to wait for user input, by first
calling ‘stdscr.refresh()’ or the ‘refresh()’ method of some other
relevant window.

A pad is a special case of a window; it can be larger than the actual
display screen, and only a portion of the pad displayed at a time.
Creating a pad requires the pad’s height and width, while refreshing a
pad requires giving the coordinates of the on-screen area where a
subsection of the pad will be displayed.

     pad = curses.newpad(100, 100)
     # These loops fill the pad with letters; addch() is
     # explained in the next section
     for y in range(0, 99):
         for x in range(0, 99):
             pad.addch(y,x, ord('a') + (x*x+y*y) % 26)

     # Displays a section of the pad in the middle of the screen.
     # (0,0) : coordinate of upper-left corner of pad area to display.
     # (5,5) : coordinate of upper-left corner of window area to be filled
     #         with pad content.
     # (20, 75) : coordinate of lower-right corner of window area to be
     #          : filled with pad content.
     pad.refresh( 0,0, 5,5, 20,75)

The ‘refresh()’ call displays a section of the pad in the rectangle
extending from coordinate (5,5) to coordinate (20,75) on the screen; the
upper left corner of the displayed section is coordinate (0,0) on the
pad.  Beyond that difference, pads are exactly like ordinary windows and
support the same methods.

If you have multiple windows and pads on screen there is a more
efficient way to update the screen and prevent annoying screen flicker
as each part of the screen gets updated.  ‘refresh()’ actually does two
things:

  1. Calls the *note noutrefresh(): 1b2a. method of each window to
     update an underlying data structure representing the desired state
     of the screen.

  2. Calls the function *note doupdate(): 1ab9. function to change the
     physical screen to match the desired state recorded in the data
     structure.

Instead you can call ‘noutrefresh()’ on a number of windows to update
the data structure, and then call ‘doupdate()’ to update the screen.


File: python.info,  Node: Displaying Text,  Next: User Input,  Prev: Windows and Pads,  Up: Curses Programming with Python

10.3.4 Displaying Text
----------------------

From a C programmer’s point of view, curses may sometimes look like a
twisty maze of functions, all subtly different.  For example, ‘addstr()’
displays a string at the current cursor location in the ‘stdscr’ window,
while ‘mvaddstr()’ moves to a given y,x coordinate first before
displaying the string.  ‘waddstr()’ is just like ‘addstr()’, but allows
specifying a window to use instead of using ‘stdscr’ by default.
‘mvwaddstr()’ allows specifying both a window and a coordinate.

Fortunately the Python interface hides all these details.  ‘stdscr’ is a
window object like any other, and methods such as *note addstr(): 1af7.
accept multiple argument forms.  Usually there are four different forms.

Form                                  Description
                                      
------------------------------------------------------------------------------------------
                                      
`str' or `ch'                         Display the string `str' or character `ch' at the
                                      current position
                                      
                                      
`str' or `ch', `attr'                 Display the string `str' or character `ch', using
                                      attribute `attr' at the current position
                                      
                                      
`y', `x', `str' or `ch'               Move to position `y,x' within the window, and
                                      display `str' or `ch'
                                      
                                      
`y', `x', `str' or `ch', `attr'       Move to position `y,x' within the window, and
                                      display `str' or `ch', using attribute `attr'
                                      

Attributes allow displaying text in highlighted forms such as boldface,
underline, reverse code, or in color.  They’ll be explained in more
detail in the next subsection.

The *note addstr(): 1af7. method takes a Python string or bytestring as
the value to be displayed.  The contents of bytestrings are sent to the
terminal as-is.  Strings are encoded to bytes using the value of the
window’s ‘encoding’ attribute; this defaults to the default system
encoding as returned by *note locale.getpreferredencoding(): fb0.

The *note addch(): 1af5. methods take a character, which can be either a
string of length 1, a bytestring of length 1, or an integer.

Constants are provided for extension characters; these constants are
integers greater than 255.  For example, ‘ACS_PLMINUS’ is a +/- symbol,
and ‘ACS_ULCORNER’ is the upper left corner of a box (handy for drawing
borders).  You can also use the appropriate Unicode character.

Windows remember where the cursor was left after the last operation, so
if you leave out the `y,x' coordinates, the string or character will be
displayed wherever the last operation left off.  You can also move the
cursor with the ‘move(y,x)’ method.  Because some terminals always
display a flashing cursor, you may want to ensure that the cursor is
positioned in some location where it won’t be distracting; it can be
confusing to have the cursor blinking at some apparently random
location.

If your application doesn’t need a blinking cursor at all, you can call
‘curs_set(False)’ to make it invisible.  For compatibility with older
curses versions, there’s a ‘leaveok(bool)’ function that’s a synonym for
*note curs_set(): 1ab3.  When `bool' is true, the curses library will
attempt to suppress the flashing cursor, and you won’t need to worry
about leaving it in odd locations.

* Menu:

* Attributes and Color:: 


File: python.info,  Node: Attributes and Color,  Up: Displaying Text

10.3.4.1 Attributes and Color
.............................

Characters can be displayed in different ways.  Status lines in a
text-based application are commonly shown in reverse video, or a text
viewer may need to highlight certain words.  curses supports this by
allowing you to specify an attribute for each cell on the screen.

An attribute is an integer, each bit representing a different attribute.
You can try to display text with multiple attribute bits set, but curses
doesn’t guarantee that all the possible combinations are available, or
that they’re all visually distinct.  That depends on the ability of the
terminal being used, so it’s safest to stick to the most commonly
available attributes, listed here.

Attribute                  Description
                           
----------------------------------------------------------------------
                           
‘A_BLINK’                  Blinking text
                           
                           
‘A_BOLD’                   Extra bright or bold text
                           
                           
‘A_DIM’                    Half bright text
                           
                           
‘A_REVERSE’                Reverse-video text
                           
                           
‘A_STANDOUT’               The best highlighting mode available
                           
                           
‘A_UNDERLINE’              Underlined text
                           

So, to display a reverse-video status line on the top line of the
screen, you could code:

     stdscr.addstr(0, 0, "Current mode: Typing mode",
                   curses.A_REVERSE)
     stdscr.refresh()

The curses library also supports color on those terminals that provide
it.  The most common such terminal is probably the Linux console,
followed by color xterms.

To use color, you must call the *note start_color(): 1ae5. function soon
after calling *note initscr(): 1abe, to initialize the default color set
(the *note curses.wrapper(): 1af2. function does this automatically).
Once that’s done, the *note has_colors(): 1ac4. function returns TRUE if
the terminal in use can actually display color.  (Note: curses uses the
American spelling ’color’, instead of the Canadian/British spelling
’colour’.  If you’re used to the British spelling, you’ll have to resign
yourself to misspelling it for the sake of these functions.)

The curses library maintains a finite number of color pairs, containing
a foreground (or text) color and a background color.  You can get the
attribute value corresponding to a color pair with the *note
color_pair(): 1ab1. function; this can be bitwise-OR’ed with other
attributes such as ‘A_REVERSE’, but again, such combinations are not
guaranteed to work on all terminals.

An example, which displays a line of text using color pair 1:

     stdscr.addstr("Pretty text", curses.color_pair(1))
     stdscr.refresh()

As I said before, a color pair consists of a foreground and background
color.  The ‘init_pair(n, f, b)’ function changes the definition of
color pair `n', to foreground color f and background color b.  Color
pair 0 is hard-wired to white on black, and cannot be changed.

Colors are numbered, and ‘start_color()’ initializes 8 basic colors when
it activates color mode.  They are: 0:black, 1:red, 2:green, 3:yellow,
4:blue, 5:magenta, 6:cyan, and 7:white.  The *note curses: 2b. module
defines named constants for each of these colors: ‘curses.COLOR_BLACK’,
‘curses.COLOR_RED’, and so forth.

Let’s put all this together.  To change color 1 to red text on a white
background, you would call:

     curses.init_pair(1, curses.COLOR_RED, curses.COLOR_WHITE)

When you change a color pair, any text already displayed using that
color pair will change to the new colors.  You can also display new text
in this color with:

     stdscr.addstr(0,0, "RED ALERT!", curses.color_pair(1))

Very fancy terminals can change the definitions of the actual colors to
a given RGB value.  This lets you change color 1, which is usually red,
to purple or blue or any other color you like.  Unfortunately, the Linux
console doesn’t support this, so I’m unable to try it out, and can’t
provide any examples.  You can check if your terminal can do this by
calling *note can_change_color(): 1aad, which returns ‘True’ if the
capability is there.  If you’re lucky enough to have such a talented
terminal, consult your system’s man pages for more information.


File: python.info,  Node: User Input,  Next: For More Information,  Prev: Displaying Text,  Up: Curses Programming with Python

10.3.5 User Input
-----------------

The C curses library offers only very simple input mechanisms.  Python’s
*note curses: 2b. module adds a basic text-input widget.  (Other
libraries such as Urwid(1) have more extensive collections of widgets.)

There are two methods for getting input from a window:

   * *note getch(): 1b10. refreshes the screen and then waits for the
     user to hit a key, displaying the key if *note echo(): 1aba. has
     been called earlier.  You can optionally specify a coordinate to
     which the cursor should be moved before pausing.

   * *note getkey(): 1b11. does the same thing but converts the integer
     to a string.  Individual characters are returned as 1-character
     strings, and special keys such as function keys return longer
     strings containing a key name such as ‘KEY_UP’ or ‘^G’.

It’s possible to not wait for the user using the *note nodelay(): 1b28.
window method.  After ‘nodelay(True)’, ‘getch()’ and ‘getkey()’ for the
window become non-blocking.  To signal that no input is ready, ‘getch()’
returns ‘curses.ERR’ (a value of -1) and ‘getkey()’ raises an exception.
There’s also a *note halfdelay(): 1ac8. function, which can be used to
(in effect) set a timer on each ‘getch()’; if no input becomes available
within a specified delay (measured in tenths of a second), curses raises
an exception.

The ‘getch()’ method returns an integer; if it’s between 0 and 255, it
represents the ASCII code of the key pressed.  Values greater than 255
are special keys such as Page Up, Home, or the cursor keys.  You can
compare the value returned to constants such as ‘curses.KEY_PPAGE’,
‘curses.KEY_HOME’, or ‘curses.KEY_LEFT’.  The main loop of your program
may look something like this:

     while True:
         c = stdscr.getch()
         if c == ord('p'):
             PrintDocument()
         elif c == ord('q'):
             break  # Exit the while loop
         elif c == curses.KEY_HOME:
             x = y = 0

The *note curses.ascii: 2c. module supplies ASCII class membership
functions that take either integer or 1-character string arguments;
these may be useful in writing more readable tests for such loops.  It
also supplies conversion functions that take either integer or
1-character-string arguments and return the same type.  For example,
*note curses.ascii.ctrl(): 1b5d. returns the control character
corresponding to its argument.

There’s also a method to retrieve an entire string, *note getstr():
1b14.  It isn’t used very often, because its functionality is quite
limited; the only editing keys available are the backspace key and the
Enter key, which terminates the string.  It can optionally be limited to
a fixed number of characters.

     curses.echo()            # Enable echoing of characters

     # Get a 15-character string, with the cursor on the top line
     s = stdscr.getstr(0,0, 15)

The *note curses.textpad: 2e. module supplies a text box that supports
an Emacs-like set of keybindings.  Various methods of the *note Textbox:
1b43. class support editing with input validation and gathering the edit
results either with or without trailing spaces.  Here’s an example:

     import curses
     from curses.textpad import Textbox, rectangle

     def main(stdscr):
         stdscr.addstr(0, 0, "Enter IM message: (hit Ctrl-G to send)")

         editwin = curses.newwin(5,30, 2,1)
         rectangle(stdscr, 1,0, 1+5+1, 1+30+1)
         stdscr.refresh()

         box = Textbox(editwin)

         # Let the user edit until Ctrl-G is struck.
         box.edit()

         # Get resulting contents
         message = box.gather()

See the library documentation on *note curses.textpad: 2e. for more
details.

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/urwid/


File: python.info,  Node: For More Information,  Prev: User Input,  Up: Curses Programming with Python

10.3.6 For More Information
---------------------------

This HOWTO doesn’t cover some advanced topics, such as reading the
contents of the screen or capturing mouse events from an xterm instance,
but the Python library page for the *note curses: 2b. module is now
reasonably complete.  You should browse it next.

If you’re in doubt about the detailed behavior of the curses functions,
consult the manual pages for your curses implementation, whether it’s
ncurses or a proprietary Unix vendor’s.  The manual pages will document
any quirks, and provide complete lists of all the functions, attributes,
and ‘ACS_*’ characters available to you.

Because the curses API is so large, some functions aren’t supported in
the Python interface.  Often this isn’t because they’re difficult to
implement, but because no one has needed them yet.  Also, Python doesn’t
yet support the menu library associated with ncurses.  Patches adding
support for these would be welcome; see the Python Developer’s Guide(1)
to learn more about submitting patches to Python.

   * Writing Programs with NCURSES(2): a lengthy tutorial for C
     programmers.

   * The ncurses man page(3)

   * The ncurses FAQ(4)

   * "Use curses...  don’t swear"(5): video of a PyCon 2013 talk on
     controlling terminals using curses or Urwid.

   * "Console Applications with Urwid"(6): video of a PyCon CA 2012 talk
     demonstrating some applications written using Urwid.

   ---------- Footnotes ----------

   (1) https://docs.python.org/devguide/

   (2) http://invisible-island.net/ncurses/ncurses-intro.html

   (3) http://linux.die.net/man/3/ncurses

   (4) http://invisible-island.net/ncurses/ncurses.faq.html

   (5) https://www.youtube.com/watch?v=eN1eZtjLEnU

   (6) http://www.pyvideo.org/video/1568/console-applications-with-urwid


File: python.info,  Node: Descriptor HowTo Guide,  Next: Functional Programming HOWTO,  Prev: Curses Programming with Python,  Up: Python HOWTOs

10.4 Descriptor HowTo Guide
===========================


Author: Raymond Hettinger


Contact: <python at rcn dot com>

* Menu:

* Abstract:: 
* Definition and Introduction:: 
* Descriptor Protocol:: 
* Invoking Descriptors: Invoking Descriptors<2>. 
* Descriptor Example:: 
* Properties:: 
* Functions and Methods:: 
* Static Methods and Class Methods:: 


File: python.info,  Node: Abstract,  Next: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.1 Abstract
---------------

Defines descriptors, summarizes the protocol, and shows how descriptors
are called.  Examines a custom descriptor and several built-in python
descriptors including functions, properties, static methods, and class
methods.  Shows how each works by giving a pure Python equivalent and a
sample application.

Learning about descriptors not only provides access to a larger toolset,
it creates a deeper understanding of how Python works and an
appreciation for the elegance of its design.


File: python.info,  Node: Definition and Introduction,  Next: Descriptor Protocol,  Prev: Abstract,  Up: Descriptor HowTo Guide

10.4.2 Definition and Introduction
----------------------------------

In general, a descriptor is an object attribute with "binding behavior",
one whose attribute access has been overridden by methods in the
descriptor protocol.  Those methods are *note __get__(): e02, *note
__set__(): e03, and *note __delete__(): e04.  If any of those methods
are defined for an object, it is said to be a descriptor.

The default behavior for attribute access is to get, set, or delete the
attribute from an object’s dictionary.  For instance, ‘a.x’ has a lookup
chain starting with ‘a.__dict__['x']’, then ‘type(a).__dict__['x']’, and
continuing through the base classes of ‘type(a)’ excluding metaclasses.
If the looked-up value is an object defining one of the descriptor
methods, then Python may override the default behavior and invoke the
descriptor method instead.  Where this occurs in the precedence chain
depends on which descriptor methods were defined.

Descriptors are a powerful, general purpose protocol.  They are the
mechanism behind properties, methods, static methods, class methods, and
*note super(): 56a.  They are used throughout Python itself to implement
the new style classes introduced in version 2.2.  Descriptors simplify
the underlying C-code and offer a flexible set of new tools for everyday
Python programs.


File: python.info,  Node: Descriptor Protocol,  Next: Invoking Descriptors<2>,  Prev: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.3 Descriptor Protocol
--------------------------

‘descr.__get__(self, obj, type=None) --> value’

‘descr.__set__(self, obj, value) --> None’

‘descr.__delete__(self, obj) --> None’

That is all there is to it.  Define any of these methods and an object
is considered a descriptor and can override default behavior upon being
looked up as an attribute.

If an object defines both *note __get__(): e02. and *note __set__():
e03, it is considered a data descriptor.  Descriptors that only define
*note __get__(): e02. are called non-data descriptors (they are
typically used for methods but other uses are possible).

Data and non-data descriptors differ in how overrides are calculated
with respect to entries in an instance’s dictionary.  If an instance’s
dictionary has an entry with the same name as a data descriptor, the
data descriptor takes precedence.  If an instance’s dictionary has an
entry with the same name as a non-data descriptor, the dictionary entry
takes precedence.

To make a read-only data descriptor, define both *note __get__(): e02.
and *note __set__(): e03. with the *note __set__(): e03. raising an
*note AttributeError: 356. when called.  Defining the *note __set__():
e03. method with an exception raising placeholder is enough to make it a
data descriptor.


File: python.info,  Node: Invoking Descriptors<2>,  Next: Descriptor Example,  Prev: Descriptor Protocol,  Up: Descriptor HowTo Guide

10.4.4 Invoking Descriptors
---------------------------

A descriptor can be called directly by its method name.  For example,
‘d.__get__(obj)’.

Alternatively, it is more common for a descriptor to be invoked
automatically upon attribute access.  For example, ‘obj.d’ looks up ‘d’
in the dictionary of ‘obj’.  If ‘d’ defines the method *note __get__():
e02, then ‘d.__get__(obj)’ is invoked according to the precedence rules
listed below.

The details of invocation depend on whether ‘obj’ is an object or a
class.

For objects, the machinery is in *note object.__getattribute__(): 783.
which transforms ‘b.x’ into ‘type(b).__dict__['x'].__get__(b, type(b))’.
The implementation works through a precedence chain that gives data
descriptors priority over instance variables, instance variables
priority over non-data descriptors, and assigns lowest priority to *note
__getattr__(): 782. if provided.  The full C implementation can be found
in *note PyObject_GenericGetAttr(): 34af. in Objects/object.c(1).

For classes, the machinery is in ‘type.__getattribute__()’ which
transforms ‘B.x’ into ‘B.__dict__['x'].__get__(None, B)’.  In pure
Python, it looks like:

     def __getattribute__(self, key):
         "Emulate type_getattro() in Objects/typeobject.c"
         v = object.__getattribute__(self, key)
         if hasattr(v, '__get__'):
            return v.__get__(None, self)
         return v

The important points to remember are:

   * descriptors are invoked by the *note __getattribute__(): 783.
     method

   * overriding *note __getattribute__(): 783. prevents automatic
     descriptor calls

   * *note object.__getattribute__(): 783. and ‘type.__getattribute__()’
     make different calls to *note __get__(): e02.

   * data descriptors always override instance dictionaries.

   * non-data descriptors may be overridden by instance dictionaries.

The object returned by ‘super()’ also has a custom *note
__getattribute__(): 783. method for invoking descriptors.  The call
‘super(B, obj).m()’ searches ‘obj.__class__.__mro__’ for the base class
‘A’ immediately following ‘B’ and then returns
‘A.__dict__['m'].__get__(obj, B)’.  If not a descriptor, ‘m’ is returned
unchanged.  If not in the dictionary, ‘m’ reverts to a search using
*note object.__getattribute__(): 783.

The implementation details are in ‘super_getattro()’ in
Objects/typeobject.c(2).  and a pure Python equivalent can be found in
Guido’s Tutorial(3).

The details above show that the mechanism for descriptors is embedded in
the *note __getattribute__(): 783. methods for *note object: 5cb, *note
type: 376, and *note super(): 56a.  Classes inherit this machinery when
they derive from *note object: 5cb. or if they have a meta-class
providing similar functionality.  Likewise, classes can turn-off
descriptor invocation by overriding *note __getattribute__(): 783.

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Objects/object.c

   (2) https://hg.python.org/cpython/file/default/Objects/typeobject.c

   (3) 
https://www.python.org/download/releases/2.2.3/descrintro/#cooperation


File: python.info,  Node: Descriptor Example,  Next: Properties,  Prev: Invoking Descriptors<2>,  Up: Descriptor HowTo Guide

10.4.5 Descriptor Example
-------------------------

The following code creates a class whose objects are data descriptors
which print a message for each get or set.  Overriding *note
__getattribute__(): 783. is alternate approach that could do this for
every attribute.  However, this descriptor is useful for monitoring just
a few chosen attributes:

     class RevealAccess(object):
         """A data descriptor that sets and returns values
            normally and prints a message logging their access.
         """

         def __init__(self, initval=None, name='var'):
             self.val = initval
             self.name = name

         def __get__(self, obj, objtype):
             print('Retrieving', self.name)
             return self.val

         def __set__(self, obj, val):
             print('Updating', self.name)
             self.val = val

     >>> class MyClass(object):
         x = RevealAccess(10, 'var "x"')
         y = 5

     >>> m = MyClass()
     >>> m.x
     Retrieving var "x"
     10
     >>> m.x = 20
     Updating var "x"
     >>> m.x
     Retrieving var "x"
     20
     >>> m.y
     5

The protocol is simple and offers exciting possibilities.  Several use
cases are so common that they have been packaged into individual
function calls.  Properties, bound and unbound methods, static methods,
and class methods are all based on the descriptor protocol.


File: python.info,  Node: Properties,  Next: Functions and Methods,  Prev: Descriptor Example,  Up: Descriptor HowTo Guide

10.4.6 Properties
-----------------

Calling *note property(): 377. is a succinct way of building a data
descriptor that triggers function calls upon access to an attribute.
Its signature is:

     property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

The documentation shows a typical use to define a managed attribute ‘x’:

     class C(object):
         def getx(self): return self.__x
         def setx(self, value): self.__x = value
         def delx(self): del self.__x
         x = property(getx, setx, delx, "I'm the 'x' property.")

To see how *note property(): 377. is implemented in terms of the
descriptor protocol, here is a pure Python equivalent:

     class Property(object):
         "Emulate PyProperty_Type() in Objects/descrobject.c"

         def __init__(self, fget=None, fset=None, fdel=None, doc=None):
             self.fget = fget
             self.fset = fset
             self.fdel = fdel
             if doc is None and fget is not None:
                 doc = fget.__doc__
             self.__doc__ = doc

         def __get__(self, obj, objtype=None):
             if obj is None:
                 return self
             if self.fget is None:
                 raise AttributeError("unreadable attribute")
             return self.fget(obj)

         def __set__(self, obj, value):
             if self.fset is None:
                 raise AttributeError("can't set attribute")
             self.fset(obj, value)

         def __delete__(self, obj):
             if self.fdel is None:
                 raise AttributeError("can't delete attribute")
             self.fdel(obj)

         def getter(self, fget):
             return type(self)(fget, self.fset, self.fdel, self.__doc__)

         def setter(self, fset):
             return type(self)(self.fget, fset, self.fdel, self.__doc__)

         def deleter(self, fdel):
             return type(self)(self.fget, self.fset, fdel, self.__doc__)

The *note property(): 377. builtin helps whenever a user interface has
granted attribute access and then subsequent changes require the
intervention of a method.

For instance, a spreadsheet class may grant access to a cell value
through ‘Cell('b10').value’.  Subsequent improvements to the program
require the cell to be recalculated on every access; however, the
programmer does not want to affect existing client code accessing the
attribute directly.  The solution is to wrap access to the value
attribute in a property data descriptor:

     class Cell(object):
         . . .
         def getvalue(self, obj):
             "Recalculate cell before returning value"
             self.recalc()
             return obj._value
         value = property(getvalue)


File: python.info,  Node: Functions and Methods,  Next: Static Methods and Class Methods,  Prev: Properties,  Up: Descriptor HowTo Guide

10.4.7 Functions and Methods
----------------------------

Python’s object oriented features are built upon a function based
environment.  Using non-data descriptors, the two are merged seamlessly.

Class dictionaries store methods as functions.  In a class definition,
methods are written using *note def: a3a. and *note lambda: 894, the
usual tools for creating functions.  The only difference from regular
functions is that the first argument is reserved for the object
instance.  By Python convention, the instance reference is called `self'
but may be called `this' or any other variable name.

To support method calls, functions include the *note __get__(): e02.
method for binding methods during attribute access.  This means that all
functions are non-data descriptors which return bound or unbound methods
depending whether they are invoked from an object or a class.  In pure
python, it works like this:

     class Function(object):
         . . .
         def __get__(self, obj, objtype=None):
             "Simulate func_descr_get() in Objects/funcobject.c"
             return types.MethodType(self, obj, objtype)

Running the interpreter shows how the function descriptor works in
practice:

     >>> class D(object):
          def f(self, x):
               return x

     >>> d = D()
     >>> D.__dict__['f'] # Stored internally as a function
     <function f at 0x00C45070>
     >>> D.f             # Get from a class becomes an unbound method
     <unbound method D.f>
     >>> d.f             # Get from an instance becomes a bound method
     <bound method D.f of <__main__.D object at 0x00B18C90>>

The output suggests that bound and unbound methods are two different
types.  While they could have been implemented that way, the actual C
implementation of *note PyMethod_Type: 36a7. in Objects/classobject.c(1)
is a single object with two different representations depending on
whether the ‘im_self’ field is set or is `NULL' (the C equivalent of
`None').

Likewise, the effects of calling a method object depend on the ‘im_self’
field.  If set (meaning bound), the original function (stored in the
‘im_func’ field) is called as expected with the first argument set to
the instance.  If unbound, all of the arguments are passed unchanged to
the original function.  The actual C implementation of
‘instancemethod_call()’ is only slightly more complex in that it
includes some type checking.

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Objects/classobject.c


File: python.info,  Node: Static Methods and Class Methods,  Prev: Functions and Methods,  Up: Descriptor HowTo Guide

10.4.8 Static Methods and Class Methods
---------------------------------------

Non-data descriptors provide a simple mechanism for variations on the
usual patterns of binding functions into methods.

To recap, functions have a *note __get__(): e02. method so that they can
be converted to a method when accessed as attributes.  The non-data
descriptor transforms an ‘obj.f(*args)’ call into ‘f(obj, *args)’.
Calling ‘klass.f(*args)’ becomes ‘f(*args)’.

This chart summarizes the binding and its two most useful variants:

     Transformation        Called from an Object      Called from a Class
                                                      
     ------------------------------------------------------------------------
                                                      
     function              f(obj, *args)              f(*args)
                                                      
                                                      
     staticmethod          f(*args)                   f(*args)
                                                      
                                                      
     classmethod           f(type(obj), *args)        f(klass, *args)
                                                      

Static methods return the underlying function without changes.  Calling
either ‘c.f’ or ‘C.f’ is the equivalent of a direct lookup into
‘object.__getattribute__(c, "f")’ or ‘object.__getattribute__(C, "f")’.
As a result, the function becomes identically accessible from either an
object or a class.

Good candidates for static methods are methods that do not reference the
‘self’ variable.

For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the
average, mean, median, and other descriptive statistics that depend on
the data.  However, there may be useful functions which are conceptually
related but do not depend on the data.  For instance, ‘erf(x)’ is handy
conversion routine that comes up in statistical work but does not
directly depend on a particular dataset.  It can be called either from
an object or the class: ‘s.erf(1.5) --> .9332’ or ‘Sample.erf(1.5) -->
.9332’.

Since staticmethods return the underlying function with no changes, the
example calls are unexciting:

     >>> class E(object):
          def f(x):
               print(x)
          f = staticmethod(f)

     >>> print(E.f(3))
     3
     >>> print(E().f(3))
     3

Using the non-data descriptor protocol, a pure Python version of *note
staticmethod(): 5f6. would look like this:

     class StaticMethod(object):
      "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

      def __init__(self, f):
           self.f = f

      def __get__(self, obj, objtype=None):
           return self.f

Unlike static methods, class methods prepend the class reference to the
argument list before calling the function.  This format is the same for
whether the caller is an object or a class:

     >>> class E(object):
          def f(klass, x):
               return klass.__name__, x
          f = classmethod(f)

     >>> print(E.f(3))
     ('E', 3)
     >>> print(E().f(3))
     ('E', 3)

This behavior is useful whenever the function only needs to have a class
reference and does not care about any underlying data.  One use for
classmethods is to create alternate class constructors.  In Python 2.3,
the classmethod *note dict.fromkeys(): 1088. creates a new dictionary
from a list of keys.  The pure Python equivalent is:

     class Dict(object):
         . . .
         def fromkeys(klass, iterable, value=None):
             "Emulate dict_fromkeys() in Objects/dictobject.c"
             d = klass()
             for key in iterable:
                 d[key] = value
             return d
         fromkeys = classmethod(fromkeys)

Now a new dictionary of unique keys can be constructed like this:

     >>> Dict.fromkeys('abracadabra')
     {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

Using the non-data descriptor protocol, a pure Python version of *note
classmethod(): 5f4. would look like this:

     class ClassMethod(object):
          "Emulate PyClassMethod_Type() in Objects/funcobject.c"

          def __init__(self, f):
               self.f = f

          def __get__(self, obj, klass=None):
               if klass is None:
                    klass = type(obj)
               def newfunc(*args):
                    return self.f(klass, *args)
               return newfunc


File: python.info,  Node: Functional Programming HOWTO,  Next: Logging HOWTO,  Prev: Descriptor HowTo Guide,  Up: Python HOWTOs

10.5 Functional Programming HOWTO
=================================


Author: A. M. Kuchling


Release: 0.32

In this document, we’ll take a tour of Python’s features suitable for
implementing programs in a functional style.  After an introduction to
the concepts of functional programming, we’ll look at language features
such as *note iterator: e4f.s and *note generator: 5c0.s and relevant
library modules such as *note itertools: a1. and *note functools: 84.

* Menu:

* Introduction: Introduction<14>. 
* Iterators: Iterators<2>. 
* Generator expressions and list comprehensions:: 
* Generators: Generators<2>. 
* Built-in functions:: 
* The itertools module:: 
* The functools module:: 
* Small functions and the lambda expression:: 
* Revision History and Acknowledgements:: 
* References: References<2>. 


File: python.info,  Node: Introduction<14>,  Next: Iterators<2>,  Up: Functional Programming HOWTO

10.5.1 Introduction
-------------------

This section explains the basic concept of functional programming; if
you’re just interested in learning about Python language features, skip
to the next section on *note Iterators: 3867.

Programming languages support decomposing problems in several different
ways:

   * Most programming languages are `procedural': programs are lists of
     instructions that tell the computer what to do with the program’s
     input.  C, Pascal, and even Unix shells are procedural languages.

   * In `declarative' languages, you write a specification that
     describes the problem to be solved, and the language implementation
     figures out how to perform the computation efficiently.  SQL is the
     declarative language you’re most likely to be familiar with; a SQL
     query describes the data set you want to retrieve, and the SQL
     engine decides whether to scan tables or use indexes, which
     subclauses should be performed first, etc.

   * `Object-oriented' programs manipulate collections of objects.
     Objects have internal state and support methods that query or
     modify this internal state in some way.  Smalltalk and Java are
     object-oriented languages.  C++ and Python are languages that
     support object-oriented programming, but don’t force the use of
     object-oriented features.

   * `Functional' programming decomposes a problem into a set of
     functions.  Ideally, functions only take inputs and produce
     outputs, and don’t have any internal state that affects the output
     produced for a given input.  Well-known functional languages
     include the ML family (Standard ML, OCaml, and other variants) and
     Haskell.

The designers of some computer languages choose to emphasize one
particular approach to programming.  This often makes it difficult to
write programs that use a different approach.  Other languages are
multi-paradigm languages that support several different approaches.
Lisp, C++, and Python are multi-paradigm; you can write programs or
libraries that are largely procedural, object-oriented, or functional in
all of these languages.  In a large program, different sections might be
written using different approaches; the GUI might be object-oriented
while the processing logic is procedural or functional, for example.

In a functional program, input flows through a set of functions.  Each
function operates on its input and produces some output.  Functional
style discourages functions with side effects that modify internal state
or make other changes that aren’t visible in the function’s return
value.  Functions that have no side effects at all are called `purely
functional'.  Avoiding side effects means not using data structures that
get updated as a program runs; every function’s output must only depend
on its input.

Some languages are very strict about purity and don’t even have
assignment statements such as ‘a=3’ or ‘c = a + b’, but it’s difficult
to avoid all side effects.  Printing to the screen or writing to a disk
file are side effects, for example.  For example, in Python a call to
the *note print(): 481. or *note time.sleep(): 216. function both return
no useful value; they’re only called for their side effects of sending
some text to the screen or pausing execution for a second.

Python programs written in functional style usually won’t go to the
extreme of avoiding all I/O or all assignments; instead, they’ll provide
a functional-appearing interface but will use non-functional features
internally.  For example, the implementation of a function will still
use assignments to local variables, but won’t modify global variables or
have other side effects.

Functional programming can be considered the opposite of object-oriented
programming.  Objects are little capsules containing some internal state
along with a collection of method calls that let you modify this state,
and programs consist of making the right set of state changes.
Functional programming wants to avoid state changes as much as possible
and works with data flowing between functions.  In Python you might
combine the two approaches by writing functions that take and return
instances representing objects in your application (e-mail messages,
transactions, etc.).

Functional design may seem like an odd constraint to work under.  Why
should you avoid objects and side effects?  There are theoretical and
practical advantages to the functional style:

   * Formal provability.

   * Modularity.

   * Composability.

   * Ease of debugging and testing.

* Menu:

* Formal provability:: 
* Modularity:: 
* Ease of debugging and testing:: 
* Composability:: 


File: python.info,  Node: Formal provability,  Next: Modularity,  Up: Introduction<14>

10.5.1.1 Formal provability
...........................

A theoretical benefit is that it’s easier to construct a mathematical
proof that a functional program is correct.

For a long time researchers have been interested in finding ways to
mathematically prove programs correct.  This is different from testing a
program on numerous inputs and concluding that its output is usually
correct, or reading a program’s source code and concluding that the code
looks right; the goal is instead a rigorous proof that a program
produces the right result for all possible inputs.

The technique used to prove programs correct is to write down
`invariants', properties of the input data and of the program’s
variables that are always true.  For each line of code, you then show
that if invariants X and Y are true `before' the line is executed, the
slightly different invariants X’ and Y’ are true `after' the line is
executed.  This continues until you reach the end of the program, at
which point the invariants should match the desired conditions on the
program’s output.

Functional programming’s avoidance of assignments arose because
assignments are difficult to handle with this technique; assignments can
break invariants that were true before the assignment without producing
any new invariants that can be propagated onward.

Unfortunately, proving programs correct is largely impractical and not
relevant to Python software.  Even trivial programs require proofs that
are several pages long; the proof of correctness for a moderately
complicated program would be enormous, and few or none of the programs
you use daily (the Python interpreter, your XML parser, your web
browser) could be proven correct.  Even if you wrote down or generated a
proof, there would then be the question of verifying the proof; maybe
there’s an error in it, and you wrongly believe you’ve proved the
program correct.


File: python.info,  Node: Modularity,  Next: Ease of debugging and testing,  Prev: Formal provability,  Up: Introduction<14>

10.5.1.2 Modularity
...................

A more practical benefit of functional programming is that it forces you
to break apart your problem into small pieces.  Programs are more
modular as a result.  It’s easier to specify and write a small function
that does one thing than a large function that performs a complicated
transformation.  Small functions are also easier to read and to check
for errors.


File: python.info,  Node: Ease of debugging and testing,  Next: Composability,  Prev: Modularity,  Up: Introduction<14>

10.5.1.3 Ease of debugging and testing
......................................

Testing and debugging a functional-style program is easier.

Debugging is simplified because functions are generally small and
clearly specified.  When a program doesn’t work, each function is an
interface point where you can check that the data are correct.  You can
look at the intermediate inputs and outputs to quickly isolate the
function that’s responsible for a bug.

Testing is easier because each function is a potential subject for a
unit test.  Functions don’t depend on system state that needs to be
replicated before running a test; instead you only have to synthesize
the right input and then check that the output matches expectations.


File: python.info,  Node: Composability,  Prev: Ease of debugging and testing,  Up: Introduction<14>

10.5.1.4 Composability
......................

As you work on a functional-style program, you’ll write a number of
functions with varying inputs and outputs.  Some of these functions will
be unavoidably specialized to a particular application, but others will
be useful in a wide variety of programs.  For example, a function that
takes a directory path and returns all the XML files in the directory,
or a function that takes a filename and returns its contents, can be
applied to many different situations.

Over time you’ll form a personal library of utilities.  Often you’ll
assemble new programs by arranging existing functions in a new
configuration and writing a few functions specialized for the current
task.


File: python.info,  Node: Iterators<2>,  Next: Generator expressions and list comprehensions,  Prev: Introduction<14>,  Up: Functional Programming HOWTO

10.5.2 Iterators
----------------

I’ll start by looking at a Python language feature that’s an important
foundation for writing functional-style programs: iterators.

An iterator is an object representing a stream of data; this object
returns the data one element at a time.  A Python iterator must support
a method called *note __next__(): 8cf. that takes no arguments and
always returns the next element of the stream.  If there are no more
elements in the stream, *note __next__(): 8cf. must raise the *note
StopIteration: 191. exception.  Iterators don’t have to be finite,
though; it’s perfectly reasonable to write an iterator that produces an
infinite stream of data.

The built-in *note iter(): 99a. function takes an arbitrary object and
tries to return an iterator that will return the object’s contents or
elements, raising *note TypeError: 562. if the object doesn’t support
iteration.  Several of Python’s built-in data types support iteration,
the most common being lists and dictionaries.  An object is called *note
iterable: 80a. if you can get an iterator for it.

You can experiment with the iteration interface manually:

     >>> L = [1,2,3]
     >>> it = iter(L)
     >>> it  #doctest: +ELLIPSIS
     <...iterator object at ...>
     >>> it.__next__()  # same as next(it)
     1
     >>> next(it)
     2
     >>> next(it)
     3
     >>> next(it)
     Traceback (most recent call last):
       File "<stdin>", line 1, in ?
     StopIteration
     >>>

Python expects iterable objects in several different contexts, the most
important being the *note for: 895. statement.  In the statement ‘for X
in Y’, Y must be an iterator or some object for which *note iter(): 99a.
can create an iterator.  These two statements are equivalent:

     for i in iter(obj):
         print(i)

     for i in obj:
         print(i)

Iterators can be materialized as lists or tuples by using the *note
list(): 25d. or *note tuple(): 25c. constructor functions:

     >>> L = [1,2,3]
     >>> iterator = iter(L)
     >>> t = tuple(iterator)
     >>> t
     (1, 2, 3)

Sequence unpacking also supports iterators: if you know an iterator will
return N elements, you can unpack them into an N-tuple:

     >>> L = [1,2,3]
     >>> iterator = iter(L)
     >>> a,b,c = iterator
     >>> a,b,c
     (1, 2, 3)

Built-in functions such as *note max(): 3fa. and *note min(): 3f9. can
take a single iterator argument and will return the largest or smallest
element.  The ‘"in"’ and ‘"not in"’ operators also support iterators: ‘X
in iterator’ is true if X is found in the stream returned by the
iterator.  You’ll run into obvious problems if the iterator is infinite;
*note max(): 3fa, *note min(): 3f9. will never return, and if the
element X never appears in the stream, the ‘"in"’ and ‘"not in"’
operators won’t return either.

Note that you can only go forward in an iterator; there’s no way to get
the previous element, reset the iterator, or make a copy of it.
Iterator objects can optionally provide these additional capabilities,
but the iterator protocol only specifies the *note __next__(): 8cf.
method.  Functions may therefore consume all of the iterator’s output,
and if you need to do something different with the same stream, you’ll
have to create a new iterator.

* Menu:

* Data Types That Support Iterators:: 


File: python.info,  Node: Data Types That Support Iterators,  Up: Iterators<2>

10.5.2.1 Data Types That Support Iterators
..........................................

We’ve already seen how lists and tuples support iterators.  In fact, any
Python sequence type, such as strings, will automatically support
creation of an iterator.

Calling *note iter(): 99a. on a dictionary returns an iterator that will
loop over the dictionary’s keys:

     >>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
     ...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
     >>> for key in m:  #doctest: +SKIP
     ...     print(key, m[key])
     Mar 3
     Feb 2
     Aug 8
     Sep 9
     Apr 4
     Jun 6
     Jul 7
     Jan 1
     May 5
     Nov 11
     Dec 12
     Oct 10

Note that the order is essentially random, because it’s based on the
hash ordering of the objects in the dictionary.

Applying *note iter(): 99a. to a dictionary always loops over the keys,
but dictionaries have methods that return other iterators.  If you want
to iterate over values or key/value pairs, you can explicitly call the
*note values(): 891. or *note items(): 890. methods to get an
appropriate iterator.

The *note dict(): 3b0. constructor can accept an iterator that returns a
finite stream of ‘(key, value)’ tuples:

     >>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
     >>> dict(iter(L))  #doctest: +SKIP
     {'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}

Files also support iteration by calling the *note readline(): faa.
method until there are no more lines in the file.  This means you can
read each line of a file like this:

     for line in file:
         # do something for each line
         ...

Sets can take their contents from an iterable and let you iterate over
the set’s elements:

     S = {2, 3, 5, 7, 11, 13}
     for i in S:
         print(i)


File: python.info,  Node: Generator expressions and list comprehensions,  Next: Generators<2>,  Prev: Iterators<2>,  Up: Functional Programming HOWTO

10.5.3 Generator expressions and list comprehensions
----------------------------------------------------

Two common operations on an iterator’s output are 1) performing some
operation for every element, 2) selecting a subset of elements that meet
some condition.  For example, given a list of strings, you might want to
strip off trailing whitespace from each line or extract all the strings
containing a given substring.

List comprehensions and generator expressions (short form: "listcomps"
and "genexps") are a concise notation for such operations, borrowed from
the functional programming language Haskell
(‘https://www.haskell.org/’).  You can strip all the whitespace from a
stream of strings with the following code:

     line_list = ['  line 1\n', 'line 2  \n', ...]

     # Generator expression -- returns iterator
     stripped_iter = (line.strip() for line in line_list)

     # List comprehension -- returns list
     stripped_list = [line.strip() for line in line_list]

You can select only certain elements by adding an ‘"if"’ condition:

     stripped_list = [line.strip() for line in line_list
                      if line != ""]

With a list comprehension, you get back a Python list; ‘stripped_list’
is a list containing the resulting lines, not an iterator.  Generator
expressions return an iterator that computes the values as necessary,
not needing to materialize all the values at once.  This means that list
comprehensions aren’t useful if you’re working with iterators that
return an infinite stream or a very large amount of data.  Generator
expressions are preferable in these situations.

Generator expressions are surrounded by parentheses ("()") and list
comprehensions are surrounded by square brackets ("[]").  Generator
expressions have the form:

     ( expression for expr in sequence1
                  if condition1
                  for expr2 in sequence2
                  if condition2
                  for expr3 in sequence3 ...
                  if condition3
                  for exprN in sequenceN
                  if conditionN )

Again, for a list comprehension only the outside brackets are different
(square brackets instead of parentheses).

The elements of the generated output will be the successive values of
‘expression’.  The ‘if’ clauses are all optional; if present,
‘expression’ is only evaluated and added to the result when ‘condition’
is true.

Generator expressions always have to be written inside parentheses, but
the parentheses signalling a function call also count.  If you want to
create an iterator that will be immediately passed to a function you can
write:

     obj_total = sum(obj.count for obj in list_all_objects())

The ‘for...in’ clauses contain the sequences to be iterated over.  The
sequences do not have to be the same length, because they are iterated
over from left to right, `not' in parallel.  For each element in
‘sequence1’, ‘sequence2’ is looped over from the beginning.  ‘sequence3’
is then looped over for each resulting pair of elements from ‘sequence1’
and ‘sequence2’.

To put it another way, a list comprehension or generator expression is
equivalent to the following Python code:

     for expr1 in sequence1:
         if not (condition1):
             continue   # Skip this element
         for expr2 in sequence2:
             if not (condition2):
                 continue    # Skip this element
             ...
             for exprN in sequenceN:
                  if not (conditionN):
                      continue   # Skip this element

                  # Output the value of
                  # the expression.

This means that when there are multiple ‘for...in’ clauses but no ‘if’
clauses, the length of the resulting output will be equal to the product
of the lengths of all the sequences.  If you have two lists of length 3,
the output list is 9 elements long:

     >>> seq1 = 'abc'
     >>> seq2 = (1,2,3)
     >>> [(x, y) for x in seq1 for y in seq2]  #doctest: +NORMALIZE_WHITESPACE
     [('a', 1), ('a', 2), ('a', 3),
      ('b', 1), ('b', 2), ('b', 3),
      ('c', 1), ('c', 2), ('c', 3)]

To avoid introducing an ambiguity into Python’s grammar, if ‘expression’
is creating a tuple, it must be surrounded with parentheses.  The first
list comprehension below is a syntax error, while the second one is
correct:

     # Syntax error
     [x, y for x in seq1 for y in seq2]
     # Correct
     [(x, y) for x in seq1 for y in seq2]


File: python.info,  Node: Generators<2>,  Next: Built-in functions,  Prev: Generator expressions and list comprehensions,  Up: Functional Programming HOWTO

10.5.4 Generators
-----------------

Generators are a special class of functions that simplify the task of
writing iterators.  Regular functions compute a value and return it, but
generators return an iterator that returns a stream of values.

You’re doubtless familiar with how regular function calls work in Python
or C. When you call a function, it gets a private namespace where its
local variables are created.  When the function reaches a ‘return’
statement, the local variables are destroyed and the value is returned
to the caller.  A later call to the same function creates a new private
namespace and a fresh set of local variables.  But, what if the local
variables weren’t thrown away on exiting a function?  What if you could
later resume the function where it left off?  This is what generators
provide; they can be thought of as resumable functions.

Here’s the simplest example of a generator function:

     >>> def generate_ints(N):
     ...    for i in range(N):
     ...        yield i

Any function containing a *note yield: 480. keyword is a generator
function; this is detected by Python’s *note bytecode: d06. compiler
which compiles the function specially as a result.

When you call a generator function, it doesn’t return a single value;
instead it returns a generator object that supports the iterator
protocol.  On executing the ‘yield’ expression, the generator outputs
the value of ‘i’, similar to a ‘return’ statement.  The big difference
between ‘yield’ and a ‘return’ statement is that on reaching a ‘yield’
the generator’s state of execution is suspended and local variables are
preserved.  On the next call to the generator’s *note __next__(): c99.
method, the function will resume executing.

Here’s a sample usage of the ‘generate_ints()’ generator:

     >>> gen = generate_ints(3)
     >>> gen  #doctest: +ELLIPSIS
     <generator object generate_ints at ...>
     >>> next(gen)
     0
     >>> next(gen)
     1
     >>> next(gen)
     2
     >>> next(gen)
     Traceback (most recent call last):
       File "stdin", line 1, in ?
       File "stdin", line 2, in generate_ints
     StopIteration

You could equally write ‘for i in generate_ints(5)’, or ‘a,b,c =
generate_ints(3)’.

Inside a generator function, ‘return value’ causes
‘StopIteration(value)’ to be raised from the *note __next__(): c99.
method.  Once this happens, or the bottom of the function is reached,
the procession of values ends and the generator cannot yield any further
values.

You could achieve the effect of generators manually by writing your own
class and storing all the local variables of the generator as instance
variables.  For example, returning a list of integers could be done by
setting ‘self.count’ to 0, and having the *note __next__(): 8cf. method
increment ‘self.count’ and return it.  However, for a moderately
complicated generator, writing a corresponding class can be much
messier.

The test suite included with Python’s library,
Lib/test/test_generators.py(1), contains a number of more interesting
examples.  Here’s one generator that implements an in-order traversal of
a tree using generators recursively.

     # A recursive generator that generates Tree leaves in in-order.
     def inorder(t):
         if t:
             for x in inorder(t.left):
                 yield x

             yield t.label

             for x in inorder(t.right):
                 yield x

Two other examples in ‘test_generators.py’ produce solutions for the
N-Queens problem (placing N queens on an NxN chess board so that no
queen threatens another) and the Knight’s Tour (finding a route that
takes a knight to every square of an NxN chessboard without visiting any
square twice).

* Menu:

* Passing values into a generator:: 

   ---------- Footnotes ----------

   (1) 
https://hg.python.org/cpython/file/default/Lib/test/test_generators.py


File: python.info,  Node: Passing values into a generator,  Up: Generators<2>

10.5.4.1 Passing values into a generator
........................................

In Python 2.4 and earlier, generators only produced output.  Once a
generator’s code was invoked to create an iterator, there was no way to
pass any new information into the function when its execution is
resumed.  You could hack together this ability by making the generator
look at a global variable or by passing in some mutable object that
callers then modify, but these approaches are messy.

In Python 2.5 there’s a simple way to pass values into a generator.
*note yield: 480. became an expression, returning a value that can be
assigned to a variable or otherwise operated on:

     val = (yield i)

I recommend that you `always' put parentheses around a ‘yield’
expression when you’re doing something with the returned value, as in
the above example.  The parentheses aren’t always necessary, but it’s
easier to always add them instead of having to remember when they’re
needed.

( PEP 342(1) explains the exact rules, which are that a
‘yield’-expression must always be parenthesized except when it occurs at
the top-level expression on the right-hand side of an assignment.  This
means you can write ‘val = yield i’ but have to use parentheses when
there’s an operation, as in ‘val = (yield i) + 12’.)

Values are sent into a generator by calling its *note send(value): e54.
method.  This method resumes the generator’s code and the ‘yield’
expression returns the specified value.  If the regular *note
__next__(): c99. method is called, the ‘yield’ returns ‘None’.

Here’s a simple counter that increments by 1 and allows changing the
value of the internal counter.

     def counter(maximum):
         i = 0
         while i < maximum:
             val = (yield i)
             # If value provided, change counter
             if val is not None:
                 i = val
             else:
                 i += 1

And here’s an example of changing the counter:

     >>> it = counter(10)  #doctest: +SKIP
     >>> next(it)  #doctest: +SKIP
     0
     >>> next(it)  #doctest: +SKIP
     1
     >>> it.send(8)  #doctest: +SKIP
     8
     >>> next(it)  #doctest: +SKIP
     9
     >>> next(it)  #doctest: +SKIP
     Traceback (most recent call last):
       File "t.py", line 15, in ?
         it.next()
     StopIteration

Because ‘yield’ will often be returning ‘None’, you should always check
for this case.  Don’t just use its value in expressions unless you’re
sure that the *note send(): e54. method will be the only method used to
resume your generator function.

In addition to *note send(): e54, there are two other methods on
generators:

   * *note throw(type, value=None, traceback=None): e56. is used to
     raise an exception inside the generator; the exception is raised by
     the ‘yield’ expression where the generator’s execution is paused.

   * *note close(): e58. raises a *note GeneratorExit: 9a6. exception
     inside the generator to terminate the iteration.  On receiving this
     exception, the generator’s code must either raise *note
     GeneratorExit: 9a6. or *note StopIteration: 191.; catching the
     exception and doing anything else is illegal and will trigger a
     *note RuntimeError: 193.  *note close(): e58. will also be called
     by Python’s garbage collector when the generator is
     garbage-collected.

     If you need to run cleanup code when a *note GeneratorExit: 9a6.
     occurs, I suggest using a ‘try: ... finally:’ suite instead of
     catching *note GeneratorExit: 9a6.

The cumulative effect of these changes is to turn generators from
one-way producers of information into both producers and consumers.

Generators also become `coroutines', a more generalized form of
subroutines.  Subroutines are entered at one point and exited at another
point (the top of the function, and a ‘return’ statement), but
coroutines can be entered, exited, and resumed at many different points
(the ‘yield’ statements).

   ---------- Footnotes ----------

   (1) https://www.python.org/dev/peps/pep-0342


File: python.info,  Node: Built-in functions,  Next: The itertools module,  Prev: Generators<2>,  Up: Functional Programming HOWTO

10.5.5 Built-in functions
-------------------------

Let’s look in more detail at built-in functions often used with
iterators.

Two of Python’s built-in functions, *note map(): 892. and *note
filter(): 893. duplicate the features of generator expressions:

*note map(f, iterA, iterB, ...): 892. returns an iterator over the sequence

     ‘f(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2],
     iterB[2]), ...’.

          >>> def upper(s):
          ...     return s.upper()

          >>> list(map(upper, ['sentence', 'fragment']))
          ['SENTENCE', 'FRAGMENT']
          >>> [upper(s) for s in ['sentence', 'fragment']]
          ['SENTENCE', 'FRAGMENT']

You can of course achieve the same effect with a list comprehension.

*note filter(predicate, iter): 893. returns an iterator over all the
sequence elements that meet a certain condition, and is similarly
duplicated by list comprehensions.  A `predicate' is a function that
returns the truth value of some condition; for use with *note filter():
893, the predicate must take a single value.

     >>> def is_even(x):
     ...     return (x % 2) == 0

     >>> list(filter(is_even, range(10)))
     [0, 2, 4, 6, 8]

This can also be written as a list comprehension:

     >>> list(x for x in range(10) if is_even(x))
     [0, 2, 4, 6, 8]

*note enumerate(iter): a61. counts off the elements in the iterable,
returning 2-tuples containing the count and each element.

     >>> for item in enumerate(['subject', 'verb', 'object']):
     ...     print(item)
     (0, 'subject')
     (1, 'verb')
     (2, 'object')

*note enumerate(): a61. is often used when looping through a list and
recording the indexes at which certain conditions are met:

     f = open('data.txt', 'r')
     for i, line in enumerate(f):
         if line.strip() == '':
             print('Blank line at line #%i' % i)

*note sorted(iterable, key=None, reverse=False): 84e. collects all the
elements of the iterable into a list, sorts the list, and returns the
sorted result.  The `key' and `reverse' arguments are passed through to
the constructed list’s *note sort(): 84d. method.

     >>> import random
     >>> # Generate 8 random numbers between [0, 10000)
     >>> rand_list = random.sample(range(10000), 8)
     >>> rand_list  #doctest: +SKIP
     [769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
     >>> sorted(rand_list)  #doctest: +SKIP
     [769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
     >>> sorted(rand_list, reverse=True)  #doctest: +SKIP
     [9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]

(For a more detailed discussion of sorting, see the *note Sorting HOW
TO: fb8.)

The *note any(iter): 9fd. and *note all(iter): 9fe. built-ins look at
the truth values of an iterable’s contents.  *note any(): 9fd. returns
‘True’ if any element in the iterable is a true value, and *note all():
9fe. returns ‘True’ if all of the elements are true values:

     >>> any([0,1,0])
     True
     >>> any([0,0,0])
     False
     >>> any([1,1,1])
     True
     >>> all([0,1,0])
     False
     >>> all([0,0,0])
     False
     >>> all([1,1,1])
     True

*note zip(iterA, iterB, ...): 897. takes one element from each iterable
and returns them in a tuple:

     zip(['a', 'b', 'c'], (1, 2, 3)) =>
       ('a', 1), ('b', 2), ('c', 3)

It doesn’t construct an in-memory list and exhaust all the input
iterators before returning; instead tuples are constructed and returned
only if they’re requested.  (The technical term for this behaviour is
lazy evaluation(1).)

This iterator is intended to be used with iterables that are all of the
same length.  If the iterables are of different lengths, the resulting
stream will be the same length as the shortest iterable.

     zip(['a', 'b'], (1, 2, 3)) =>
       ('a', 1), ('b', 2)

You should avoid doing this, though, because an element may be taken
from the longer iterators and discarded.  This means you can’t go on to
use the iterators further because you risk skipping a discarded element.

   ---------- Footnotes ----------

   (1) https://en.wikipedia.org/wiki/Lazy_evaluation


File: python.info,  Node: The itertools module,  Next: The functools module,  Prev: Built-in functions,  Up: Functional Programming HOWTO

10.5.6 The itertools module
---------------------------

The *note itertools: a1. module contains a number of commonly-used
iterators as well as functions for combining several iterators.  This
section will introduce the module’s contents by showing small examples.

The module’s functions fall into a few broad classes:

   * Functions that create a new iterator based on an existing iterator.

   * Functions for treating an iterator’s elements as function
     arguments.

   * Functions for selecting portions of an iterator’s output.

   * A function for grouping an iterator’s output.

* Menu:

* Creating new iterators:: 
* Calling functions on elements:: 
* Selecting elements:: 
* Combinatoric functions:: 
* Grouping elements:: 


File: python.info,  Node: Creating new iterators,  Next: Calling functions on elements,  Up: The itertools module

10.5.6.1 Creating new iterators
...............................

*note itertools.count(n): 87f. returns an infinite stream of integers,
increasing by 1 each time.  You can optionally supply the starting
number, which defaults to 0:

     itertools.count() =>
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
     itertools.count(10) =>
       10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

*note itertools.cycle(iter): 14c6. saves a copy of the contents of a
provided iterable and returns a new iterator that returns its elements
from first to last.  The new iterator will repeat these elements
infinitely.

     itertools.cycle([1,2,3,4,5]) =>
       1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...

*note itertools.repeat(elem, [n]): 14c7. returns the provided element
`n' times, or returns the element endlessly if `n' is not provided.

     itertools.repeat('abc') =>
       abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
     itertools.repeat('abc', 5) =>
       abc, abc, abc, abc, abc

*note itertools.chain(iterA, iterB, ...): fba. takes an arbitrary number
of iterables as input, and returns all the elements of the first
iterator, then all the elements of the second, and so on, until all of
the iterables have been exhausted.

     itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
       a, b, c, 1, 2, 3

*note itertools.islice(iter, [start], stop, [step]): a0d. returns a
stream that’s a slice of the iterator.  With a single `stop' argument,
it will return the first `stop' elements.  If you supply a starting
index, you’ll get `stop-start' elements, and if you supply a value for
`step', elements will be skipped accordingly.  Unlike Python’s string
and list slicing, you can’t use negative values for `start', `stop', or
`step'.

     itertools.islice(range(10), 8) =>
       0, 1, 2, 3, 4, 5, 6, 7
     itertools.islice(range(10), 2, 8) =>
       2, 3, 4, 5, 6, 7
     itertools.islice(range(10), 2, 8, 2) =>
       2, 4, 6

*note itertools.tee(iter, [n]): 14cc. replicates an iterator; it returns
`n' independent iterators that will all return the contents of the
source iterator.  If you don’t supply a value for `n', the default is 2.
Replicating iterators requires saving some of the contents of the source
iterator, so this can consume significant memory if the iterator is
large and one of the new iterators is consumed more than the others.

     itertools.tee( itertools.count() ) =>
        iterA, iterB

     where iterA ->
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

     and   iterB ->
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...


File: python.info,  Node: Calling functions on elements,  Next: Selecting elements,  Prev: Creating new iterators,  Up: The itertools module

10.5.6.2 Calling functions on elements
......................................

The *note operator: c0. module contains a set of functions corresponding
to Python’s operators.  Some examples are *note operator.add(a, b):
14f1. (adds two values), *note operator.ne(a, b): 14e1. (same as ‘a !=
b’), and *note operator.attrgetter(’id’): 2d7. (returns a callable that
fetches the ‘.id’ attribute).

*note itertools.starmap(func, iter): 659. assumes that the iterable will
return a stream of tuples, and calls `func' using these tuples as the
arguments:

     itertools.starmap(os.path.join,
                       [('/bin', 'python'), ('/usr', 'bin', 'java'),
                        ('/usr', 'bin', 'perl'), ('/usr', 'bin', 'ruby')])
     =>
       /bin/python, /usr/bin/java, /usr/bin/perl, /usr/bin/ruby


File: python.info,  Node: Selecting elements,  Next: Combinatoric functions,  Prev: Calling functions on elements,  Up: The itertools module

10.5.6.3 Selecting elements
...........................

Another group of functions chooses a subset of an iterator’s elements
based on a predicate.

*note itertools.filterfalse(predicate, iter): fa3. is the opposite of
*note filter(): 893, returning all elements for which the predicate
returns false:

     itertools.filterfalse(is_even, itertools.count()) =>
       1, 3, 5, 7, 9, 11, 13, 15, ...

*note itertools.takewhile(predicate, iter): 14cb. returns elements for
as long as the predicate returns true.  Once the predicate returns
false, the iterator will signal the end of its results.

     def less_than_10(x):
         return x < 10

     itertools.takewhile(less_than_10, itertools.count()) =>
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9

     itertools.takewhile(is_even, itertools.count()) =>
       0

*note itertools.dropwhile(predicate, iter): 14c9. discards elements
while the predicate returns true, and then returns the rest of the
iterable’s results.

     itertools.dropwhile(less_than_10, itertools.count()) =>
       10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

     itertools.dropwhile(is_even, itertools.count()) =>
       1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...

*note itertools.compress(data, selectors): 87e. takes two iterators and
returns only those elements of `data' for which the corresponding
element of `selectors' is true, stopping whenever either one is
exhausted:

     itertools.compress([1,2,3,4,5], [True, True, False, False, True]) =>
        1, 2, 5


File: python.info,  Node: Combinatoric functions,  Next: Grouping elements,  Prev: Selecting elements,  Up: The itertools module

10.5.6.4 Combinatoric functions
...............................

The *note itertools.combinations(iterable, r): 919. returns an iterator
giving all possible `r'-tuple combinations of the elements contained in
`iterable'.

     itertools.combinations([1, 2, 3, 4, 5], 2) =>
       (1, 2), (1, 3), (1, 4), (1, 5),
       (2, 3), (2, 4), (2, 5),
       (3, 4), (3, 5),
       (4, 5)

     itertools.combinations([1, 2, 3, 4, 5], 3) =>
       (1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 3, 4), (1, 3, 5), (1, 4, 5),
       (2, 3, 4), (2, 3, 5), (2, 4, 5),
       (3, 4, 5)

The elements within each tuple remain in the same order as `iterable'
returned them.  For example, the number 1 is always before 2, 3, 4, or 5
in the examples above.  A similar function, *note
itertools.permutations(iterable, r=None): 14cd, removes this constraint
on the order, returning all possible arrangements of length `r':

     itertools.permutations([1, 2, 3, 4, 5], 2) =>
       (1, 2), (1, 3), (1, 4), (1, 5),
       (2, 1), (2, 3), (2, 4), (2, 5),
       (3, 1), (3, 2), (3, 4), (3, 5),
       (4, 1), (4, 2), (4, 3), (4, 5),
       (5, 1), (5, 2), (5, 3), (5, 4)

     itertools.permutations([1, 2, 3, 4, 5]) =>
       (1, 2, 3, 4, 5), (1, 2, 3, 5, 4), (1, 2, 4, 3, 5),
       ...
       (5, 4, 3, 2, 1)

If you don’t supply a value for `r' the length of the iterable is used,
meaning that all the elements are permuted.

Note that these functions produce all of the possible combinations by
position and don’t require that the contents of `iterable' are unique:

     itertools.permutations('aba', 3) =>
       ('a', 'b', 'a'), ('a', 'a', 'b'), ('b', 'a', 'a'),
       ('b', 'a', 'a'), ('a', 'a', 'b'), ('a', 'b', 'a')

The identical tuple ‘('a', 'a', 'b')’ occurs twice, but the two ’a’
strings came from different positions.

The *note itertools.combinations_with_replacement(iterable, r): 87d.
function relaxes a different constraint: elements can be repeated within
a single tuple.  Conceptually an element is selected for the first
position of each tuple and then is replaced before the second element is
selected.

     itertools.combinations_with_replacement([1, 2, 3, 4, 5], 2) =>
       (1, 1), (1, 2), (1, 3), (1, 4), (1, 5),
       (2, 2), (2, 3), (2, 4), (2, 5),
       (3, 3), (3, 4), (3, 5),
       (4, 4), (4, 5),
       (5, 5)


File: python.info,  Node: Grouping elements,  Prev: Combinatoric functions,  Up: The itertools module

10.5.6.5 Grouping elements
..........................

The last function I’ll discuss, *note itertools.groupby(iter,
key_func=None): 14ca, is the most complicated.  ‘key_func(elem)’ is a
function that can compute a key value for each element returned by the
iterable.  If you don’t supply a key function, the key is simply each
element itself.

*note groupby(): 14ca. collects all the consecutive elements from the
underlying iterable that have the same key value, and returns a stream
of 2-tuples containing a key value and an iterator for the elements with
that key.

     city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
                  ('Anchorage', 'AK'), ('Nome', 'AK'),
                  ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
                  ...
                 ]

     def get_state(city_state):
         return city_state[1]

     itertools.groupby(city_list, get_state) =>
       ('AL', iterator-1),
       ('AK', iterator-2),
       ('AZ', iterator-3), ...

     where
     iterator-1 =>
       ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
     iterator-2 =>
       ('Anchorage', 'AK'), ('Nome', 'AK')
     iterator-3 =>
       ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

*note groupby(): 14ca. assumes that the underlying iterable’s contents
will already be sorted based on the key.  Note that the returned
iterators also use the underlying iterable, so you have to consume the
results of iterator-1 before requesting iterator-2 and its corresponding
key.


File: python.info,  Node: The functools module,  Next: Small functions and the lambda expression,  Prev: The itertools module,  Up: Functional Programming HOWTO

10.5.7 The functools module
---------------------------

The *note functools: 84. module in Python 2.5 contains some higher-order
functions.  A `higher-order function' takes one or more functions as
input and returns a new function.  The most useful tool in this module
is the *note functools.partial(): 3b7. function.

For programs written in a functional style, you’ll sometimes want to
construct variants of existing functions that have some of the
parameters filled in.  Consider a Python function ‘f(a, b, c)’; you may
wish to create a new function ‘g(b, c)’ that’s equivalent to ‘f(1, b,
c)’; you’re filling in a value for one of ‘f()’’s parameters.  This is
called "partial function application".

The constructor for *note partial(): 3b7. takes the arguments
‘(function, arg1, arg2, ..., kwarg1=value1, kwarg2=value2)’.  The
resulting object is callable, so you can just call it to invoke
‘function’ with the filled-in arguments.

Here’s a small but realistic example:

     import functools

     def log(message, subsystem):
         """Write the contents of 'message' to the specified subsystem."""
         print('%s: %s' % (subsystem, message))
         ...

     server_log = functools.partial(log, subsystem='server')
     server_log('Unable to open socket')

*note functools.reduce(func, iter, [initial_value]): 8db. cumulatively
performs an operation on all the iterable’s elements and, therefore,
can’t be applied to infinite iterables.  `func' must be a function that
takes two elements and returns a single value.  *note
functools.reduce(): 8db. takes the first two elements A and B returned
by the iterator and calculates ‘func(A, B)’.  It then requests the third
element, C, calculates ‘func(func(A, B), C)’, combines this result with
the fourth element returned, and continues until the iterable is
exhausted.  If the iterable returns no values at all, a *note TypeError:
562. exception is raised.  If the initial value is supplied, it’s used
as a starting point and ‘func(initial_value, A)’ is the first
calculation.

     >>> import operator, functools
     >>> functools.reduce(operator.concat, ['A', 'BB', 'C'])
     'ABBC'
     >>> functools.reduce(operator.concat, [])
     Traceback (most recent call last):
       ...
     TypeError: reduce() of empty sequence with no initial value
     >>> functools.reduce(operator.mul, [1,2,3], 1)
     6
     >>> functools.reduce(operator.mul, [], 1)
     1

If you use *note operator.add(): 14f1. with *note functools.reduce():
8db, you’ll add up all the elements of the iterable.  This case is so
common that there’s a special built-in called *note sum(): a60. to
compute it:

     >>> import functools
     >>> functools.reduce(operator.add, [1,2,3,4], 0)
     10
     >>> sum([1,2,3,4])
     10
     >>> sum([])
     0

For many uses of *note functools.reduce(): 8db, though, it can be
clearer to just write the obvious *note for: 895. loop:

     import functools
     # Instead of:
     product = functools.reduce(operator.mul, [1,2,3], 1)

     # You can write:
     product = 1
     for i in [1,2,3]:
         product *= i

A related function is ‘itertools.accumulate(iterable, func=operator.add)
<itertools.accumulate’.  It performs the same calculation, but instead
of returning only the final result, ‘accumulate()’ returns an iterator
that also yields each partial result:

     itertools.accumulate([1,2,3,4,5]) =>
       1, 3, 6, 10, 15

     itertools.accumulate([1,2,3,4,5], operator.mul) =>
       1, 2, 6, 24, 120

* Menu:

* The operator module:: 


File: python.info,  Node: The operator module,  Up: The functools module

10.5.7.1 The operator module
............................

The *note operator: c0. module was mentioned earlier.  It contains a set
of functions corresponding to Python’s operators.  These functions are
often useful in functional-style code because they save you from writing
trivial functions that perform a single operation.

Some of the functions in this module are:

   * Math operations: ‘add()’, ‘sub()’, ‘mul()’, ‘floordiv()’, ‘abs()’,
     ...

   * Logical operations: ‘not_()’, ‘truth()’.

   * Bitwise operations: ‘and_()’, ‘or_()’, ‘invert()’.

   * Comparisons: ‘eq()’, ‘ne()’, ‘lt()’, ‘le()’, ‘gt()’, and ‘ge()’.

   * Object identity: ‘is_()’, ‘is_not()’.

Consult the operator module’s documentation for a complete list.


File: python.info,  Node: Small functions and the lambda expression,  Next: Revision History and Acknowledgements,  Prev: The functools module,  Up: Functional Programming HOWTO

10.5.8 Small functions and the lambda expression
------------------------------------------------

When writing functional-style programs, you’ll often need little
functions that act as predicates or that combine elements in some way.

If there’s a Python built-in or a module function that’s suitable, you
don’t need to define a new function at all:

     stripped_lines = [line.strip() for line in lines]
     existing_files = filter(os.path.exists, file_list)

If the function you need doesn’t exist, you need to write it.  One way
to write small functions is to use the *note lambda: 894. statement.
‘lambda’ takes a number of parameters and an expression combining these
parameters, and creates an anonymous function that returns the value of
the expression:

     adder = lambda x, y: x+y

     print_assign = lambda name, value: name + '=' + str(value)

An alternative is to just use the ‘def’ statement and define a function
in the usual way:

     def adder(x, y):
         return x + y

     def print_assign(name, value):
         return name + '=' + str(value)

Which alternative is preferable?  That’s a style question; my usual
course is to avoid using ‘lambda’.

One reason for my preference is that ‘lambda’ is quite limited in the
functions it can define.  The result has to be computable as a single
expression, which means you can’t have multiway ‘if... elif... else’
comparisons or ‘try... except’ statements.  If you try to do too much in
a ‘lambda’ statement, you’ll end up with an overly complicated
expression that’s hard to read.  Quick, what’s the following code doing?

     import functools
     total = functools.reduce(lambda a, b: (0, a[1] + b[1]), items)[1]

You can figure it out, but it takes time to disentangle the expression
to figure out what’s going on.  Using a short nested ‘def’ statements
makes things a little bit better:

     import functools
     def combine(a, b):
         return 0, a[1] + b[1]

     total = functools.reduce(combine, items)[1]

But it would be best of all if I had simply used a ‘for’ loop:

     total = 0
     for a, b in items:
         total += b

Or the *note sum(): a60. built-in and a generator expression:

     total = sum(b for a,b in items)

Many uses of *note functools.reduce(): 8db. are clearer when written as
‘for’ loops.

Fredrik Lundh once suggested the following set of rules for refactoring
uses of ‘lambda’:

  1. Write a lambda function.

  2. Write a comment explaining what the heck that lambda does.

  3. Study the comment for a while, and think of a name that captures
     the essence of the comment.

  4. Convert the lambda to a def statement, using that name.

  5. Remove the comment.

I really like these rules, but you’re free to disagree about whether
this lambda-free style is better.


File: python.info,  Node: Revision History and Acknowledgements,  Next: References<2>,  Prev: Small functions and the lambda expression,  Up: Functional Programming HOWTO

10.5.9 Revision History and Acknowledgements
--------------------------------------------

The author would like to thank the following people for offering
suggestions, corrections and assistance with various drafts of this
article: Ian Bicking, Nick Coghlan, Nick Efford, Raymond Hettinger, Jim
Jewett, Mike Krell, Leandro Lameiro, Jussi Salmela, Collin Winter, Blake
Winton.

Version 0.1: posted June 30 2006.

Version 0.11: posted July 1 2006.  Typo fixes.

Version 0.2: posted July 10 2006.  Merged genexp and listcomp sections
into one.  Typo fixes.

Version 0.21: Added more references suggested on the tutor mailing list.

Version 0.30: Adds a section on the ‘functional’ module written by
Collin Winter; adds short section on the operator module; a few other
edits.


File: python.info,  Node: References<2>,  Prev: Revision History and Acknowledgements,  Up: Functional Programming HOWTO

10.5.10 References
------------------

* Menu:

* General:: 
* Python-specific:: 
* Python documentation:: 


File: python.info,  Node: General,  Next: Python-specific,  Up: References<2>

10.5.10.1 General
.................

`Structure and Interpretation of Computer Programs', by Harold Abelson
and Gerald Jay Sussman with Julie Sussman.  Full text at
‘https://mitpress.mit.edu/sicp/’.  In this classic textbook of computer
science, chapters 2 and 3 discuss the use of sequences and streams to
organize the data flow inside a program.  The book uses Scheme for its
examples, but many of the design approaches described in these chapters
are applicable to functional-style Python code.

‘http://www.defmacro.org/ramblings/fp.html’: A general introduction to
functional programming that uses Java examples and has a lengthy
historical introduction.

‘https://en.wikipedia.org/wiki/Functional_programming’: General
Wikipedia entry describing functional programming.

‘https://en.wikipedia.org/wiki/Coroutine’: Entry for coroutines.

‘https://en.wikipedia.org/wiki/Currying’: Entry for the concept of
currying.


File: python.info,  Node: Python-specific,  Next: Python documentation,  Prev: General,  Up: References<2>

10.5.10.2 Python-specific
.........................

‘http://gnosis.cx/TPiP/’: The first chapter of David Mertz’s book ‘Text
Processing in Python’ discusses functional programming for text
processing, in the section titled "Utilizing Higher-Order Functions in
Text Processing".

Mertz also wrote a 3-part series of articles on functional programming
for IBM’s DeveloperWorks site; see part 1(1), part 2(2), and part 3(3),

   ---------- Footnotes ----------

   (1) http://www.ibm.com/developerworks/linux/library/l-prog/index.html

   (2) 
http://www.ibm.com/developerworks/linux/library/l-prog2/index.html

   (3) 
http://www.ibm.com/developerworks/linux/library/l-prog3/index.html


File: python.info,  Node: Python documentation,  Prev: Python-specific,  Up: References<2>

10.5.10.3 Python documentation
..............................

Documentation for the *note itertools: a1. module.

Documentation for the *note operator: c0. module.

PEP 289(1): "Generator Expressions"

PEP 342(2): "Coroutines via Enhanced Generators" describes the new
generator features in Python 2.5.

   ---------- Footnotes ----------

   (1) https://www.python.org/dev/peps/pep-0289

   (2) https://www.python.org/dev/peps/pep-0342


File: python.info,  Node: Logging HOWTO,  Next: Logging Cookbook,  Prev: Functional Programming HOWTO,  Up: Python HOWTOs

10.6 Logging HOWTO
==================


Author: Vinay Sajip <vinay_sajip at red-dove dot com>

* Menu:

* Basic Logging Tutorial:: 
* Advanced Logging Tutorial:: 
* Logging Levels: Logging Levels<2>. 
* Useful Handlers:: 
* Exceptions raised during logging:: 
* Using arbitrary objects as messages:: 
* Optimization:: 


File: python.info,  Node: Basic Logging Tutorial,  Next: Advanced Logging Tutorial,  Up: Logging HOWTO

10.6.1 Basic Logging Tutorial
-----------------------------

Logging is a means of tracking events that happen when some software
runs.  The software’s developer adds logging calls to their code to
indicate that certain events have occurred.  An event is described by a
descriptive message which can optionally contain variable data (i.e.
data that is potentially different for each occurrence of the event).
Events also have an importance which the developer ascribes to the
event; the importance can also be called the `level' or `severity'.

* Menu:

* When to use logging:: 
* A simple example:: 
* Logging to a file:: 
* Logging from multiple modules:: 
* Logging variable data:: 
* Changing the format of displayed messages:: 
* Displaying the date/time in messages:: 
* Next Steps:: 


File: python.info,  Node: When to use logging,  Next: A simple example,  Up: Basic Logging Tutorial

10.6.1.1 When to use logging
............................

Logging provides a set of convenience functions for simple logging
usage.  These are *note debug(): 19dc, *note info(): 1a07, *note
warning(): 1a19, *note error(): 1a1a. and *note critical(): 1a1b.  To
determine when to use logging, see the table below, which states, for
each of a set of common tasks, the best tool to use for it.

Task you want to perform                  The best tool for the task
                                          
-------------------------------------------------------------------------------------
                                          
Display console output for ordinary       *note print(): 481.
usage of a command line script or         
program

Report events that occur during normal    *note logging.info(): 1a07. (or
operation of a program (e.g.  for         *note logging.debug(): 19dc. for very
status monitoring or fault                detailed output for diagnostic purposes)
investigation)                            

Issue a warning regarding a particular    *note warnings.warn(): add. in library
runtime event                             code if the issue is avoidable and the
                                          client application should be modified to
                                          eliminate the warning
                                          
                                          *note logging.warning(): 1a19. if there
                                          is nothing the client application can do
                                          about the situation, but the event
                                          should still be noted
                                          
                                          
Report an error regarding a particular    Raise an exception
runtime event                             

Report suppression of an error without    *note logging.error(): 1a1a,
raising an exception (e.g.  error         *note logging.exception(): 1a1c. or
handler in a long-running server          *note logging.critical(): 1a1b. as
process)                                  appropriate for the specific error and
                                          application domain
                                          

The logging functions are named after the level or severity of the
events they are used to track.  The standard levels and their
applicability are described below (in increasing order of severity):

Level              When it’s used
                   
---------------------------------------------------------------------
                   
‘DEBUG’            Detailed information, typically of interest
                   only when diagnosing problems.
                   
                   
‘INFO’             Confirmation that things are working as
                   expected.
                   
                   
‘WARNING’          An indication that something unexpected
                   happened, or indicative of some problem in the
                   near future (e.g.  ’disk space low’).  The
                   software is still working as expected.
                   
                   
‘ERROR’            Due to a more serious problem, the software has
                   not been able to perform some function.
                   
                   
‘CRITICAL’         A serious error, indicating that the program
                   itself may be unable to continue running.
                   

The default level is ‘WARNING’, which means that only events of this
level and above will be tracked, unless the logging package is
configured to do otherwise.

Events that are tracked can be handled in different ways.  The simplest
way of handling tracked events is to print them to the console.  Another
common way is to write them to a disk file.


File: python.info,  Node: A simple example,  Next: Logging to a file,  Prev: When to use logging,  Up: Basic Logging Tutorial

10.6.1.2 A simple example
.........................

A very simple example is:

     import logging
     logging.warning('Watch out!') # will print a message to the console
     logging.info('I told you so') # will not print anything

If you type these lines into a script and run it, you’ll see:

     WARNING:root:Watch out!

printed out on the console.  The ‘INFO’ message doesn’t appear because
the default level is ‘WARNING’.  The printed message includes the
indication of the level and the description of the event provided in the
logging call, i.e.  ’Watch out!’.  Don’t worry about the ’root’ part for
now: it will be explained later.  The actual output can be formatted
quite flexibly if you need that; formatting options will also be
explained later.


File: python.info,  Node: Logging to a file,  Next: Logging from multiple modules,  Prev: A simple example,  Up: Basic Logging Tutorial

10.6.1.3 Logging to a file
..........................

A very common situation is that of recording logging events in a file,
so let’s look at that next.  Be sure to try the following in a
newly-started Python interpreter, and don’t just continue from the
session described above:

     import logging
     logging.basicConfig(filename='example.log',level=logging.DEBUG)
     logging.debug('This message should go to the log file')
     logging.info('So should this')
     logging.warning('And this, too')

And now if we open the file and look at what we have, we should find the
log messages:

     DEBUG:root:This message should go to the log file
     INFO:root:So should this
     WARNING:root:And this, too

This example also shows how you can set the logging level which acts as
the threshold for tracking.  In this case, because we set the threshold
to ‘DEBUG’, all of the messages were printed.

If you want to set the logging level from a command-line option such as:

     --log=INFO

and you have the value of the parameter passed for ‘--log’ in some
variable `loglevel', you can use:

     getattr(logging, loglevel.upper())

to get the value which you’ll pass to *note basicConfig(): 64c. via the
`level' argument.  You may want to error check any user input value,
perhaps as in the following example:

     # assuming loglevel is bound to the string value obtained from the
     # command line argument. Convert to upper case to allow the user to
     # specify --log=DEBUG or --log=debug
     numeric_level = getattr(logging, loglevel.upper(), None)
     if not isinstance(numeric_level, int):
         raise ValueError('Invalid log level: %s' % loglevel)
     logging.basicConfig(level=numeric_level, ...)

The call to *note basicConfig(): 64c. should come `before' any calls to
*note debug(): 19dc, *note info(): 1a07. etc.  As it’s intended as a
one-off simple configuration facility, only the first call will actually
do anything: subsequent calls are effectively no-ops.

If you run the above script several times, the messages from successive
runs are appended to the file `example.log'.  If you want each run to
start afresh, not remembering the messages from earlier runs, you can
specify the `filemode' argument, by changing the call in the above
example to:

     logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)

The output will be the same as before, but the log file is no longer
appended to, so the messages from earlier runs are lost.


File: python.info,  Node: Logging from multiple modules,  Next: Logging variable data,  Prev: Logging to a file,  Up: Basic Logging Tutorial

10.6.1.4 Logging from multiple modules
......................................

If your program consists of multiple modules, here’s an example of how
you could organize logging in it:

     # myapp.py
     import logging
     import mylib

     def main():
         logging.basicConfig(filename='myapp.log', level=logging.INFO)
         logging.info('Started')
         mylib.do_something()
         logging.info('Finished')

     if __name__ == '__main__':
         main()

     # mylib.py
     import logging

     def do_something():
         logging.info('Doing something')

If you run `myapp.py', you should see this in `myapp.log':

     INFO:root:Started
     INFO:root:Doing something
     INFO:root:Finished

which is hopefully what you were expecting to see.  You can generalize
this to multiple modules, using the pattern in `mylib.py'.  Note that
for this simple usage pattern, you won’t know, by looking in the log
file, `where' in your application your messages came from, apart from
looking at the event description.  If you want to track the location of
your messages, you’ll need to refer to the documentation beyond the
tutorial level – see *note Advanced Logging Tutorial: 7c2.


File: python.info,  Node: Logging variable data,  Next: Changing the format of displayed messages,  Prev: Logging from multiple modules,  Up: Basic Logging Tutorial

10.6.1.5 Logging variable data
..............................

To log variable data, use a format string for the event description
message and append the variable data as arguments.  For example:

     import logging
     logging.warning('%s before you %s', 'Look', 'leap!')

will display:

     WARNING:root:Look before you leap!

As you can see, merging of variable data into the event description
message uses the old, %-style of string formatting.  This is for
backwards compatibility: the logging package pre-dates newer formatting
options such as *note str.format(): 14d. and *note string.Template: 7c4.
These newer formatting options `are' supported, but exploring them is
outside the scope of this tutorial: see *note Using particular
formatting styles throughout your application: 19fe. for more
information.


File: python.info,  Node: Changing the format of displayed messages,  Next: Displaying the date/time in messages,  Prev: Logging variable data,  Up: Basic Logging Tutorial

10.6.1.6 Changing the format of displayed messages
..................................................

To change the format which is used to display messages, you need to
specify the format you want to use:

     import logging
     logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
     logging.debug('This message should appear on the console')
     logging.info('So should this')
     logging.warning('And this, too')

which would print:

     DEBUG:This message should appear on the console
     INFO:So should this
     WARNING:And this, too

Notice that the ’root’ which appeared in earlier examples has
disappeared.  For a full set of things that can appear in format
strings, you can refer to the documentation for *note LogRecord
attributes: 19fd, but for simple usage, you just need the `levelname'
(severity), `message' (event description, including variable data) and
perhaps to display when the event occurred.  This is described in the
next section.


File: python.info,  Node: Displaying the date/time in messages,  Next: Next Steps,  Prev: Changing the format of displayed messages,  Up: Basic Logging Tutorial

10.6.1.7 Displaying the date/time in messages
.............................................

To display the date and time of an event, you would place ’%(asctime)s’
in your format string:

     import logging
     logging.basicConfig(format='%(asctime)s %(message)s')
     logging.warning('is when this event was logged.')

which should print something like this:

     2010-12-12 11:41:42,612 is when this event was logged.

The default format for date/time display (shown above) is ISO8601.  If
you need more control over the formatting of the date/time, provide a
`datefmt' argument to ‘basicConfig’, as in this example:

     import logging
     logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
     logging.warning('is when this event was logged.')

which would display something like this:

     12/12/2010 11:46:36 AM is when this event was logged.

The format of the `datefmt' argument is the same as supported by *note
time.strftime(): 7b1.


File: python.info,  Node: Next Steps,  Prev: Displaying the date/time in messages,  Up: Basic Logging Tutorial

10.6.1.8 Next Steps
...................

That concludes the basic tutorial.  It should be enough to get you up
and running with logging.  There’s a lot more that the logging package
offers, but to get the best out of it, you’ll need to invest a little
more of your time in reading the following sections.  If you’re ready
for that, grab some of your favourite beverage and carry on.

If your logging needs are simple, then use the above examples to
incorporate logging into your own scripts, and if you run into problems
or don’t understand something, please post a question on the
comp.lang.python Usenet group (available at
‘https://groups.google.com/group/comp.lang.python’) and you should
receive help before too long.

Still here?  You can carry on reading the next few sections, which
provide a slightly more advanced/in-depth tutorial than the basic one
above.  After that, you can take a look at the *note Logging Cookbook:
7c3.


File: python.info,  Node: Advanced Logging Tutorial,  Next: Logging Levels<2>,  Prev: Basic Logging Tutorial,  Up: Logging HOWTO

10.6.2 Advanced Logging Tutorial
--------------------------------

The logging library takes a modular approach and offers several
categories of components: loggers, handlers, filters, and formatters.

   * Loggers expose the interface that application code directly uses.

   * Handlers send the log records (created by loggers) to the
     appropriate destination.

   * Filters provide a finer grained facility for determining which log
     records to output.

   * Formatters specify the layout of log records in the final output.

Log event information is passed between loggers, handlers, filters and
formatters in a *note LogRecord: 508. instance.

Logging is performed by calling methods on instances of the *note
Logger: 2c6. class (hereafter called `loggers').  Each instance has a
name, and they are conceptually arranged in a namespace hierarchy using
dots (periods) as separators.  For example, a logger named ’scan’ is the
parent of loggers ’scan.text’, ’scan.html’ and ’scan.pdf’.  Logger names
can be anything you want, and indicate the area of an application in
which a logged message originates.

A good convention to use when naming loggers is to use a module-level
logger, in each module which uses logging, named as follows:

     logger = logging.getLogger(__name__)

This means that logger names track the package/module hierarchy, and
it’s intuitively obvious where events are logged just from the logger
name.

The root of the hierarchy of loggers is called the root logger.  That’s
the logger used by the functions *note debug(): 19dc, *note info():
1a07, *note warning(): 1a19, *note error(): 1a1a. and *note critical():
1a1b, which just call the same-named method of the root logger.  The
functions and the methods have the same signatures.  The root logger’s
name is printed as ’root’ in the logged output.

It is, of course, possible to log messages to different destinations.
Support is included in the package for writing log messages to files,
HTTP GET/POST locations, email via SMTP, generic sockets, queues, or
OS-specific logging mechanisms such as syslog or the Windows NT event
log.  Destinations are served by `handler' classes.  You can create your
own log destination class if you have special requirements not met by
any of the built-in handler classes.

By default, no destination is set for any logging messages.  You can
specify a destination (such as console or file) by using *note
basicConfig(): 64c. as in the tutorial examples.  If you call the
functions *note debug(): 19dc, *note info(): 1a07, *note warning():
1a19, *note error(): 1a1a. and *note critical(): 1a1b, they will check
to see if no destination is set; and if one is not set, they will set a
destination of the console (‘sys.stderr’) and a default format for the
displayed message before delegating to the root logger to do the actual
message output.

The default format set by *note basicConfig(): 64c. for messages is:

     severity:logger name:message

You can change this by passing a format string to *note basicConfig():
64c. with the `format' keyword argument.  For all options regarding how
a format string is constructed, see *note Formatter Objects: 19fb.

* Menu:

* Logging Flow:: 
* Loggers:: 
* Handlers:: 
* Formatters:: 
* Configuring Logging:: 
* What happens if no configuration is provided:: 
* Configuring Logging for a Library:: 


File: python.info,  Node: Logging Flow,  Next: Loggers,  Up: Advanced Logging Tutorial

10.6.2.1 Logging Flow
.....................

The flow of log event information in loggers and handlers is illustrated
in the following diagram.

 [image src="logging_flow.png" ]


File: python.info,  Node: Loggers,  Next: Handlers,  Prev: Logging Flow,  Up: Advanced Logging Tutorial

10.6.2.2 Loggers
................

*note Logger: 2c6. objects have a threefold job.  First, they expose
several methods to application code so that applications can log
messages at runtime.  Second, logger objects determine which log
messages to act upon based upon severity (the default filtering
facility) or filter objects.  Third, logger objects pass along relevant
log messages to all interested log handlers.

The most widely used methods on logger objects fall into two categories:
configuration and message sending.

These are the most common configuration methods:

   * *note Logger.setLevel(): 19d6. specifies the lowest-severity log
     message a logger will handle, where debug is the lowest built-in
     severity level and critical is the highest built-in severity.  For
     example, if the severity level is INFO, the logger will handle only
     INFO, WARNING, ERROR, and CRITICAL messages and will ignore DEBUG
     messages.

   * *note Logger.addHandler(): 19e2. and *note Logger.removeHandler():
     19e3. add and remove handler objects from the logger object.
     Handlers are covered in more detail in *note Handlers: 388f.

   * *note Logger.addFilter(): 19df. and *note Logger.removeFilter():
     19e0. add and remove filter objects from the logger object.
     Filters are covered in more detail in *note Filter Objects: 1a04.

You don’t need to always call these methods on every logger you create.
See the last two paragraphs in this section.

With the logger object configured, the following methods create log
messages:

   * *note Logger.debug(): 2ca, *note Logger.info(): 19db, *note
     Logger.warning(): 19dd, *note Logger.error(): 19de, and *note
     Logger.critical(): 2c9. all create log records with a message and a
     level that corresponds to their respective method names.  The
     message is actually a format string, which may contain the standard
     string substitution syntax of ‘%s’, ‘%d’, ‘%f’, and so on.  The
     rest of their arguments is a list of objects that correspond with
     the substitution fields in the message.  With regard to ‘**kwargs’,
     the logging methods care only about a keyword of ‘exc_info’ and use
     it to determine whether to log exception information.

   * *note Logger.exception(): 2c8. creates a log message similar to
     *note Logger.error(): 19de.  The difference is that *note
     Logger.exception(): 2c8. dumps a stack trace along with it.  Call
     this method only from an exception handler.

   * *note Logger.log(): 2c7. takes a log level as an explicit argument.
     This is a little more verbose for logging messages than using the
     log level convenience methods listed above, but this is how to log
     at custom log levels.

*note getLogger(): 19d4. returns a reference to a logger instance with
the specified name if it is provided, or ‘root’ if not.  The names are
period-separated hierarchical structures.  Multiple calls to *note
getLogger(): 19d4. with the same name will return a reference to the
same logger object.  Loggers that are further down in the hierarchical
list are children of loggers higher up in the list.  For example, given
a logger with a name of ‘foo’, loggers with names of ‘foo.bar’,
‘foo.bar.baz’, and ‘foo.bam’ are all descendants of ‘foo’.

Loggers have a concept of `effective level'.  If a level is not
explicitly set on a logger, the level of its parent is used instead as
its effective level.  If the parent has no explicit level set, `its'
parent is examined, and so on - all ancestors are searched until an
explicitly set level is found.  The root logger always has an explicit
level set (‘WARNING’ by default).  When deciding whether to process an
event, the effective level of the logger is used to determine whether
the event is passed to the logger’s handlers.

Child loggers propagate messages up to the handlers associated with
their ancestor loggers.  Because of this, it is unnecessary to define
and configure handlers for all the loggers an application uses.  It is
sufficient to configure handlers for a top-level logger and create child
loggers as needed.  (You can, however, turn off propagation by setting
the `propagate' attribute of a logger to `False'.)


File: python.info,  Node: Handlers,  Next: Formatters,  Prev: Loggers,  Up: Advanced Logging Tutorial

10.6.2.3 Handlers
.................

‘Handler’ objects are responsible for dispatching the appropriate log
messages (based on the log messages’ severity) to the handler’s
specified destination.  *note Logger: 2c6. objects can add zero or more
handler objects to themselves with an *note addHandler(): 19e2. method.
As an example scenario, an application may want to send all log messages
to a log file, all log messages of error or higher to stdout, and all
messages of critical to an email address.  This scenario requires three
individual handlers where each handler is responsible for sending
messages of a specific severity to a specific location.

The standard library includes quite a few handler types (see *note
Useful Handlers: 3891.); the tutorials use mainly *note StreamHandler:
7c5. and *note FileHandler: 1a3f. in its examples.

There are very few methods in a handler for application developers to
concern themselves with.  The only handler methods that seem relevant
for application developers who are using the built-in handler objects
(that is, not creating custom handlers) are the following configuration
methods:

   * The *note setLevel(): 19ef. method, just as in logger objects,
     specifies the lowest severity that will be dispatched to the
     appropriate destination.  Why are there two ‘setLevel()’ methods?
     The level set in the logger determines which severity of messages
     it will pass to its handlers.  The level set in each handler
     determines which messages that handler will send on.

   * *note setFormatter(): 19f0. selects a Formatter object for this
     handler to use.

   * *note addFilter(): 19f1. and *note removeFilter(): 19f2.
     respectively configure and deconfigure filter objects on handlers.

Application code should not directly instantiate and use instances of
‘Handler’.  Instead, the ‘Handler’ class is a base class that defines
the interface that all handlers should have and establishes some default
behavior that child classes can use (or override).


File: python.info,  Node: Formatters,  Next: Configuring Logging,  Prev: Handlers,  Up: Advanced Logging Tutorial

10.6.2.4 Formatters
...................

Formatter objects configure the final order, structure, and contents of
the log message.  Unlike the base ‘logging.Handler’ class, application
code may instantiate formatter classes, although you could likely
subclass the formatter if your application needs special behavior.  The
constructor takes three optional arguments – a message format string, a
date format string and a style indicator.

 -- Method: logging.Formatter.__init__ (fmt=None, datefmt=None,
          style='%')

If there is no message format string, the default is to use the raw
message.  If there is no date format string, the default date format is:

     %Y-%m-%d %H:%M:%S

with the milliseconds tacked on at the end.  The ‘style’ is one of ‘%’,
’{’ or ’$’.  If one of these is not specified, then ’%’ will be used.

If the ‘style’ is ’%’, the message format string uses ‘%(<dictionary
key>)s’ styled string substitution; the possible keys are documented in
*note LogRecord attributes: 19fd.  If the style is ’{’, the message
format string is assumed to be compatible with *note str.format(): 14d.
(using keyword arguments), while if the style is ’$’ then the message
format string should conform to what is expected by *note
string.Template.substitute(): cbe.

Changed in version 3.2: Added the ‘style’ parameter.

The following message format string will log the time in a
human-readable format, the severity of the message, and the contents of
the message, in that order:

     '%(asctime)s - %(levelname)s - %(message)s'

Formatters use a user-configurable function to convert the creation time
of a record to a tuple.  By default, *note time.localtime(): 125d. is
used; to change this for a particular formatter instance, set the
‘converter’ attribute of the instance to a function with the same
signature as *note time.localtime(): 125d. or *note time.gmtime(): 786.
To change it for all formatters, for example if you want all logging
times to be shown in GMT, set the ‘converter’ attribute in the Formatter
class (to ‘time.gmtime’ for GMT display).


File: python.info,  Node: Configuring Logging,  Next: What happens if no configuration is provided,  Prev: Formatters,  Up: Advanced Logging Tutorial

10.6.2.5 Configuring Logging
............................

Programmers can configure logging in three ways:

  1. Creating loggers, handlers, and formatters explicitly using Python
     code that calls the configuration methods listed above.

  2. Creating a logging config file and reading it using the *note
     fileConfig(): 469. function.

  3. Creating a dictionary of configuration information and passing it
     to the *note dictConfig(): 76d. function.

For the reference documentation on the last two options, see *note
Configuration functions: 8f9.  The following example configures a very
simple logger, a console handler, and a simple formatter using Python
code:

     import logging

     # create logger
     logger = logging.getLogger('simple_example')
     logger.setLevel(logging.DEBUG)

     # create console handler and set level to debug
     ch = logging.StreamHandler()
     ch.setLevel(logging.DEBUG)

     # create formatter
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

     # add formatter to ch
     ch.setFormatter(formatter)

     # add ch to logger
     logger.addHandler(ch)

     # 'application' code
     logger.debug('debug message')
     logger.info('info message')
     logger.warn('warn message')
     logger.error('error message')
     logger.critical('critical message')

Running this module from the command line produces the following output:

     $ python simple_logging_module.py
     2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message
     2005-03-19 15:10:26,620 - simple_example - INFO - info message
     2005-03-19 15:10:26,695 - simple_example - WARNING - warn message
     2005-03-19 15:10:26,697 - simple_example - ERROR - error message
     2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message

The following Python module creates a logger, handler, and formatter
nearly identical to those in the example listed above, with the only
difference being the names of the objects:

     import logging
     import logging.config

     logging.config.fileConfig('logging.conf')

     # create logger
     logger = logging.getLogger('simpleExample')

     # 'application' code
     logger.debug('debug message')
     logger.info('info message')
     logger.warn('warn message')
     logger.error('error message')
     logger.critical('critical message')

Here is the logging.conf file:

     [loggers]
     keys=root,simpleExample

     [handlers]
     keys=consoleHandler

     [formatters]
     keys=simpleFormatter

     [logger_root]
     level=DEBUG
     handlers=consoleHandler

     [logger_simpleExample]
     level=DEBUG
     handlers=consoleHandler
     qualname=simpleExample
     propagate=0

     [handler_consoleHandler]
     class=StreamHandler
     level=DEBUG
     formatter=simpleFormatter
     args=(sys.stdout,)

     [formatter_simpleFormatter]
     format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
     datefmt=

The output is nearly identical to that of the non-config-file-based
example:

     $ python simple_logging_config.py
     2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message
     2005-03-19 15:38:55,979 - simpleExample - INFO - info message
     2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message
     2005-03-19 15:38:56,055 - simpleExample - ERROR - error message
     2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message

You can see that the config file approach has a few advantages over the
Python code approach, mainly separation of configuration and code and
the ability of noncoders to easily modify the logging properties.

     Warning: The *note fileConfig(): 469. function takes a default
     parameter, ‘disable_existing_loggers’, which defaults to ‘True’ for
     reasons of backward compatibility.  This may or may not be what you
     want, since it will cause any loggers existing before the *note
     fileConfig(): 469. call to be disabled unless they (or an ancestor)
     are explicitly named in the configuration.  Please refer to the
     reference documentation for more information, and specify ‘False’
     for this parameter if you wish.

     The dictionary passed to *note dictConfig(): 76d. can also specify
     a Boolean value with key ‘disable_existing_loggers’, which if not
     specified explicitly in the dictionary also defaults to being
     interpreted as ‘True’.  This leads to the logger-disabling
     behaviour described above, which may not be what you want - in
     which case, provide the key explicitly with a value of ‘False’.

Note that the class names referenced in config files need to be either
relative to the logging module, or absolute values which can be resolved
using normal import mechanisms.  Thus, you could use either *note
WatchedFileHandler: 1a50. (relative to the logging module) or
‘mypackage.mymodule.MyHandler’ (for a class defined in package
‘mypackage’ and module ‘mymodule’, where ‘mypackage’ is available on the
Python import path).

In Python 3.2, a new means of configuring logging has been introduced,
using dictionaries to hold configuration information.  This provides a
superset of the functionality of the config-file-based approach outlined
above, and is the recommended configuration method for new applications
and deployments.  Because a Python dictionary is used to hold
configuration information, and since you can populate that dictionary
using different means, you have more options for configuration.  For
example, you can use a configuration file in JSON format, or, if you
have access to YAML processing functionality, a file in YAML format, to
populate the configuration dictionary.  Or, of course, you can construct
the dictionary in Python code, receive it in pickled form over a socket,
or use whatever approach makes sense for your application.

Here’s an example of the same configuration as above, in YAML format for
the new dictionary-based approach:

     version: 1
     formatters:
       simple:
         format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
     handlers:
       console:
         class: logging.StreamHandler
         level: DEBUG
         formatter: simple
         stream: ext://sys.stdout
     loggers:
       simpleExample:
         level: DEBUG
         handlers: [console]
         propagate: no
     root:
       level: DEBUG
       handlers: [console]

For more information about logging using a dictionary, see *note
Configuration functions: 8f9.


File: python.info,  Node: What happens if no configuration is provided,  Next: Configuring Logging for a Library,  Prev: Configuring Logging,  Up: Advanced Logging Tutorial

10.6.2.6 What happens if no configuration is provided
.....................................................

If no logging configuration is provided, it is possible to have a
situation where a logging event needs to be output, but no handlers can
be found to output the event.  The behaviour of the logging package in
these circumstances is dependent on the Python version.

For versions of Python prior to 3.2, the behaviour is as follows:

   * If `logging.raiseExceptions' is `False' (production mode), the
     event is silently dropped.

   * If `logging.raiseExceptions' is `True' (development mode), a
     message ’No handlers could be found for logger X.Y.Z’ is printed
     once.

In Python 3.2 and later, the behaviour is as follows:

   * The event is output using a ’handler of last resort’, stored in
     ‘logging.lastResort’.  This internal handler is not associated with
     any logger, and acts like a *note StreamHandler: 7c5. which writes
     the event description message to the current value of ‘sys.stderr’
     (therefore respecting any redirections which may be in effect).  No
     formatting is done on the message - just the bare event description
     message is printed.  The handler’s level is set to ‘WARNING’, so
     all events at this and greater severities will be output.

To obtain the pre-3.2 behaviour, ‘logging.lastResort’ can be set to
`None'.


File: python.info,  Node: Configuring Logging for a Library,  Prev: What happens if no configuration is provided,  Up: Advanced Logging Tutorial

10.6.2.7 Configuring Logging for a Library
..........................................

When developing a library which uses logging, you should take care to
document how the library uses logging - for example, the names of
loggers used.  Some consideration also needs to be given to its logging
configuration.  If the using application does not use logging, and
library code makes logging calls, then (as described in the previous
section) events of severity ‘WARNING’ and greater will be printed to
‘sys.stderr’.  This is regarded as the best default behaviour.

If for some reason you `don’t' want these messages printed in the
absence of any logging configuration, you can attach a do-nothing
handler to the top-level logger for your library.  This avoids the
message being printed, since a handler will be always be found for the
library’s events: it just doesn’t produce any output.  If the library
user configures logging for application use, presumably that
configuration will add some handlers, and if levels are suitably
configured then logging calls made in library code will send output to
those handlers, as normal.

A do-nothing handler is included in the logging package: *note
NullHandler: 880. (since Python 3.1).  An instance of this handler could
be added to the top-level logger of the logging namespace used by the
library (`if' you want to prevent your library’s logged events being
output to ‘sys.stderr’ in the absence of logging configuration).  If all
logging by a library `foo' is done using loggers with names matching
’foo.x’, ’foo.x.y’, etc.  then the code:

     import logging
     logging.getLogger('foo').addHandler(logging.NullHandler())

should have the desired effect.  If an organisation produces a number of
libraries, then the logger name specified can be ’orgname.foo’ rather
than just ’foo’.

     Note: It is strongly advised that you `do not add any handlers
     other than' *note NullHandler: 880. `to your library’s loggers'.
     This is because the configuration of handlers is the prerogative of
     the application developer who uses your library.  The application
     developer knows their target audience and what handlers are most
     appropriate for their application: if you add handlers ’under the
     hood’, you might well interfere with their ability to carry out
     unit tests and deliver logs which suit their requirements.


File: python.info,  Node: Logging Levels<2>,  Next: Useful Handlers,  Prev: Advanced Logging Tutorial,  Up: Logging HOWTO

10.6.3 Logging Levels
---------------------

The numeric values of logging levels are given in the following table.
These are primarily of interest if you want to define your own levels,
and need them to have specific values relative to the predefined levels.
If you define a level with the same numeric value, it overwrites the
predefined value; the predefined name is lost.

Level              Numeric value
                   
---------------------------------------
                   
‘CRITICAL’         50
                   
                   
‘ERROR’            40
                   
                   
‘WARNING’          30
                   
                   
‘INFO’             20
                   
                   
‘DEBUG’            10
                   
                   
‘NOTSET’           0
                   

Levels can also be associated with loggers, being set either by the
developer or through loading a saved logging configuration.  When a
logging method is called on a logger, the logger compares its own level
with the level associated with the method call.  If the logger’s level
is higher than the method call’s, no logging message is actually
generated.  This is the basic mechanism controlling the verbosity of
logging output.

Logging messages are encoded as instances of the *note LogRecord: 508.
class.  When a logger decides to actually log an event, a *note
LogRecord: 508. instance is created from the logging message.

Logging messages are subjected to a dispatch mechanism through the use
of `handlers', which are instances of subclasses of the ‘Handler’ class.
Handlers are responsible for ensuring that a logged message (in the form
of a *note LogRecord: 508.) ends up in a particular location (or set of
locations) which is useful for the target audience for that message
(such as end users, support desk staff, system administrators,
developers).  Handlers are passed *note LogRecord: 508. instances
intended for particular destinations.  Each logger can have zero, one or
more handlers associated with it (via the *note addHandler(): 19e2.
method of *note Logger: 2c6.).  In addition to any handlers directly
associated with a logger, `all handlers associated with all ancestors of
the logger' are called to dispatch the message (unless the `propagate'
flag for a logger is set to a false value, at which point the passing to
ancestor handlers stops).

Just as for loggers, handlers can have levels associated with them.  A
handler’s level acts as a filter in the same way as a logger’s level
does.  If a handler decides to actually dispatch an event, the *note
emit(): 19f9. method is used to send the message to its destination.
Most user-defined subclasses of ‘Handler’ will need to override this
*note emit(): 19f9.

* Menu:

* Custom Levels:: 


File: python.info,  Node: Custom Levels,  Up: Logging Levels<2>

10.6.3.1 Custom Levels
......................

Defining your own levels is possible, but should not be necessary, as
the existing levels have been chosen on the basis of practical
experience.  However, if you are convinced that you need custom levels,
great care should be exercised when doing this, and it is possibly `a
very bad idea to define custom levels if you are developing a library'.
That’s because if multiple library authors all define their own custom
levels, there is a chance that the logging output from such multiple
libraries used together will be difficult for the using developer to
control and/or interpret, because a given numeric value might mean
different things for different libraries.


File: python.info,  Node: Useful Handlers,  Next: Exceptions raised during logging,  Prev: Logging Levels<2>,  Up: Logging HOWTO

10.6.4 Useful Handlers
----------------------

In addition to the base ‘Handler’ class, many useful subclasses are
provided:

  1. *note StreamHandler: 7c5. instances send messages to streams
     (file-like objects).

  2. *note FileHandler: 1a3f. instances send messages to disk files.

  3. *note BaseRotatingHandler: 1a55. is the base class for handlers
     that rotate log files at a certain point.  It is not meant to be
     instantiated directly.  Instead, use *note RotatingFileHandler:
     1a30. or *note TimedRotatingFileHandler: 466.

  4. *note RotatingFileHandler: 1a30. instances send messages to disk
     files, with support for maximum log file sizes and log file
     rotation.

  5. *note TimedRotatingFileHandler: 466. instances send messages to
     disk files, rotating the log file at certain timed intervals.

  6. *note SocketHandler: 467. instances send messages to TCP/IP
     sockets.  Since 3.4, Unix domain sockets are also supported.

  7. *note DatagramHandler: 468. instances send messages to UDP sockets.
     Since 3.4, Unix domain sockets are also supported.

  8. *note SMTPHandler: 1a81. instances send messages to a designated
     email address.

  9. *note SysLogHandler: 64d. instances send messages to a Unix syslog
     daemon, possibly on a remote machine.

  10. *note NTEventLogHandler: 1a79. instances send messages to a
     Windows NT/2000/XP event log.

  11. *note MemoryHandler: 1a39. instances send messages to a buffer in
     memory, which is flushed whenever specific criteria are met.

  12. *note HTTPHandler: 2cb. instances send messages to an HTTP server
     using either ‘GET’ or ‘POST’ semantics.

  13. *note WatchedFileHandler: 1a50. instances watch the file they are
     logging to.  If the file changes, it is closed and reopened using
     the file name.  This handler is only useful on Unix-like systems;
     Windows does not support the underlying mechanism used.

  14. *note QueueHandler: 1a94. instances send messages to a queue, such
     as those implemented in the *note queue: d8. or *note
     multiprocessing: b6. modules.

  15. *note NullHandler: 880. instances do nothing with error messages.
     They are used by library developers who want to use logging, but
     want to avoid the ’No handlers could be found for logger XXX’
     message which can be displayed if the library user has not
     configured logging.  See *note Configuring Logging for a Library:
     1a4d. for more information.

New in version 3.1: The *note NullHandler: 880. class.

New in version 3.2: The *note QueueHandler: 1a94. class.

The *note NullHandler: 880, *note StreamHandler: 7c5. and *note
FileHandler: 1a3f. classes are defined in the core logging package.  The
other handlers are defined in a sub- module, *note logging.handlers: aa.
(There is also another sub-module, *note logging.config: a9, for
configuration functionality.)

Logged messages are formatted for presentation through instances of the
*note Formatter: 19da. class.  They are initialized with a format string
suitable for use with the % operator and a dictionary.

For formatting multiple messages in a batch, instances of
‘BufferingFormatter’ can be used.  In addition to the format string
(which is applied to each message in the batch), there is provision for
header and trailer format strings.

When filtering based on logger level and/or handler level is not enough,
instances of *note Filter: 7c7. can be added to both *note Logger: 2c6.
and ‘Handler’ instances (through their *note addFilter(): 19f1. method).
Before deciding to process a message further, both loggers and handlers
consult all their filters for permission.  If any filter returns a false
value, the message is not processed further.

The basic *note Filter: 7c7. functionality allows filtering by specific
logger name.  If this feature is used, messages sent to the named logger
and its children are allowed through the filter, and all others dropped.


File: python.info,  Node: Exceptions raised during logging,  Next: Using arbitrary objects as messages,  Prev: Useful Handlers,  Up: Logging HOWTO

10.6.5 Exceptions raised during logging
---------------------------------------

The logging package is designed to swallow exceptions which occur while
logging in production.  This is so that errors which occur while
handling logging events - such as logging misconfiguration, network or
other similar errors - do not cause the application using logging to
terminate prematurely.

*note SystemExit: 1a2. and *note KeyboardInterrupt: 1a3. exceptions are
never swallowed.  Other exceptions which occur during the *note emit():
19f9. method of a ‘Handler’ subclass are passed to its *note
handleError(): 19f8. method.

The default implementation of *note handleError(): 19f8. in ‘Handler’
checks to see if a module-level variable, ‘raiseExceptions’, is set.  If
set, a traceback is printed to *note sys.stderr: 270.  If not set, the
exception is swallowed.

     Note: The default value of ‘raiseExceptions’ is ‘True’.  This is
     because during development, you typically want to be notified of
     any exceptions that occur.  It’s advised that you set
     ‘raiseExceptions’ to ‘False’ for production usage.


File: python.info,  Node: Using arbitrary objects as messages,  Next: Optimization,  Prev: Exceptions raised during logging,  Up: Logging HOWTO

10.6.6 Using arbitrary objects as messages
------------------------------------------

In the preceding sections and examples, it has been assumed that the
message passed when logging the event is a string.  However, this is not
the only possibility.  You can pass an arbitrary object as a message,
and its *note __str__(): ab9. method will be called when the logging
system needs to convert it to a string representation.  In fact, if you
want to, you can avoid computing a string representation altogether -
for example, the *note SocketHandler: 467. emits an event by pickling it
and sending it over the wire.


File: python.info,  Node: Optimization,  Prev: Using arbitrary objects as messages,  Up: Logging HOWTO

10.6.7 Optimization
-------------------

Formatting of message arguments is deferred until it cannot be avoided.
However, computing the arguments passed to the logging method can also
be expensive, and you may want to avoid doing it if the logger will just
throw away your event.  To decide what to do, you can call the *note
isEnabledFor(): 19d9. method which takes a level argument and returns
true if the event would be created by the Logger for that level of call.
You can write code like this:

     if logger.isEnabledFor(logging.DEBUG):
         logger.debug('Message with %s, %s', expensive_func1(),
                                             expensive_func2())

so that if the logger’s threshold is set above ‘DEBUG’, the calls to
‘expensive_func1()’ and ‘expensive_func2()’ are never made.

     Note: In some cases, *note isEnabledFor(): 19d9. can itself be more
     expensive than you’d like (e.g.  for deeply nested loggers where an
     explicit level is only set high up in the logger hierarchy).  In
     such cases (or if you want to avoid calling a method in tight
     loops), you can cache the result of a call to *note isEnabledFor():
     19d9. in a local or instance variable, and use that instead of
     calling the method each time.  Such a cached value would only need
     to be recomputed when the logging configuration changes dynamically
     while the application is running (which is not all that common).

There are other optimizations which can be made for specific
applications which need more precise control over what logging
information is collected.  Here’s a list of things you can do to avoid
processing during logging which you don’t need:

What you don’t want to collect                      How to avoid collecting it
                                                    
-------------------------------------------------------------------------------------------------
                                                    
Information about where calls were made from.       Set ‘logging._srcfile’ to ‘None’.  This
                                                    avoids calling *note sys._getframe(): aea,
                                                    which may help to speed up your code in
                                                    environments like PyPy (which can’t speed
                                                    up code that uses
                                                    *note sys._getframe(): aea.), if and when
                                                    PyPy supports Python 3.x.
                                                    
                                                    
Threading information.                              Set ‘logging.logThreads’ to ‘0’.
                                                    
                                                    
Process information.                                Set ‘logging.logProcesses’ to ‘0’.
                                                    

Also note that the core logging module only includes the basic handlers.
If you don’t import *note logging.handlers: aa. and *note
logging.config: a9, they won’t take up any memory.

See also
........

Module *note logging: a8.

     API reference for the logging module.

Module *note logging.config: a9.

     Configuration API for the logging module.

Module *note logging.handlers: aa.

     Useful handlers included with the logging module.

*note A logging cookbook: 7c3.


File: python.info,  Node: Logging Cookbook,  Next: Regular Expression HOWTO,  Prev: Logging HOWTO,  Up: Python HOWTOs

10.7 Logging Cookbook
=====================


Author: Vinay Sajip <vinay_sajip at red-dove dot com>

This page contains a number of recipes related to logging, which have
been found useful in the past.

* Menu:

* Using logging in multiple modules:: 
* Logging from multiple threads:: 
* Multiple handlers and formatters:: 
* Logging to multiple destinations:: 
* Configuration server example:: 
* Dealing with handlers that block:: 
* Sending and receiving logging events across a network:: 
* Adding contextual information to your logging output:: 
* Logging to a single file from multiple processes:: 
* Using file rotation:: 
* Use of alternative formatting styles:: 
* Customizing LogRecord:: 
* Subclassing QueueHandler - a ZeroMQ example:: 
* Subclassing QueueListener - a ZeroMQ example:: 
* An example dictionary-based configuration:: 
* Using a rotator and namer to customize log rotation processing:: 
* A more elaborate multiprocessing example:: 
* Inserting a BOM into messages sent to a SysLogHandler:: 
* Implementing structured logging:: 
* Customizing handlers with dictConfig(): Customizing handlers with dictConfig. 
* Using particular formatting styles throughout your application:: 
* Configuring filters with dictConfig(): Configuring filters with dictConfig. 
* Customized exception formatting:: 
* Speaking logging messages:: 
* Buffering logging messages and outputting them conditionally:: 
* Formatting times using UTC (GMT) via configuration: Formatting times using UTC GMT via configuration. 
* Using a context manager for selective logging:: 


File: python.info,  Node: Using logging in multiple modules,  Next: Logging from multiple threads,  Up: Logging Cookbook

10.7.1 Using logging in multiple modules
----------------------------------------

Multiple calls to ‘logging.getLogger('someLogger')’ return a reference
to the same logger object.  This is true not only within the same
module, but also across modules as long as it is in the same Python
interpreter process.  It is true for references to the same object;
additionally, application code can define and configure a parent logger
in one module and create (but not configure) a child logger in a
separate module, and all logger calls to the child will pass up to the
parent.  Here is a main module:

     import logging
     import auxiliary_module

     # create logger with 'spam_application'
     logger = logging.getLogger('spam_application')
     logger.setLevel(logging.DEBUG)
     # create file handler which logs even debug messages
     fh = logging.FileHandler('spam.log')
     fh.setLevel(logging.DEBUG)
     # create console handler with a higher log level
     ch = logging.StreamHandler()
     ch.setLevel(logging.ERROR)
     # create formatter and add it to the handlers
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
     fh.setFormatter(formatter)
     ch.setFormatter(formatter)
     # add the handlers to the logger
     logger.addHandler(fh)
     logger.addHandler(ch)

     logger.info('creating an instance of auxiliary_module.Auxiliary')
     a = auxiliary_module.Auxiliary()
     logger.info('created an instance of auxiliary_module.Auxiliary')
     logger.info('calling auxiliary_module.Auxiliary.do_something')
     a.do_something()
     logger.info('finished auxiliary_module.Auxiliary.do_something')
     logger.info('calling auxiliary_module.some_function()')
     auxiliary_module.some_function()
     logger.info('done with auxiliary_module.some_function()')

Here is the auxiliary module:

     import logging

     # create logger
     module_logger = logging.getLogger('spam_application.auxiliary')

     class Auxiliary:
         def __init__(self):
             self.logger = logging.getLogger('spam_application.auxiliary.Auxiliary')
             self.logger.info('creating an instance of Auxiliary')
         def do_something(self):
             self.logger.info('doing something')
             a = 1 + 1
             self.logger.info('done doing something')

     def some_function():
         module_logger.info('received a call to "some_function"')

The output looks like this:

     2005-03-23 23:47:11,663 - spam_application - INFO -
        creating an instance of auxiliary_module.Auxiliary
     2005-03-23 23:47:11,665 - spam_application.auxiliary.Auxiliary - INFO -
        creating an instance of Auxiliary
     2005-03-23 23:47:11,665 - spam_application - INFO -
        created an instance of auxiliary_module.Auxiliary
     2005-03-23 23:47:11,668 - spam_application - INFO -
        calling auxiliary_module.Auxiliary.do_something
     2005-03-23 23:47:11,668 - spam_application.auxiliary.Auxiliary - INFO -
        doing something
     2005-03-23 23:47:11,669 - spam_application.auxiliary.Auxiliary - INFO -
        done doing something
     2005-03-23 23:47:11,670 - spam_application - INFO -
        finished auxiliary_module.Auxiliary.do_something
     2005-03-23 23:47:11,671 - spam_application - INFO -
        calling auxiliary_module.some_function()
     2005-03-23 23:47:11,672 - spam_application.auxiliary - INFO -
        received a call to 'some_function'
     2005-03-23 23:47:11,673 - spam_application - INFO -
        done with auxiliary_module.some_function()


File: python.info,  Node: Logging from multiple threads,  Next: Multiple handlers and formatters,  Prev: Using logging in multiple modules,  Up: Logging Cookbook

10.7.2 Logging from multiple threads
------------------------------------

Logging from multiple threads requires no special effort.  The following
example shows logging from the main (initial) thread and another thread:

     import logging
     import threading
     import time

     def worker(arg):
         while not arg['stop']:
             logging.debug('Hi from myfunc')
             time.sleep(0.5)

     def main():
         logging.basicConfig(level=logging.DEBUG, format='%(relativeCreated)6d %(threadName)s %(message)s')
         info = {'stop': False}
         thread = threading.Thread(target=worker, args=(info,))
         thread.start()
         while True:
             try:
                 logging.debug('Hello from main')
                 time.sleep(0.75)
             except KeyboardInterrupt:
                 info['stop'] = True
                 break
         thread.join()

     if __name__ == '__main__':
         main()

When run, the script should print something like the following:

        0 Thread-1 Hi from myfunc
        3 MainThread Hello from main
      505 Thread-1 Hi from myfunc
      755 MainThread Hello from main
     1007 Thread-1 Hi from myfunc
     1507 MainThread Hello from main
     1508 Thread-1 Hi from myfunc
     2010 Thread-1 Hi from myfunc
     2258 MainThread Hello from main
     2512 Thread-1 Hi from myfunc
     3009 MainThread Hello from main
     3013 Thread-1 Hi from myfunc
     3515 Thread-1 Hi from myfunc
     3761 MainThread Hello from main
     4017 Thread-1 Hi from myfunc
     4513 MainThread Hello from main
     4518 Thread-1 Hi from myfunc

This shows the logging output interspersed as one might expect.  This
approach works for more threads than shown here, of course.


File: python.info,  Node: Multiple handlers and formatters,  Next: Logging to multiple destinations,  Prev: Logging from multiple threads,  Up: Logging Cookbook

10.7.3 Multiple handlers and formatters
---------------------------------------

Loggers are plain Python objects.  The *note addHandler(): 19e2. method
has no minimum or maximum quota for the number of handlers you may add.
Sometimes it will be beneficial for an application to log all messages
of all severities to a text file while simultaneously logging errors or
above to the console.  To set this up, simply configure the appropriate
handlers.  The logging calls in the application code will remain
unchanged.  Here is a slight modification to the previous simple
module-based configuration example:

     import logging

     logger = logging.getLogger('simple_example')
     logger.setLevel(logging.DEBUG)
     # create file handler which logs even debug messages
     fh = logging.FileHandler('spam.log')
     fh.setLevel(logging.DEBUG)
     # create console handler with a higher log level
     ch = logging.StreamHandler()
     ch.setLevel(logging.ERROR)
     # create formatter and add it to the handlers
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
     ch.setFormatter(formatter)
     fh.setFormatter(formatter)
     # add the handlers to logger
     logger.addHandler(ch)
     logger.addHandler(fh)

     # 'application' code
     logger.debug('debug message')
     logger.info('info message')
     logger.warn('warn message')
     logger.error('error message')
     logger.critical('critical message')

Notice that the ’application’ code does not care about multiple
handlers.  All that changed was the addition and configuration of a new
handler named `fh'.

The ability to create new handlers with higher- or lower-severity
filters can be very helpful when writing and testing an application.
Instead of using many ‘print’ statements for debugging, use
‘logger.debug’: Unlike the print statements, which you will have to
delete or comment out later, the logger.debug statements can remain
intact in the source code and remain dormant until you need them again.
At that time, the only change that needs to happen is to modify the
severity level of the logger and/or handler to debug.


File: python.info,  Node: Logging to multiple destinations,  Next: Configuration server example,  Prev: Multiple handlers and formatters,  Up: Logging Cookbook

10.7.4 Logging to multiple destinations
---------------------------------------

Let’s say you want to log to console and file with different message
formats and in differing circumstances.  Say you want to log messages
with levels of DEBUG and higher to file, and those messages at level
INFO and higher to the console.  Let’s also assume that the file should
contain timestamps, but the console messages should not.  Here’s how you
can achieve this:

     import logging

     # set up logging to file - see previous section for more details
     logging.basicConfig(level=logging.DEBUG,
                         format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                         datefmt='%m-%d %H:%M',
                         filename='/temp/myapp.log',
                         filemode='w')
     # define a Handler which writes INFO messages or higher to the sys.stderr
     console = logging.StreamHandler()
     console.setLevel(logging.INFO)
     # set a format which is simpler for console use
     formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
     # tell the handler to use this format
     console.setFormatter(formatter)
     # add the handler to the root logger
     logging.getLogger('').addHandler(console)

     # Now, we can log to the root logger, or any other logger. First the root...
     logging.info('Jackdaws love my big sphinx of quartz.')

     # Now, define a couple of other loggers which might represent areas in your
     # application:

     logger1 = logging.getLogger('myapp.area1')
     logger2 = logging.getLogger('myapp.area2')

     logger1.debug('Quick zephyrs blow, vexing daft Jim.')
     logger1.info('How quickly daft jumping zebras vex.')
     logger2.warning('Jail zesty vixen who grabbed pay from quack.')
     logger2.error('The five boxing wizards jump quickly.')

When you run this, on the console you will see

     root        : INFO     Jackdaws love my big sphinx of quartz.
     myapp.area1 : INFO     How quickly daft jumping zebras vex.
     myapp.area2 : WARNING  Jail zesty vixen who grabbed pay from quack.
     myapp.area2 : ERROR    The five boxing wizards jump quickly.

and in the file you will see something like

     10-22 22:19 root         INFO     Jackdaws love my big sphinx of quartz.
     10-22 22:19 myapp.area1  DEBUG    Quick zephyrs blow, vexing daft Jim.
     10-22 22:19 myapp.area1  INFO     How quickly daft jumping zebras vex.
     10-22 22:19 myapp.area2  WARNING  Jail zesty vixen who grabbed pay from quack.
     10-22 22:19 myapp.area2  ERROR    The five boxing wizards jump quickly.

As you can see, the DEBUG message only shows up in the file.  The other
messages are sent to both destinations.

This example uses console and file handlers, but you can use any number
and combination of handlers you choose.


File: python.info,  Node: Configuration server example,  Next: Dealing with handlers that block,  Prev: Logging to multiple destinations,  Up: Logging Cookbook

10.7.5 Configuration server example
-----------------------------------

Here is an example of a module using the logging configuration server:

     import logging
     import logging.config
     import time
     import os

     # read initial config file
     logging.config.fileConfig('logging.conf')

     # create and start listener on port 9999
     t = logging.config.listen(9999)
     t.start()

     logger = logging.getLogger('simpleExample')

     try:
         # loop through logging calls to see the difference
         # new configurations make, until Ctrl+C is pressed
         while True:
             logger.debug('debug message')
             logger.info('info message')
             logger.warn('warn message')
             logger.error('error message')
             logger.critical('critical message')
             time.sleep(5)
     except KeyboardInterrupt:
         # cleanup
         logging.config.stopListening()
         t.join()

And here is a script that takes a filename and sends that file to the
server, properly preceded with the binary-encoded length, as the new
logging configuration:

     #!/usr/bin/env python
     import socket, sys, struct

     with open(sys.argv[1], 'rb') as f:
         data_to_send = f.read()

     HOST = 'localhost'
     PORT = 9999
     s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     print('connecting...')
     s.connect((HOST, PORT))
     print('sending config...')
     s.send(struct.pack('>L', len(data_to_send)))
     s.send(data_to_send)
     s.close()
     print('complete')


File: python.info,  Node: Dealing with handlers that block,  Next: Sending and receiving logging events across a network,  Prev: Configuration server example,  Up: Logging Cookbook

10.7.6 Dealing with handlers that block
---------------------------------------

Sometimes you have to get your logging handlers to do their work without
blocking the thread you’re logging from.  This is common in Web
applications, though of course it also occurs in other scenarios.

A common culprit which demonstrates sluggish behaviour is the *note
SMTPHandler: 1a81.: sending emails can take a long time, for a number of
reasons outside the developer’s control (for example, a poorly
performing mail or network infrastructure).  But almost any
network-based handler can block: Even a *note SocketHandler: 467.
operation may do a DNS query under the hood which is too slow (and this
query can be deep in the socket library code, below the Python layer,
and outside your control).

One solution is to use a two-part approach.  For the first part, attach
only a *note QueueHandler: 1a94. to those loggers which are accessed
from performance-critical threads.  They simply write to their queue,
which can be sized to a large enough capacity or initialized with no
upper bound to their size.  The write to the queue will typically be
accepted quickly, though you will probably need to catch the *note
queue.Full: 1d1f. exception as a precaution in your code.  If you are a
library developer who has performance-critical threads in their code, be
sure to document this (together with a suggestion to attach only
‘QueueHandlers’ to your loggers) for the benefit of other developers who
will use your code.

The second part of the solution is *note QueueListener: 2cc, which has
been designed as the counterpart to *note QueueHandler: 1a94.  A *note
QueueListener: 2cc. is very simple: it’s passed a queue and some
handlers, and it fires up an internal thread which listens to its queue
for LogRecords sent from ‘QueueHandlers’ (or any other source of
‘LogRecords’, for that matter).  The ‘LogRecords’ are removed from the
queue and passed to the handlers for processing.

The advantage of having a separate *note QueueListener: 2cc. class is
that you can use the same instance to service multiple ‘QueueHandlers’.
This is more resource-friendly than, say, having threaded versions of
the existing handler classes, which would eat up one thread per handler
for no particular benefit.

An example of using these two classes follows (imports omitted):

     que = queue.Queue(-1) # no limit on size
     queue_handler = QueueHandler(que)
     handler = logging.StreamHandler()
     listener = QueueListener(que, handler)
     root = logging.getLogger()
     root.addHandler(queue_handler)
     formatter = logging.Formatter('%(threadName)s: %(message)s')
     handler.setFormatter(formatter)
     listener.start()
     # The log output will display the thread which generated
     # the event (the main thread) rather than the internal
     # thread which monitors the internal queue. This is what
     # you want to happen.
     root.warning('Look out!')
     listener.stop()

which, when run, will produce:

     MainThread: Look out!

Changed in version 3.5: Prior to Python 3.5, the *note QueueListener:
2cc. always passed every message received from the queue to every
handler it was initialized with.  (This was because it was assumed that
level filtering was all done on the other side, where the queue is
filled.)  From 3.5 onwards, this behaviour can be changed by passing a
keyword argument ‘respect_handler_level=True’ to the listener’s
constructor.  When this is done, the listener compares the level of each
message with the handler’s level, and only passes a message to a handler
if it’s appropriate to do so.


File: python.info,  Node: Sending and receiving logging events across a network,  Next: Adding contextual information to your logging output,  Prev: Dealing with handlers that block,  Up: Logging Cookbook

10.7.7 Sending and receiving logging events across a network
------------------------------------------------------------

Let’s say you want to send logging events across a network, and handle
them at the receiving end.  A simple way of doing this is attaching a
*note SocketHandler: 467. instance to the root logger at the sending
end:

     import logging, logging.handlers

     rootLogger = logging.getLogger('')
     rootLogger.setLevel(logging.DEBUG)
     socketHandler = logging.handlers.SocketHandler('localhost',
                         logging.handlers.DEFAULT_TCP_LOGGING_PORT)
     # don't bother with a formatter, since a socket handler sends the event as
     # an unformatted pickle
     rootLogger.addHandler(socketHandler)

     # Now, we can log to the root logger, or any other logger. First the root...
     logging.info('Jackdaws love my big sphinx of quartz.')

     # Now, define a couple of other loggers which might represent areas in your
     # application:

     logger1 = logging.getLogger('myapp.area1')
     logger2 = logging.getLogger('myapp.area2')

     logger1.debug('Quick zephyrs blow, vexing daft Jim.')
     logger1.info('How quickly daft jumping zebras vex.')
     logger2.warning('Jail zesty vixen who grabbed pay from quack.')
     logger2.error('The five boxing wizards jump quickly.')

At the receiving end, you can set up a receiver using the *note
socketserver: ee. module.  Here is a basic working example:

     import pickle
     import logging
     import logging.handlers
     import socketserver
     import struct


     class LogRecordStreamHandler(socketserver.StreamRequestHandler):
         """Handler for a streaming logging request.

         This basically logs the record using whatever logging policy is
         configured locally.
         """

         def handle(self):
             """
             Handle multiple requests - each expected to be a 4-byte length,
             followed by the LogRecord in pickle format. Logs the record
             according to whatever policy is configured locally.
             """
             while True:
                 chunk = self.connection.recv(4)
                 if len(chunk) < 4:
                     break
                 slen = struct.unpack('>L', chunk)[0]
                 chunk = self.connection.recv(slen)
                 while len(chunk) < slen:
                     chunk = chunk + self.connection.recv(slen - len(chunk))
                 obj = self.unPickle(chunk)
                 record = logging.makeLogRecord(obj)
                 self.handleLogRecord(record)

         def unPickle(self, data):
             return pickle.loads(data)

         def handleLogRecord(self, record):
             # if a name is specified, we use the named logger rather than the one
             # implied by the record.
             if self.server.logname is not None:
                 name = self.server.logname
             else:
                 name = record.name
             logger = logging.getLogger(name)
             # N.B. EVERY record gets logged. This is because Logger.handle
             # is normally called AFTER logger-level filtering. If you want
             # to do filtering, do it at the client end to save wasting
             # cycles and network bandwidth!
             logger.handle(record)

     class LogRecordSocketReceiver(socketserver.ThreadingTCPServer):
         """
         Simple TCP socket-based logging receiver suitable for testing.
         """

         allow_reuse_address = True

         def __init__(self, host='localhost',
                      port=logging.handlers.DEFAULT_TCP_LOGGING_PORT,
                      handler=LogRecordStreamHandler):
             socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)
             self.abort = 0
             self.timeout = 1
             self.logname = None

         def serve_until_stopped(self):
             import select
             abort = 0
             while not abort:
                 rd, wr, ex = select.select([self.socket.fileno()],
                                            [], [],
                                            self.timeout)
                 if rd:
                     self.handle_request()
                 abort = self.abort

     def main():
         logging.basicConfig(
             format='%(relativeCreated)5d %(name)-15s %(levelname)-8s %(message)s')
         tcpserver = LogRecordSocketReceiver()
         print('About to start TCP server...')
         tcpserver.serve_until_stopped()

     if __name__ == '__main__':
         main()

First run the server, and then the client.  On the client side, nothing
is printed on the console; on the server side, you should see something
like:

     About to start TCP server...
        59 root            INFO     Jackdaws love my big sphinx of quartz.
        59 myapp.area1     DEBUG    Quick zephyrs blow, vexing daft Jim.
        69 myapp.area1     INFO     How quickly daft jumping zebras vex.
        69 myapp.area2     WARNING  Jail zesty vixen who grabbed pay from quack.
        69 myapp.area2     ERROR    The five boxing wizards jump quickly.

Note that there are some security issues with pickle in some scenarios.
If these affect you, you can use an alternative serialization scheme by
overriding the ‘makePickle()’ method and implementing your alternative
there, as well as adapting the above script to use your alternative
serialization.


File: python.info,  Node: Adding contextual information to your logging output,  Next: Logging to a single file from multiple processes,  Prev: Sending and receiving logging events across a network,  Up: Logging Cookbook

10.7.8 Adding contextual information to your logging output
-----------------------------------------------------------

Sometimes you want logging output to contain contextual information in
addition to the parameters passed to the logging call.  For example, in
a networked application, it may be desirable to log client-specific
information in the log (e.g.  remote client’s username, or IP address).
Although you could use the `extra' parameter to achieve this, it’s not
always convenient to pass the information in this way.  While it might
be tempting to create ‘Logger’ instances on a per-connection basis, this
is not a good idea because these instances are not garbage collected.
While this is not a problem in practice, when the number of ‘Logger’
instances is dependent on the level of granularity you want to use in
logging an application, it could be hard to manage if the number of
‘Logger’ instances becomes effectively unbounded.

* Menu:

* Using LoggerAdapters to impart contextual information:: 
* Using Filters to impart contextual information:: 


File: python.info,  Node: Using LoggerAdapters to impart contextual information,  Next: Using Filters to impart contextual information,  Up: Adding contextual information to your logging output

10.7.8.1 Using LoggerAdapters to impart contextual information
..............................................................

An easy way in which you can pass contextual information to be output
along with logging event information is to use the ‘LoggerAdapter’
class.  This class is designed to look like a ‘Logger’, so that you can
call ‘debug()’, ‘info()’, ‘warning()’, ‘error()’, ‘exception()’,
‘critical()’ and ‘log()’.  These methods have the same signatures as
their counterparts in ‘Logger’, so you can use the two types of
instances interchangeably.

When you create an instance of ‘LoggerAdapter’, you pass it a ‘Logger’
instance and a dict-like object which contains your contextual
information.  When you call one of the logging methods on an instance of
‘LoggerAdapter’, it delegates the call to the underlying instance of
‘Logger’ passed to its constructor, and arranges to pass the contextual
information in the delegated call.  Here’s a snippet from the code of
‘LoggerAdapter’:

     def debug(self, msg, *args, **kwargs):
         """
         Delegate a debug call to the underlying logger, after adding
         contextual information from this adapter instance.
         """
         msg, kwargs = self.process(msg, kwargs)
         self.logger.debug(msg, *args, **kwargs)

The ‘process()’ method of ‘LoggerAdapter’ is where the contextual
information is added to the logging output.  It’s passed the message and
keyword arguments of the logging call, and it passes back (potentially)
modified versions of these to use in the call to the underlying logger.
The default implementation of this method leaves the message alone, but
inserts an ’extra’ key in the keyword argument whose value is the
dict-like object passed to the constructor.  Of course, if you had
passed an ’extra’ keyword argument in the call to the adapter, it will
be silently overwritten.

The advantage of using ’extra’ is that the values in the dict-like
object are merged into the ‘LogRecord’ instance’s __dict__, allowing you
to use customized strings with your ‘Formatter’ instances which know
about the keys of the dict-like object.  If you need a different method,
e.g.  if you want to prepend or append the contextual information to the
message string, you just need to subclass ‘LoggerAdapter’ and override
‘process()’ to do what you need.  Here is a simple example:

     class CustomAdapter(logging.LoggerAdapter):
         """
         This example adapter expects the passed in dict-like object to have a
         'connid' key, whose value in brackets is prepended to the log message.
         """
         def process(self, msg, kwargs):
             return '[%s] %s' % (self.extra['connid'], msg), kwargs

which you can use like this:

     logger = logging.getLogger(__name__)
     adapter = CustomAdapter(logger, {'connid': some_conn_id})

Then any events that you log to the adapter will have the value of
‘some_conn_id’ prepended to the log messages.

* Menu:

* Using objects other than dicts to pass contextual information:: 


File: python.info,  Node: Using objects other than dicts to pass contextual information,  Up: Using LoggerAdapters to impart contextual information

10.7.8.2 Using objects other than dicts to pass contextual information
......................................................................

You don’t need to pass an actual dict to a ‘LoggerAdapter’ - you could
pass an instance of a class which implements ‘__getitem__’ and
‘__iter__’ so that it looks like a dict to logging.  This would be
useful if you want to generate values dynamically (whereas the values in
a dict would be constant).


File: python.info,  Node: Using Filters to impart contextual information,  Prev: Using LoggerAdapters to impart contextual information,  Up: Adding contextual information to your logging output

10.7.8.3 Using Filters to impart contextual information
.......................................................

You can also add contextual information to log output using a
user-defined ‘Filter’.  ‘Filter’ instances are allowed to modify the
‘LogRecords’ passed to them, including adding additional attributes
which can then be output using a suitable format string, or if needed a
custom ‘Formatter’.

For example in a web application, the request being processed (or at
least, the interesting parts of it) can be stored in a threadlocal
(*note threading.local: 1c56.) variable, and then accessed from a
‘Filter’ to add, say, information from the request - say, the remote IP
address and remote user’s username - to the ‘LogRecord’, using the
attribute names ’ip’ and ’user’ as in the ‘LoggerAdapter’ example above.
In that case, the same format string can be used to get similar output
to that shown above.  Here’s an example script:

     import logging
     from random import choice

     class ContextFilter(logging.Filter):
         """
         This is a filter which injects contextual information into the log.

         Rather than use actual contextual information, we just use random
         data in this demo.
         """

         USERS = ['jim', 'fred', 'sheila']
         IPS = ['123.231.231.123', '127.0.0.1', '192.168.0.1']

         def filter(self, record):

             record.ip = choice(ContextFilter.IPS)
             record.user = choice(ContextFilter.USERS)
             return True

     if __name__ == '__main__':
        levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')
        a1 = logging.getLogger('a.b.c')
        a2 = logging.getLogger('d.e.f')

        f = ContextFilter()
        a1.addFilter(f)
        a2.addFilter(f)
        a1.debug('A debug message')
        a1.info('An info message with %s', 'some parameters')
        for x in range(10):
            lvl = choice(levels)
            lvlname = logging.getLevelName(lvl)
            a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')

which, when run, produces something like:

     2010-09-06 22:38:15,292 a.b.c DEBUG    IP: 123.231.231.123 User: fred     A debug message
     2010-09-06 22:38:15,300 a.b.c INFO     IP: 192.168.0.1     User: sheila   An info message with some parameters
     2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f ERROR    IP: 127.0.0.1       User: jim      A message at ERROR level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 127.0.0.1       User: sheila   A message at DEBUG level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f ERROR    IP: 123.231.231.123 User: fred     A message at ERROR level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 192.168.0.1     User: jim      A message at DEBUG level with 2 parameters
     2010-09-06 22:38:15,301 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters
     2010-09-06 22:38:15,301 d.e.f DEBUG    IP: 123.231.231.123 User: fred     A message at DEBUG level with 2 parameters
     2010-09-06 22:38:15,301 d.e.f INFO     IP: 123.231.231.123 User: fred     A message at INFO level with 2 parameters


File: python.info,  Node: Logging to a single file from multiple processes,  Next: Using file rotation,  Prev: Adding contextual information to your logging output,  Up: Logging Cookbook

10.7.9 Logging to a single file from multiple processes
-------------------------------------------------------

Although logging is thread-safe, and logging to a single file from
multiple threads in a single process `is' supported, logging to a single
file from `multiple processes' is `not' supported, because there is no
standard way to serialize access to a single file across multiple
processes in Python.  If you need to log to a single file from multiple
processes, one way of doing this is to have all the processes log to a
‘SocketHandler’, and have a separate process which implements a socket
server which reads from the socket and logs to file.  (If you prefer,
you can dedicate one thread in one of the existing processes to perform
this function.)  *note This section: 38a8. documents this approach in
more detail and includes a working socket receiver which can be used as
a starting point for you to adapt in your own applications.

If you are using a recent version of Python which includes the *note
multiprocessing: b6. module, you could write your own handler which uses
the *note Lock: 1cff. class from this module to serialize access to the
file from your processes.  The existing ‘FileHandler’ and subclasses do
not make use of *note multiprocessing: b6. at present, though they may
do so in the future.  Note that at present, the *note multiprocessing:
b6. module does not provide working lock functionality on all platforms
(see ‘https://bugs.python.org/issue3770’).

Alternatively, you can use a ‘Queue’ and a *note QueueHandler: 1a94. to
send all logging events to one of the processes in your multi-process
application.  The following example script demonstrates how you can do
this; in the example a separate listener process listens for events sent
by other processes and logs them according to its own logging
configuration.  Although the example only demonstrates one way of doing
it (for example, you may want to use a listener thread rather than a
separate listener process – the implementation would be analogous) it
does allow for completely different logging configurations for the
listener and the other processes in your application, and can be used as
the basis for code meeting your own specific requirements:

     # You'll need these imports in your own code
     import logging
     import logging.handlers
     import multiprocessing

     # Next two import lines for this demo only
     from random import choice, random
     import time

     #
     # Because you'll want to define the logging configurations for listener and workers, the
     # listener and worker process functions take a configurer parameter which is a callable
     # for configuring logging for that process. These functions are also passed the queue,
     # which they use for communication.
     #
     # In practice, you can configure the listener however you want, but note that in this
     # simple example, the listener does not apply level or filter logic to received records.
     # In practice, you would probably want to do this logic in the worker processes, to avoid
     # sending events which would be filtered out between processes.
     #
     # The size of the rotated files is made small so you can see the results easily.
     def listener_configurer():
         root = logging.getLogger()
         h = logging.handlers.RotatingFileHandler('mptest.log', 'a', 300, 10)
         f = logging.Formatter('%(asctime)s %(processName)-10s %(name)s %(levelname)-8s %(message)s')
         h.setFormatter(f)
         root.addHandler(h)

     # This is the listener process top-level loop: wait for logging events
     # (LogRecords)on the queue and handle them, quit when you get a None for a
     # LogRecord.
     def listener_process(queue, configurer):
         configurer()
         while True:
             try:
                 record = queue.get()
                 if record is None: # We send this as a sentinel to tell the listener to quit.
                     break
                 logger = logging.getLogger(record.name)
                 logger.handle(record) # No level or filter logic applied - just do it!
             except Exception:
                 import sys, traceback
                 print('Whoops! Problem:', file=sys.stderr)
                 traceback.print_exc(file=sys.stderr)

     # Arrays used for random selections in this demo

     LEVELS = [logging.DEBUG, logging.INFO, logging.WARNING,
               logging.ERROR, logging.CRITICAL]

     LOGGERS = ['a.b.c', 'd.e.f']

     MESSAGES = [
         'Random message #1',
         'Random message #2',
         'Random message #3',
     ]

     # The worker configuration is done at the start of the worker process run.
     # Note that on Windows you can't rely on fork semantics, so each process
     # will run the logging configuration code when it starts.
     def worker_configurer(queue):
         h = logging.handlers.QueueHandler(queue) # Just the one handler needed
         root = logging.getLogger()
         root.addHandler(h)
         root.setLevel(logging.DEBUG) # send all messages, for demo; no other level or filter logic applied.

     # This is the worker process top-level loop, which just logs ten events with
     # random intervening delays before terminating.
     # The print messages are just so you know it's doing something!
     def worker_process(queue, configurer):
         configurer(queue)
         name = multiprocessing.current_process().name
         print('Worker started: %s' % name)
         for i in range(10):
             time.sleep(random())
             logger = logging.getLogger(choice(LOGGERS))
             level = choice(LEVELS)
             message = choice(MESSAGES)
             logger.log(level, message)
         print('Worker finished: %s' % name)

     # Here's where the demo gets orchestrated. Create the queue, create and start
     # the listener, create ten workers and start them, wait for them to finish,
     # then send a None to the queue to tell the listener to finish.
     def main():
         queue = multiprocessing.Queue(-1)
         listener = multiprocessing.Process(target=listener_process,
                                            args=(queue, listener_configurer))
         listener.start()
         workers = []
         for i in range(10):
             worker = multiprocessing.Process(target=worker_process,
                                            args=(queue, worker_configurer))
             workers.append(worker)
             worker.start()
         for w in workers:
             w.join()
         queue.put_nowait(None)
         listener.join()

     if __name__ == '__main__':
         main()

A variant of the above script keeps the logging in the main process, in
a separate thread:

     import logging
     import logging.config
     import logging.handlers
     from multiprocessing import Process, Queue
     import random
     import threading
     import time

     def logger_thread(q):
         while True:
             record = q.get()
             if record is None:
                 break
             logger = logging.getLogger(record.name)
             logger.handle(record)


     def worker_process(q):
         qh = logging.handlers.QueueHandler(q)
         root = logging.getLogger()
         root.setLevel(logging.DEBUG)
         root.addHandler(qh)
         levels = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,
                   logging.CRITICAL]
         loggers = ['foo', 'foo.bar', 'foo.bar.baz',
                    'spam', 'spam.ham', 'spam.ham.eggs']
         for i in range(100):
             lvl = random.choice(levels)
             logger = logging.getLogger(random.choice(loggers))
             logger.log(lvl, 'Message no. %d', i)

     if __name__ == '__main__':
         q = Queue()
         d = {
             'version': 1,
             'formatters': {
                 'detailed': {
                     'class': 'logging.Formatter',
                     'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'
                 }
             },
             'handlers': {
                 'console': {
                     'class': 'logging.StreamHandler',
                     'level': 'INFO',
                 },
                 'file': {
                     'class': 'logging.FileHandler',
                     'filename': 'mplog.log',
                     'mode': 'w',
                     'formatter': 'detailed',
                 },
                 'foofile': {
                     'class': 'logging.FileHandler',
                     'filename': 'mplog-foo.log',
                     'mode': 'w',
                     'formatter': 'detailed',
                 },
                 'errors': {
                     'class': 'logging.FileHandler',
                     'filename': 'mplog-errors.log',
                     'mode': 'w',
                     'level': 'ERROR',
                     'formatter': 'detailed',
                 },
             },
             'loggers': {
                 'foo': {
                     'handlers': ['foofile']
                 }
             },
             'root': {
                 'level': 'DEBUG',
                 'handlers': ['console', 'file', 'errors']
             },
         }
         workers = []
         for i in range(5):
             wp = Process(target=worker_process, name='worker %d' % (i + 1), args=(q,))
             workers.append(wp)
             wp.start()
         logging.config.dictConfig(d)
         lp = threading.Thread(target=logger_thread, args=(q,))
         lp.start()
         # At this point, the main process could do some useful work of its own
         # Once it's done that, it can wait for the workers to terminate...
         for wp in workers:
             wp.join()
         # And now tell the logging thread to finish up, too
         q.put(None)
         lp.join()

This variant shows how you can e.g.  apply configuration for particular
loggers - e.g.  the ‘foo’ logger has a special handler which stores all
events in the ‘foo’ subsystem in a file ‘mplog-foo.log’.  This will be
used by the logging machinery in the main process (even though the
logging events are generated in the worker processes) to direct the
messages to the appropriate destinations.


File: python.info,  Node: Using file rotation,  Next: Use of alternative formatting styles,  Prev: Logging to a single file from multiple processes,  Up: Logging Cookbook

10.7.10 Using file rotation
---------------------------

Sometimes you want to let a log file grow to a certain size, then open a
new file and log to that.  You may want to keep a certain number of
these files, and when that many files have been created, rotate the
files so that the number of files and the size of the files both remain
bounded.  For this usage pattern, the logging package provides a
‘RotatingFileHandler’:

     import glob
     import logging
     import logging.handlers

     LOG_FILENAME = 'logging_rotatingfile_example.out'

     # Set up a specific logger with our desired output level
     my_logger = logging.getLogger('MyLogger')
     my_logger.setLevel(logging.DEBUG)

     # Add the log message handler to the logger
     handler = logging.handlers.RotatingFileHandler(
                   LOG_FILENAME, maxBytes=20, backupCount=5)

     my_logger.addHandler(handler)

     # Log some messages
     for i in range(20):
         my_logger.debug('i = %d' % i)

     # See what files are created
     logfiles = glob.glob('%s*' % LOG_FILENAME)

     for filename in logfiles:
         print(filename)

The result should be 6 separate files, each with part of the log history
for the application:

     logging_rotatingfile_example.out
     logging_rotatingfile_example.out.1
     logging_rotatingfile_example.out.2
     logging_rotatingfile_example.out.3
     logging_rotatingfile_example.out.4
     logging_rotatingfile_example.out.5

The most current file is always ‘logging_rotatingfile_example.out’, and
each time it reaches the size limit it is renamed with the suffix ‘.1’.
Each of the existing backup files is renamed to increment the suffix
(‘.1’ becomes ‘.2’, etc.)  and the ‘.6’ file is erased.

Obviously this example sets the log length much too small as an extreme
example.  You would want to set `maxBytes' to an appropriate value.


File: python.info,  Node: Use of alternative formatting styles,  Next: Customizing LogRecord,  Prev: Using file rotation,  Up: Logging Cookbook

10.7.11 Use of alternative formatting styles
--------------------------------------------

When logging was added to the Python standard library, the only way of
formatting messages with variable content was to use the %-formatting
method.  Since then, Python has gained two new formatting approaches:
*note string.Template: 7c4. (added in Python 2.4) and *note
str.format(): 14d. (added in Python 2.6).

Logging (as of 3.2) provides improved support for these two additional
formatting styles.  The ‘Formatter’ class been enhanced to take an
additional, optional keyword parameter named ‘style’.  This defaults to
‘'%'’, but other possible values are ‘'{'’ and ‘'$'’, which correspond
to the other two formatting styles.  Backwards compatibility is
maintained by default (as you would expect), but by explicitly
specifying a style parameter, you get the ability to specify format
strings which work with *note str.format(): 14d. or *note
string.Template: 7c4.  Here’s an example console session to show the
possibilities:

     >>> import logging
     >>> root = logging.getLogger()
     >>> root.setLevel(logging.DEBUG)
     >>> handler = logging.StreamHandler()
     >>> bf = logging.Formatter('{asctime} {name} {levelname:8s} {message}',
     ...                        style='{')
     >>> handler.setFormatter(bf)
     >>> root.addHandler(handler)
     >>> logger = logging.getLogger('foo.bar')
     >>> logger.debug('This is a DEBUG message')
     2010-10-28 15:11:55,341 foo.bar DEBUG    This is a DEBUG message
     >>> logger.critical('This is a CRITICAL message')
     2010-10-28 15:12:11,526 foo.bar CRITICAL This is a CRITICAL message
     >>> df = logging.Formatter('$asctime $name ${levelname} $message',
     ...                        style='$')
     >>> handler.setFormatter(df)
     >>> logger.debug('This is a DEBUG message')
     2010-10-28 15:13:06,924 foo.bar DEBUG This is a DEBUG message
     >>> logger.critical('This is a CRITICAL message')
     2010-10-28 15:13:11,494 foo.bar CRITICAL This is a CRITICAL message
     >>>

Note that the formatting of logging messages for final output to logs is
completely independent of how an individual logging message is
constructed.  That can still use %-formatting, as shown here:

     >>> logger.error('This is an%s %s %s', 'other,', 'ERROR,', 'message')
     2010-10-28 15:19:29,833 foo.bar ERROR This is another, ERROR, message
     >>>

Logging calls (‘logger.debug()’, ‘logger.info()’ etc.)  only take
positional parameters for the actual logging message itself, with
keyword parameters used only for determining options for how to handle
the actual logging call (e.g.  the ‘exc_info’ keyword parameter to
indicate that traceback information should be logged, or the ‘extra’
keyword parameter to indicate additional contextual information to be
added to the log).  So you cannot directly make logging calls using
*note str.format(): 14d. or *note string.Template: 7c4. syntax, because
internally the logging package uses %-formatting to merge the format
string and the variable arguments.  There would no changing this while
preserving backward compatibility, since all logging calls which are out
there in existing code will be using %-format strings.

There is, however, a way that you can use {}- and $- formatting to
construct your individual log messages.  Recall that for a message you
can use an arbitrary object as a message format string, and that the
logging package will call ‘str()’ on that object to get the actual
format string.  Consider the following two classes:

     class BraceMessage:
         def __init__(self, fmt, *args, **kwargs):
             self.fmt = fmt
             self.args = args
             self.kwargs = kwargs

         def __str__(self):
             return self.fmt.format(*self.args, **self.kwargs)

     class DollarMessage:
         def __init__(self, fmt, **kwargs):
             self.fmt = fmt
             self.kwargs = kwargs

         def __str__(self):
             from string import Template
             return Template(self.fmt).substitute(**self.kwargs)

Either of these can be used in place of a format string, to allow {}- or
$-formatting to be used to build the actual "message" part which appears
in the formatted log output in place of "%(message)s" or "{message}" or
"$message".  It’s a little unwieldy to use the class names whenever you
want to log something, but it’s quite palatable if you use an alias such
as __ (double underscore – not to be confused with _, the single
underscore used as a synonym/alias for *note gettext.gettext(): a49. or
its brethren).

The above classes are not included in Python, though they’re easy enough
to copy and paste into your own code.  They can be used as follows
(assuming that they’re declared in a module called ‘wherever’):

     >>> from wherever import BraceMessage as __
     >>> print(__('Message with {0} {name}', 2, name='placeholders'))
     Message with 2 placeholders
     >>> class Point: pass
     ...
     >>> p = Point()
     >>> p.x = 0.5
     >>> p.y = 0.5
     >>> print(__('Message with coordinates: ({point.x:.2f}, {point.y:.2f})',
     ...       point=p))
     Message with coordinates: (0.50, 0.50)
     >>> from wherever import DollarMessage as __
     >>> print(__('Message with $num $what', num=2, what='placeholders'))
     Message with 2 placeholders
     >>>

While the above examples use ‘print()’ to show how the formatting works,
you would of course use ‘logger.debug()’ or similar to actually log
using this approach.

One thing to note is that you pay no significant performance penalty
with this approach: the actual formatting happens not when you make the
logging call, but when (and if) the logged message is actually about to
be output to a log by a handler.  So the only slightly unusual thing
which might trip you up is that the parentheses go around the format
string and the arguments, not just the format string.  That’s because
the __ notation is just syntax sugar for a constructor call to one of
the XXXMessage classes.

If you prefer, you can use a ‘LoggerAdapter’ to achieve a similar effect
to the above, as in the following example:

     import logging

     class Message(object):
         def __init__(self, fmt, args):
             self.fmt = fmt
             self.args = args

         def __str__(self):
             return self.fmt.format(*self.args)

     class StyleAdapter(logging.LoggerAdapter):
         def __init__(self, logger, extra=None):
             super(StyleAdapter, self).__init__(logger, extra or {})

         def log(self, level, msg, *args, **kwargs):
             if self.isEnabledFor(level):
                 msg, kwargs = self.process(msg, kwargs)
                 self.logger._log(level, Message(msg, args), (), **kwargs)

     logger = StyleAdapter(logging.getLogger(__name__))

     def main():
         logger.debug('Hello, {}', 'world!')

     if __name__ == '__main__':
         logging.basicConfig(level=logging.DEBUG)
         main()

The above script should log the message ‘Hello, world!’ when run with
Python 3.2 or later.


File: python.info,  Node: Customizing LogRecord,  Next: Subclassing QueueHandler - a ZeroMQ example,  Prev: Use of alternative formatting styles,  Up: Logging Cookbook

10.7.12 Customizing ‘LogRecord’
-------------------------------

Every logging event is represented by a *note LogRecord: 508. instance.
When an event is logged and not filtered out by a logger’s level, a
*note LogRecord: 508. is created, populated with information about the
event and then passed to the handlers for that logger (and its
ancestors, up to and including the logger where further propagation up
the hierarchy is disabled).  Before Python 3.2, there were only two
places where this creation was done:

   * *note Logger.makeRecord(): 19e6, which is called in the normal
     process of logging an event.  This invoked *note LogRecord: 508.
     directly to create an instance.

   * *note makeLogRecord(): 1a0b, which is called with a dictionary
     containing attributes to be added to the LogRecord.  This is
     typically invoked when a suitable dictionary has been received over
     the network (e.g.  in pickle form via a *note SocketHandler: 467,
     or in JSON form via an *note HTTPHandler: 2cb.).

This has usually meant that if you need to do anything special with a
*note LogRecord: 508, you’ve had to do one of the following.

   * Create your own *note Logger: 2c6. subclass, which overrides *note
     Logger.makeRecord(): 19e6, and set it using *note setLoggerClass():
     1a18. before any loggers that you care about are instantiated.

   * Add a *note Filter: 7c7. to a logger or handler, which does the
     necessary special manipulation you need when its *note filter():
     1a06. method is called.

The first approach would be a little unwieldy in the scenario where
(say) several different libraries wanted to do different things.  Each
would attempt to set its own *note Logger: 2c6. subclass, and the one
which did this last would win.

The second approach works reasonably well for many cases, but does not
allow you to e.g.  use a specialized subclass of *note LogRecord: 508.
Library developers can set a suitable filter on their loggers, but they
would have to remember to do this every time they introduced a new
logger (which they would do simply by adding new packages or modules and
doing

     logger = logging.getLogger(__name__)

at module level).  It’s probably one too many things to think about.
Developers could also add the filter to a *note NullHandler: 880.
attached to their top-level logger, but this would not be invoked if an
application developer attached a handler to a lower-level library logger
– so output from that handler would not reflect the intentions of the
library developer.

In Python 3.2 and later, *note LogRecord: 508. creation is done through
a factory, which you can specify.  The factory is just a callable you
can set with *note setLogRecordFactory(): 1a0e, and interrogate with
*note getLogRecordFactory(): 1a0d.  The factory is invoked with the same
signature as the *note LogRecord: 508. constructor, as *note LogRecord:
508. is the default setting for the factory.

This approach allows a custom factory to control all aspects of
LogRecord creation.  For example, you could return a subclass, or just
add some additional attributes to the record once created, using a
pattern similar to this:

     old_factory = logging.getLogRecordFactory()

     def record_factory(*args, **kwargs):
         record = old_factory(*args, **kwargs)
         record.custom_attribute = 0xdecafbad
         return record

     logging.setLogRecordFactory(record_factory)

This pattern allows different libraries to chain factories together, and
as long as they don’t overwrite each other’s attributes or
unintentionally overwrite the attributes provided as standard, there
should be no surprises.  However, it should be borne in mind that each
link in the chain adds run-time overhead to all logging operations, and
the technique should only be used when the use of a *note Filter: 7c7.
does not provide the desired result.


File: python.info,  Node: Subclassing QueueHandler - a ZeroMQ example,  Next: Subclassing QueueListener - a ZeroMQ example,  Prev: Customizing LogRecord,  Up: Logging Cookbook

10.7.13 Subclassing QueueHandler - a ZeroMQ example
---------------------------------------------------

You can use a ‘QueueHandler’ subclass to send messages to other kinds of
queues, for example a ZeroMQ ’publish’ socket.  In the example below,the
socket is created separately and passed to the handler (as its ’queue’):

     import zmq # using pyzmq, the Python binding for ZeroMQ
     import json # for serializing records portably

     ctx = zmq.Context()
     sock = zmq.Socket(ctx, zmq.PUB) # or zmq.PUSH, or other suitable value
     sock.bind('tcp://*:5556') # or wherever

     class ZeroMQSocketHandler(QueueHandler):
         def enqueue(self, record):
             data = json.dumps(record.__dict__)
             self.queue.send(data)

     handler = ZeroMQSocketHandler(sock)

Of course there are other ways of organizing this, for example passing
in the data needed by the handler to create the socket:

     class ZeroMQSocketHandler(QueueHandler):
         def __init__(self, uri, socktype=zmq.PUB, ctx=None):
             self.ctx = ctx or zmq.Context()
             socket = zmq.Socket(self.ctx, socktype)
             socket.bind(uri)
             QueueHandler.__init__(self, socket)

         def enqueue(self, record):
             data = json.dumps(record.__dict__)
             self.queue.send(data)

         def close(self):
             self.queue.close()


File: python.info,  Node: Subclassing QueueListener - a ZeroMQ example,  Next: An example dictionary-based configuration,  Prev: Subclassing QueueHandler - a ZeroMQ example,  Up: Logging Cookbook

10.7.14 Subclassing QueueListener - a ZeroMQ example
----------------------------------------------------

You can also subclass ‘QueueListener’ to get messages from other kinds
of queues, for example a ZeroMQ ’subscribe’ socket.  Here’s an example:

     class ZeroMQSocketListener(QueueListener):
         def __init__(self, uri, *handlers, **kwargs):
             self.ctx = kwargs.get('ctx') or zmq.Context()
             socket = zmq.Socket(self.ctx, zmq.SUB)
             socket.setsockopt(zmq.SUBSCRIBE, '') # subscribe to everything
             socket.connect(uri)

         def dequeue(self):
             msg = self.queue.recv()
             return logging.makeLogRecord(json.loads(msg))

See also
........

Module *note logging: a8.

     API reference for the logging module.

Module *note logging.config: a9.

     Configuration API for the logging module.

Module *note logging.handlers: aa.

     Useful handlers included with the logging module.

*note A basic logging tutorial: 7c1.

*note A more advanced logging tutorial: 7c2.


File: python.info,  Node: An example dictionary-based configuration,  Next: Using a rotator and namer to customize log rotation processing,  Prev: Subclassing QueueListener - a ZeroMQ example,  Up: Logging Cookbook

10.7.15 An example dictionary-based configuration
-------------------------------------------------

Below is an example of a logging configuration dictionary - it’s taken
from the documentation on the Django project(1).  This dictionary is
passed to *note dictConfig(): 76d. to put the configuration into effect:

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': True,
         'formatters': {
             'verbose': {
                 'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'
             },
             'simple': {
                 'format': '%(levelname)s %(message)s'
             },
         },
         'filters': {
             'special': {
                 '()': 'project.logging.SpecialFilter',
                 'foo': 'bar',
             }
         },
         'handlers': {
             'null': {
                 'level':'DEBUG',
                 'class':'django.utils.log.NullHandler',
             },
             'console':{
                 'level':'DEBUG',
                 'class':'logging.StreamHandler',
                 'formatter': 'simple'
             },
             'mail_admins': {
                 'level': 'ERROR',
                 'class': 'django.utils.log.AdminEmailHandler',
                 'filters': ['special']
             }
         },
         'loggers': {
             'django': {
                 'handlers':['null'],
                 'propagate': True,
                 'level':'INFO',
             },
             'django.request': {
                 'handlers': ['mail_admins'],
                 'level': 'ERROR',
                 'propagate': False,
             },
             'myproject.custom': {
                 'handlers': ['console', 'mail_admins'],
                 'level': 'INFO',
                 'filters': ['special']
             }
         }
     }

For more information about this configuration, you can see the relevant
section(2) of the Django documentation.

   ---------- Footnotes ----------

   (1) 
https://docs.djangoproject.com/en/1.9/topics/logging/#configuring-logging

   (2) 
https://docs.djangoproject.com/en/1.9/topics/logging/#configuring-logging


File: python.info,  Node: Using a rotator and namer to customize log rotation processing,  Next: A more elaborate multiprocessing example,  Prev: An example dictionary-based configuration,  Up: Logging Cookbook

10.7.16 Using a rotator and namer to customize log rotation processing
----------------------------------------------------------------------

An example of how you can define a namer and rotator is given in the
following snippet, which shows zlib-based compression of the log file:

     def namer(name):
         return name + ".gz"

     def rotator(source, dest):
         with open(source, "rb") as sf:
             data = sf.read()
             compressed = zlib.compress(data, 9)
             with open(dest, "wb") as df:
                 df.write(compressed)
         os.remove(source)

     rh = logging.handlers.RotatingFileHandler(...)
     rh.rotator = rotator
     rh.namer = namer

These are not "true" .gz files, as they are bare compressed data, with
no "container" such as you’d find in an actual gzip file.  This snippet
is just for illustration purposes.


File: python.info,  Node: A more elaborate multiprocessing example,  Next: Inserting a BOM into messages sent to a SysLogHandler,  Prev: Using a rotator and namer to customize log rotation processing,  Up: Logging Cookbook

10.7.17 A more elaborate multiprocessing example
------------------------------------------------

The following working example shows how logging can be used with
multiprocessing using configuration files.  The configurations are
fairly simple, but serve to illustrate how more complex ones could be
implemented in a real multiprocessing scenario.

In the example, the main process spawns a listener process and some
worker processes.  Each of the main process, the listener and the
workers have three separate configurations (the workers all share the
same configuration).  We can see logging in the main process, how the
workers log to a QueueHandler and how the listener implements a
QueueListener and a more complex logging configuration, and arranges to
dispatch events received via the queue to the handlers specified in the
configuration.  Note that these configurations are purely illustrative,
but you should be able to adapt this example to your own scenario.

Here’s the script - the docstrings and the comments hopefully explain
how it works:

     import logging
     import logging.config
     import logging.handlers
     from multiprocessing import Process, Queue, Event, current_process
     import os
     import random
     import time

     class MyHandler:
         """
         A simple handler for logging events. It runs in the listener process and
         dispatches events to loggers based on the name in the received record,
         which then get dispatched, by the logging system, to the handlers
         configured for those loggers.
         """
         def handle(self, record):
             logger = logging.getLogger(record.name)
             # The process name is transformed just to show that it's the listener
             # doing the logging to files and console
             record.processName = '%s (for %s)' % (current_process().name, record.processName)
             logger.handle(record)

     def listener_process(q, stop_event, config):
         """
         This could be done in the main process, but is just done in a separate
         process for illustrative purposes.

         This initialises logging according to the specified configuration,
         starts the listener and waits for the main process to signal completion
         via the event. The listener is then stopped, and the process exits.
         """
         logging.config.dictConfig(config)
         listener = logging.handlers.QueueListener(q, MyHandler())
         listener.start()
         if os.name == 'posix':
             # On POSIX, the setup logger will have been configured in the
             # parent process, but should have been disabled following the
             # dictConfig call.
             # On Windows, since fork isn't used, the setup logger won't
             # exist in the child, so it would be created and the message
             # would appear - hence the "if posix" clause.
             logger = logging.getLogger('setup')
             logger.critical('Should not appear, because of disabled logger ...')
         stop_event.wait()
         listener.stop()

     def worker_process(config):
         """
         A number of these are spawned for the purpose of illustration. In
         practice, they could be a heterogeneous bunch of processes rather than
         ones which are identical to each other.

         This initialises logging according to the specified configuration,
         and logs a hundred messages with random levels to randomly selected
         loggers.

         A small sleep is added to allow other processes a chance to run. This
         is not strictly needed, but it mixes the output from the different
         processes a bit more than if it's left out.
         """
         logging.config.dictConfig(config)
         levels = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,
                   logging.CRITICAL]
         loggers = ['foo', 'foo.bar', 'foo.bar.baz',
                    'spam', 'spam.ham', 'spam.ham.eggs']
         if os.name == 'posix':
             # On POSIX, the setup logger will have been configured in the
             # parent process, but should have been disabled following the
             # dictConfig call.
             # On Windows, since fork isn't used, the setup logger won't
             # exist in the child, so it would be created and the message
             # would appear - hence the "if posix" clause.
             logger = logging.getLogger('setup')
             logger.critical('Should not appear, because of disabled logger ...')
         for i in range(100):
             lvl = random.choice(levels)
             logger = logging.getLogger(random.choice(loggers))
             logger.log(lvl, 'Message no. %d', i)
             time.sleep(0.01)

     def main():
         q = Queue()
         # The main process gets a simple configuration which prints to the console.
         config_initial = {
             'version': 1,
             'formatters': {
                 'detailed': {
                     'class': 'logging.Formatter',
                     'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'
                 }
             },
             'handlers': {
                 'console': {
                     'class': 'logging.StreamHandler',
                     'level': 'INFO',
                 },
             },
             'root': {
                 'level': 'DEBUG',
                 'handlers': ['console']
             },
         }
         # The worker process configuration is just a QueueHandler attached to the
         # root logger, which allows all messages to be sent to the queue.
         # We disable existing loggers to disable the "setup" logger used in the
         # parent process. This is needed on POSIX because the logger will
         # be there in the child following a fork().
         config_worker = {
             'version': 1,
             'disable_existing_loggers': True,
             'handlers': {
                 'queue': {
                     'class': 'logging.handlers.QueueHandler',
                     'queue': q,
                 },
             },
             'root': {
                 'level': 'DEBUG',
                 'handlers': ['queue']
             },
         }
         # The listener process configuration shows that the full flexibility of
         # logging configuration is available to dispatch events to handlers however
         # you want.
         # We disable existing loggers to disable the "setup" logger used in the
         # parent process. This is needed on POSIX because the logger will
         # be there in the child following a fork().
         config_listener = {
             'version': 1,
             'disable_existing_loggers': True,
             'formatters': {
                 'detailed': {
                     'class': 'logging.Formatter',
                     'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'
                 },
                 'simple': {
                     'class': 'logging.Formatter',
                     'format': '%(name)-15s %(levelname)-8s %(processName)-10s %(message)s'
                 }
             },
             'handlers': {
                 'console': {
                     'class': 'logging.StreamHandler',
                     'level': 'INFO',
                     'formatter': 'simple',
                 },
                 'file': {
                     'class': 'logging.FileHandler',
                     'filename': 'mplog.log',
                     'mode': 'w',
                     'formatter': 'detailed',
                 },
                 'foofile': {
                     'class': 'logging.FileHandler',
                     'filename': 'mplog-foo.log',
                     'mode': 'w',
                     'formatter': 'detailed',
                 },
                 'errors': {
                     'class': 'logging.FileHandler',
                     'filename': 'mplog-errors.log',
                     'mode': 'w',
                     'level': 'ERROR',
                     'formatter': 'detailed',
                 },
             },
             'loggers': {
                 'foo': {
                     'handlers': ['foofile']
                 }
             },
             'root': {
                 'level': 'DEBUG',
                 'handlers': ['console', 'file', 'errors']
             },
         }
         # Log some initial events, just to show that logging in the parent works
         # normally.
         logging.config.dictConfig(config_initial)
         logger = logging.getLogger('setup')
         logger.info('About to create workers ...')
         workers = []
         for i in range(5):
             wp = Process(target=worker_process, name='worker %d' % (i + 1),
                          args=(config_worker,))
             workers.append(wp)
             wp.start()
             logger.info('Started worker: %s', wp.name)
         logger.info('About to create listener ...')
         stop_event = Event()
         lp = Process(target=listener_process, name='listener',
                      args=(q, stop_event, config_listener))
         lp.start()
         logger.info('Started listener')
         # We now hang around for the workers to finish their work.
         for wp in workers:
             wp.join()
         # Workers all done, listening can now stop.
         # Logging in the parent still works normally.
         logger.info('Telling listener to stop ...')
         stop_event.set()
         lp.join()
         logger.info('All done.')

     if __name__ == '__main__':
         main()


File: python.info,  Node: Inserting a BOM into messages sent to a SysLogHandler,  Next: Implementing structured logging,  Prev: A more elaborate multiprocessing example,  Up: Logging Cookbook

10.7.18 Inserting a BOM into messages sent to a SysLogHandler
-------------------------------------------------------------

RFC 5424(1) requires that a Unicode message be sent to a syslog daemon
as a set of bytes which have the following structure: an optional
pure-ASCII component, followed by a UTF-8 Byte Order Mark (BOM),
followed by Unicode encoded using UTF-8.  (See the relevant section of
the specification(2).)

In Python 3.1, code was added to *note SysLogHandler: 64d. to insert a
BOM into the message, but unfortunately, it was implemented incorrectly,
with the BOM appearing at the beginning of the message and hence not
allowing any pure-ASCII component to appear before it.

As this behaviour is broken, the incorrect BOM insertion code is being
removed from Python 3.2.4 and later.  However, it is not being replaced,
and if you want to produce RFC 5424-compliant messages which include a
BOM, an optional pure-ASCII sequence before it and arbitrary Unicode
after it, encoded using UTF-8, then you need to do the following:

  1. Attach a *note Formatter: 19da. instance to your *note
     SysLogHandler: 64d. instance, with a format string such as:

          'ASCII section\ufeffUnicode section'

     The Unicode code point U+FEFF, when encoded using UTF-8, will be
     encoded as a UTF-8 BOM – the byte-string ‘b'\xef\xbb\xbf'’.

  2. Replace the ASCII section with whatever placeholders you like, but
     make sure that the data that appears in there after substitution is
     always ASCII (that way, it will remain unchanged after UTF-8
     encoding).

  3. Replace the Unicode section with whatever placeholders you like; if
     the data which appears there after substitution contains characters
     outside the ASCII range, that’s fine – it will be encoded using
     UTF-8.

The formatted message `will' be encoded using UTF-8 encoding by
‘SysLogHandler’.  If you follow the above rules, you should be able to
produce RFC 5424-compliant messages.  If you don’t, logging may not
complain, but your messages will not be RFC 5424-compliant, and your
syslog daemon may complain.

   ---------- Footnotes ----------

   (1) http://tools.ietf.org/html/rfc5424

   (2) http://tools.ietf.org/html/rfc5424#section-6


File: python.info,  Node: Implementing structured logging,  Next: Customizing handlers with dictConfig,  Prev: Inserting a BOM into messages sent to a SysLogHandler,  Up: Logging Cookbook

10.7.19 Implementing structured logging
---------------------------------------

Although most logging messages are intended for reading by humans, and
thus not readily machine-parseable, there might be cirumstances where
you want to output messages in a structured format which `is' capable of
being parsed by a program (without needing complex regular expressions
to parse the log message).  This is straightforward to achieve using the
logging package.  There are a number of ways in which this could be
achieved, but the following is a simple approach which uses JSON to
serialise the event in a machine-parseable manner:

     import json
     import logging

     class StructuredMessage(object):
         def __init__(self, message, **kwargs):
             self.message = message
             self.kwargs = kwargs

         def __str__(self):
             return '%s >>> %s' % (self.message, json.dumps(self.kwargs))

     _ = StructuredMessage   # optional, to improve readability

     logging.basicConfig(level=logging.INFO, format='%(message)s')
     logging.info(_('message 1', foo='bar', bar='baz', num=123, fnum=123.456))

If the above script is run, it prints:

     message 1 >>> {"fnum": 123.456, "num": 123, "bar": "baz", "foo": "bar"}

Note that the order of items might be different according to the version
of Python used.

If you need more specialised processing, you can use a custom JSON
encoder, as in the following complete example:

     from __future__ import unicode_literals

     import json
     import logging

     # This next bit is to ensure the script runs unchanged on 2.x and 3.x
     try:
         unicode
     except NameError:
         unicode = str

     class Encoder(json.JSONEncoder):
         def default(self, o):
             if isinstance(o, set):
                 return tuple(o)
             elif isinstance(o, unicode):
                 return o.encode('unicode_escape').decode('ascii')
             return super(Encoder, self).default(o)

     class StructuredMessage(object):
         def __init__(self, message, **kwargs):
             self.message = message
             self.kwargs = kwargs

         def __str__(self):
             s = Encoder().encode(self.kwargs)
             return '%s >>> %s' % (self.message, s)

     _ = StructuredMessage   # optional, to improve readability

     def main():
         logging.basicConfig(level=logging.INFO, format='%(message)s')
         logging.info(_('message 1', set_value={1, 2, 3}, snowman='\u2603'))

     if __name__ == '__main__':
         main()

When the above script is run, it prints:

     message 1 >>> {"snowman": "\u2603", "set_value": [1, 2, 3]}

Note that the order of items might be different according to the version
of Python used.


File: python.info,  Node: Customizing handlers with dictConfig,  Next: Using particular formatting styles throughout your application,  Prev: Implementing structured logging,  Up: Logging Cookbook

10.7.20 Customizing handlers with ‘dictConfig()’
------------------------------------------------

There are times when you want to customize logging handlers in
particular ways, and if you use *note dictConfig(): 76d. you may be able
to do this without subclassing.  As an example, consider that you may
want to set the ownership of a log file.  On POSIX, this is easily done
using *note shutil.chown(): 6ad, but the file handlers in the stdlib
don’t offer built-in support.  You can customize handler creation using
a plain function such as:

     def owned_file_handler(filename, mode='a', encoding=None, owner=None):
         if owner:
             if not os.path.exists(filename):
                 open(filename, 'a').close()
             shutil.chown(filename, *owner)
         return logging.FileHandler(filename, mode, encoding)

You can then specify, in a logging configuration passed to *note
dictConfig(): 76d, that a logging handler be created by calling this
function:

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': False,
         'formatters': {
             'default': {
                 'format': '%(asctime)s %(levelname)s %(name)s %(message)s'
             },
         },
         'handlers': {
             'file':{
                 # The values below are popped from this dictionary and
                 # used to create the handler, set the handler's level and
                 # its formatter.
                 '()': owned_file_handler,
                 'level':'DEBUG',
                 'formatter': 'default',
                 # The values below are passed to the handler creator callable
                 # as keyword arguments.
                 'owner': ['pulse', 'pulse'],
                 'filename': 'chowntest.log',
                 'mode': 'w',
                 'encoding': 'utf-8',
             },
         },
         'root': {
             'handlers': ['file'],
             'level': 'DEBUG',
         },
     }

In this example I am setting the ownership using the ‘pulse’ user and
group, just for the purposes of illustration.  Putting it together into
a working script, ‘chowntest.py’:

     import logging, logging.config, os, shutil

     def owned_file_handler(filename, mode='a', encoding=None, owner=None):
         if owner:
             if not os.path.exists(filename):
                 open(filename, 'a').close()
             shutil.chown(filename, *owner)
         return logging.FileHandler(filename, mode, encoding)

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': False,
         'formatters': {
             'default': {
                 'format': '%(asctime)s %(levelname)s %(name)s %(message)s'
             },
         },
         'handlers': {
             'file':{
                 # The values below are popped from this dictionary and
                 # used to create the handler, set the handler's level and
                 # its formatter.
                 '()': owned_file_handler,
                 'level':'DEBUG',
                 'formatter': 'default',
                 # The values below are passed to the handler creator callable
                 # as keyword arguments.
                 'owner': ['pulse', 'pulse'],
                 'filename': 'chowntest.log',
                 'mode': 'w',
                 'encoding': 'utf-8',
             },
         },
         'root': {
             'handlers': ['file'],
             'level': 'DEBUG',
         },
     }

     logging.config.dictConfig(LOGGING)
     logger = logging.getLogger('mylogger')
     logger.debug('A debug message')

To run this, you will probably need to run as ‘root’:

     $ sudo python3.3 chowntest.py
     $ cat chowntest.log
     2013-11-05 09:34:51,128 DEBUG mylogger A debug message
     $ ls -l chowntest.log
     -rw-r--r-- 1 pulse pulse 55 2013-11-05 09:34 chowntest.log

Note that this example uses Python 3.3 because that’s where *note
shutil.chown(): 6ad. makes an appearance.  This approach should work
with any Python version that supports *note dictConfig(): 76d. - namely,
Python 2.7, 3.2 or later.  With pre-3.3 versions, you would need to
implement the actual ownership change using e.g.  *note os.chown(): 666.

In practice, the handler-creating function may be in a utility module
somewhere in your project.  Instead of the line in the configuration:

     '()': owned_file_handler,

you could use e.g.:

     '()': 'ext://project.util.owned_file_handler',

where ‘project.util’ can be replaced with the actual name of the package
where the function resides.  In the above working script, using
‘'ext://__main__.owned_file_handler'’ should work.  Here, the actual
callable is resolved by *note dictConfig(): 76d. from the ‘ext://’
specification.

This example hopefully also points the way to how you could implement
other types of file change - e.g.  setting specific POSIX permission
bits - in the same way, using *note os.chmod(): 665.

Of course, the approach could also be extended to types of handler other
than a *note FileHandler: 1a3f. - for example, one of the rotating file
handlers, or a different type of handler altogether.


File: python.info,  Node: Using particular formatting styles throughout your application,  Next: Configuring filters with dictConfig,  Prev: Customizing handlers with dictConfig,  Up: Logging Cookbook

10.7.21 Using particular formatting styles throughout your application
----------------------------------------------------------------------

In Python 3.2, the *note Formatter: 19da. gained a ‘style’ keyword
parameter which, while defaulting to ‘%’ for backward compatibility,
allowed the specification of ‘{’ or ‘$’ to support the formatting
approaches supported by *note str.format(): 14d. and *note
string.Template: 7c4.  Note that this governs the formatting of logging
messages for final output to logs, and is completely orthogonal to how
an individual logging message is constructed.

Logging calls (*note debug(): 2ca, *note info(): 19db. etc.)  only take
positional parameters for the actual logging message itself, with
keyword parameters used only for determining options for how to handle
the logging call (e.g.  the ‘exc_info’ keyword parameter to indicate
that traceback information should be logged, or the ‘extra’ keyword
parameter to indicate additional contextual information to be added to
the log).  So you cannot directly make logging calls using *note
str.format(): 14d. or *note string.Template: 7c4. syntax, because
internally the logging package uses %-formatting to merge the format
string and the variable arguments.  There would no changing this while
preserving backward compatibility, since all logging calls which are out
there in existing code will be using %-format strings.

There have been suggestions to associate format styles with specific
loggers, but that approach also runs into backward compatibility
problems because any existing code could be using a given logger name
and using %-formatting.

For logging to work interoperably between any third-party libraries and
your code, decisions about formatting need to be made at the level of
the individual logging call.  This opens up a couple of ways in which
alternative formatting styles can be accommodated.

* Menu:

* Using LogRecord factories:: 
* Using custom message objects:: 


File: python.info,  Node: Using LogRecord factories,  Next: Using custom message objects,  Up: Using particular formatting styles throughout your application

10.7.21.1 Using LogRecord factories
...................................

In Python 3.2, along with the *note Formatter: 19da. changes mentioned
above, the logging package gained the ability to allow users to set
their own *note LogRecord: 508. subclasses, using the *note
setLogRecordFactory(): 1a0e. function.  You can use this to set your own
subclass of *note LogRecord: 508, which does the Right Thing by
overriding the *note getMessage(): 1a0c. method.  The base class
implementation of this method is where the ‘msg % args’ formatting
happens, and where you can substitute your alternate formatting;
however, you should be careful to support all formatting styles and
allow %-formatting as the default, to ensure interoperability with other
code.  Care should also be taken to call ‘str(self.msg)’, just as the
base implementation does.

Refer to the reference documentation on *note setLogRecordFactory():
1a0e. and *note LogRecord: 508. for more information.


File: python.info,  Node: Using custom message objects,  Prev: Using LogRecord factories,  Up: Using particular formatting styles throughout your application

10.7.21.2 Using custom message objects
......................................

There is another, perhaps simpler way that you can use {}- and $-
formatting to construct your individual log messages.  You may recall
(from *note Using arbitrary objects as messages: 1a10.) that when
logging you can use an arbitrary object as a message format string, and
that the logging package will call *note str(): 25a. on that object to
get the actual format string.  Consider the following two classes:

     class BraceMessage(object):
         def __init__(self, fmt, *args, **kwargs):
             self.fmt = fmt
             self.args = args
             self.kwargs = kwargs

         def __str__(self):
             return self.fmt.format(*self.args, **self.kwargs)

     class DollarMessage(object):
         def __init__(self, fmt, **kwargs):
             self.fmt = fmt
             self.kwargs = kwargs

         def __str__(self):
             from string import Template
             return Template(self.fmt).substitute(**self.kwargs)

Either of these can be used in place of a format string, to allow {}- or
$-formatting to be used to build the actual "message" part which appears
in the formatted log output in place of “%(message)s” or “{message}” or
“$message”.  If you find it a little unwieldy to use the class names
whenever you want to log something, you can make it more palatable if
you use an alias such as ‘M’ or ‘_’ for the message (or perhaps ‘__’, if
you are using ‘_’ for localization).

Examples of this approach are given below.  Firstly, formatting with
*note str.format(): 14d.:

     >>> __ = BraceMessage
     >>> print(__('Message with {0} {1}', 2, 'placeholders'))
     Message with 2 placeholders
     >>> class Point: pass
     ...
     >>> p = Point()
     >>> p.x = 0.5
     >>> p.y = 0.5
     >>> print(__('Message with coordinates: ({point.x:.2f}, {point.y:.2f})', point=p))
     Message with coordinates: (0.50, 0.50)

Secondly, formatting with *note string.Template: 7c4.:

     >>> __ = DollarMessage
     >>> print(__('Message with $num $what', num=2, what='placeholders'))
     Message with 2 placeholders
     >>>

One thing to note is that you pay no significant performance penalty
with this approach: the actual formatting happens not when you make the
logging call, but when (and if) the logged message is actually about to
be output to a log by a handler.  So the only slightly unusual thing
which might trip you up is that the parentheses go around the format
string and the arguments, not just the format string.  That’s because
the __ notation is just syntax sugar for a constructor call to one of
the ‘XXXMessage’ classes shown above.


File: python.info,  Node: Configuring filters with dictConfig,  Next: Customized exception formatting,  Prev: Using particular formatting styles throughout your application,  Up: Logging Cookbook

10.7.22 Configuring filters with ‘dictConfig()’
-----------------------------------------------

You `can' configure filters using *note dictConfig(): 76d, though it
might not be obvious at first glance how to do it (hence this recipe).
Since *note Filter: 7c7. is the only filter class included in the
standard library, and it is unlikely to cater to many requirements (it’s
only there as a base class), you will typically need to define your own
*note Filter: 7c7. subclass with an overridden *note filter(): 1a06.
method.  To do this, specify the ‘()’ key in the configuration
dictionary for the filter, specifying a callable which will be used to
create the filter (a class is the most obvious, but you can provide any
callable which returns a *note Filter: 7c7. instance).  Here is a
complete example:

     import logging
     import logging.config
     import sys

     class MyFilter(logging.Filter):
         def __init__(self, param=None):
             self.param = param

         def filter(self, record):
             if self.param is None:
                 allow = True
             else:
                 allow = self.param not in record.msg
             if allow:
                 record.msg = 'changed: ' + record.msg
             return allow

     LOGGING = {
         'version': 1,
         'filters': {
             'myfilter': {
                 '()': MyFilter,
                 'param': 'noshow',
             }
         },
         'handlers': {
             'console': {
                 'class': 'logging.StreamHandler',
                 'filters': ['myfilter']
             }
         },
         'root': {
             'level': 'DEBUG',
             'handlers': ['console']
         },
     }

     if __name__ == '__main__':
         logging.config.dictConfig(LOGGING)
         logging.debug('hello')
         logging.debug('hello - noshow')

This example shows how you can pass configuration data to the callable
which constructs the instance, in the form of keyword parameters.  When
run, the above script will print:

     changed: hello

which shows that the filter is working as configured.

A couple of extra points to note:

   * If you can’t refer to the callable directly in the configuration
     (e.g.  if it lives in a different module, and you can’t import it
     directly where the configuration dictionary is), you can use the
     form ‘ext://...’ as described in *note Access to external objects:
     1a35.  For example, you could have used the text
     ‘'ext://__main__.MyFilter'’ instead of ‘MyFilter’ in the above
     example.

   * As well as for filters, this technique can also be used to
     configure custom handlers and formatters.  See *note User-defined
     objects: 1a2f. for more information on how logging supports using
     user-defined objects in its configuration, and see the other
     cookbook recipe *note Customizing handlers with dictConfig(): 38bd.
     above.


File: python.info,  Node: Customized exception formatting,  Next: Speaking logging messages,  Prev: Configuring filters with dictConfig,  Up: Logging Cookbook

10.7.23 Customized exception formatting
---------------------------------------

There might be times when you want to do customized exception formatting
- for argument’s sake, let’s say you want exactly one line per logged
event, even when exception information is present.  You can do this with
a custom formatter class, as shown in the following example:

     import logging

     class OneLineExceptionFormatter(logging.Formatter):
         def formatException(self, exc_info):
             """
             Format an exception so that it prints on a single line.
             """
             result = super(OneLineExceptionFormatter, self).formatException(exc_info)
             return repr(result) # or format into one line however you want to

         def format(self, record):
             s = super(OneLineExceptionFormatter, self).format(record)
             if record.exc_text:
                 s = s.replace('\n', '') + '|'
             return s

     def configure_logging():
         fh = logging.FileHandler('output.txt', 'w')
         f = OneLineExceptionFormatter('%(asctime)s|%(levelname)s|%(message)s|',
                                       '%d/%m/%Y %H:%M:%S')
         fh.setFormatter(f)
         root = logging.getLogger()
         root.setLevel(logging.DEBUG)
         root.addHandler(fh)

     def main():
         configure_logging()
         logging.info('Sample message')
         try:
             x = 1 / 0
         except ZeroDivisionError as e:
             logging.exception('ZeroDivisionError: %s', e)

     if __name__ == '__main__':
         main()

When run, this produces a file with exactly two lines:

     28/01/2015 07:21:23|INFO|Sample message|
     28/01/2015 07:21:23|ERROR|ZeroDivisionError: integer division or modulo by zero|'Traceback (most recent call last):\n  File "logtest7.py", line 30, in main\n    x = 1 / 0\nZeroDivisionError: integer division or modulo by zero'|

While the above treatment is simplistic, it points the way to how
exception information can be formatted to your liking.  The *note
traceback: 110. module may be helpful for more specialized needs.


File: python.info,  Node: Speaking logging messages,  Next: Buffering logging messages and outputting them conditionally,  Prev: Customized exception formatting,  Up: Logging Cookbook

10.7.24 Speaking logging messages
---------------------------------

There might be situations when it is desirable to have logging messages
rendered in an audible rather than a visible format.  This is easy to do
if you have text- to-speech (TTS) functionality available in your
system, even if it doesn’t have a Python binding.  Most TTS systems have
a command line program you can run, and this can be invoked from a
handler using *note subprocess: f7.  It’s assumed here that TTS command
line programs won’t expect to interact with users or take a long time to
complete, and that the frequency of logged messages will be not so high
as to swamp the user with messages, and that it’s acceptable to have the
messages spoken one at a time rather than concurrently, The example
implementation below waits for one message to be spoken before the next
is processed, and this might cause other handlers to be kept waiting.
Here is a short example showing the approach, which assumes that the
‘espeak’ TTS package is available:

     import logging
     import subprocess
     import sys

     class TTSHandler(logging.Handler):
         def emit(self, record):
             msg = self.format(record)
             # Speak slowly in a female English voice
             cmd = ['espeak', '-s150', '-ven+f3', msg]
             p = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                  stderr=subprocess.STDOUT)
             # wait for the program to finish
             p.communicate()

     def configure_logging():
         h = TTSHandler()
         root = logging.getLogger()
         root.addHandler(h)
         # the default formatter just returns the message
         root.setLevel(logging.DEBUG)

     def main():
         logging.info('Hello')
         logging.debug('Goodbye')

     if __name__ == '__main__':
         configure_logging()
         sys.exit(main())

When run, this script should say "Hello" and then "Goodbye" in a female
voice.

The above approach can, of course, be adapted to other TTS systems and
even other systems altogether which can process messages via external
programs run from a command line.


File: python.info,  Node: Buffering logging messages and outputting them conditionally,  Next: Formatting times using UTC GMT via configuration,  Prev: Speaking logging messages,  Up: Logging Cookbook

10.7.25 Buffering logging messages and outputting them conditionally
--------------------------------------------------------------------

There might be situations where you want to log messages in a temporary
area and only output them if a certain condition occurs.  For example,
you may want to start logging debug events in a function, and if the
function completes without errors, you don’t want to clutter the log
with the collected debug information, but if there is an error, you want
all the debug information to be output as well as the error.

Here is an example which shows how you could do this using a decorator
for your functions where you want logging to behave this way.  It makes
use of the *note logging.handlers.MemoryHandler: 1a39, which allows
buffering of logged events until some condition occurs, at which point
the buffered events are ‘flushed’ - passed to another handler (the
‘target’ handler) for processing.  By default, the ‘MemoryHandler’
flushed when its buffer gets filled up or an event whose level is
greater than or equal to a specified threshold is seen.  You can use
this recipe with a more specialised subclass of ‘MemoryHandler’ if you
want custom flushing behavior.

The example script has a simple function, ‘foo’, which just cycles
through all the logging levels, writing to ‘sys.stderr’ to say what
level it’s about to log at, and then actually logging a message that
that level.  You can pass a parameter to ‘foo’ which, if true, will log
at ERROR and CRITICAL levels - otherwise, it only logs at DEBUG, INFO
and WARNING levels.

The script just arranges to decorate ‘foo’ with a decorator which will
do the conditional logging that’s required.  The decorator takes a
logger as a parameter and attaches a memory handler for the duration of
the call to the decorated function.  The decorator can be additionally
parameterised using a target handler, a level at which flushing should
occur, and a capacity for the buffer.  These default to a *note
StreamHandler: 7c5. which writes to ‘sys.stderr’, ‘logging.ERROR’ and
‘100’ respectively.

Here’s the script:

     import logging
     from logging.handlers import MemoryHandler
     import sys

     logger = logging.getLogger(__name__)
     logger.addHandler(logging.NullHandler())

     def log_if_errors(logger, target_handler=None, flush_level=None, capacity=None):
         if target_handler is None:
             target_handler = logging.StreamHandler()
         if flush_level is None:
             flush_level = logging.ERROR
         if capacity is None:
             capacity = 100
         handler = MemoryHandler(capacity, flushLevel=flush_level, target=target_handler)

         def decorator(fn):
             def wrapper(*args, **kwargs):
                 logger.addHandler(handler)
                 try:
                     return fn(*args, **kwargs)
                 except Exception:
                     logger.exception('call failed')
                     raise
                 finally:
                     super(MemoryHandler, handler).flush()
                     logger.removeHandler(handler)
             return wrapper

         return decorator

     def write_line(s):
         sys.stderr.write('%s\n' % s)

     def foo(fail=False):
         write_line('about to log at DEBUG ...')
         logger.debug('Actually logged at DEBUG')
         write_line('about to log at INFO ...')
         logger.info('Actually logged at INFO')
         write_line('about to log at WARNING ...')
         logger.warning('Actually logged at WARNING')
         if fail:
             write_line('about to log at ERROR ...')
             logger.error('Actually logged at ERROR')
             write_line('about to log at CRITICAL ...')
             logger.critical('Actually logged at CRITICAL')
         return fail

     decorated_foo = log_if_errors(logger)(foo)

     if __name__ == '__main__':
         logger.setLevel(logging.DEBUG)
         write_line('Calling undecorated foo with False')
         assert not foo(False)
         write_line('Calling undecorated foo with True')
         assert foo(True)
         write_line('Calling decorated foo with False')
         assert not decorated_foo(False)
         write_line('Calling decorated foo with True')
         assert decorated_foo(True)

When this script is run, the following output should be observed:

     Calling undecorated foo with False
     about to log at DEBUG ...
     about to log at INFO ...
     about to log at WARNING ...
     Calling undecorated foo with True
     about to log at DEBUG ...
     about to log at INFO ...
     about to log at WARNING ...
     about to log at ERROR ...
     about to log at CRITICAL ...
     Calling decorated foo with False
     about to log at DEBUG ...
     about to log at INFO ...
     about to log at WARNING ...
     Calling decorated foo with True
     about to log at DEBUG ...
     about to log at INFO ...
     about to log at WARNING ...
     about to log at ERROR ...
     Actually logged at DEBUG
     Actually logged at INFO
     Actually logged at WARNING
     Actually logged at ERROR
     about to log at CRITICAL ...
     Actually logged at CRITICAL

As you can see, actual logging output only occurs when an event is
logged whose severity is ERROR or greater, but in that case, any
previous events at lower severities are also logged.

You can of course use the conventional means of decoration:

     @log_if_errors(logger)
     def foo(fail=False):
         ...


File: python.info,  Node: Formatting times using UTC GMT via configuration,  Next: Using a context manager for selective logging,  Prev: Buffering logging messages and outputting them conditionally,  Up: Logging Cookbook

10.7.26 Formatting times using UTC (GMT) via configuration
----------------------------------------------------------

Sometimes you want to format times using UTC, which can be done using a
class such as ‘UTCFormatter’, shown below:

     import logging
     import time

     class UTCFormatter(logging.Formatter):
         converter = time.gmtime

and you can then use the ‘UTCFormatter’ in your code instead of *note
Formatter: 19da.  If you want to do that via configuration, you can use
the *note dictConfig(): 76d. API with an approach illustrated by the
following complete example:

     import logging
     import logging.config
     import time

     class UTCFormatter(logging.Formatter):
         converter = time.gmtime

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': False,
         'formatters': {
             'utc': {
                 '()': UTCFormatter,
                 'format': '%(asctime)s %(message)s',
             },
             'local': {
                 'format': '%(asctime)s %(message)s',
             }
         },
         'handlers': {
             'console1': {
                 'class': 'logging.StreamHandler',
                 'formatter': 'utc',
             },
             'console2': {
                 'class': 'logging.StreamHandler',
                 'formatter': 'local',
             },
         },
         'root': {
             'handlers': ['console1', 'console2'],
        }
     }

     if __name__ == '__main__':
         logging.config.dictConfig(LOGGING)
         logging.warning('The local time is %s', time.asctime())

When this script is run, it should print something like:

     2015-10-17 12:53:29,501 The local time is Sat Oct 17 13:53:29 2015
     2015-10-17 13:53:29,501 The local time is Sat Oct 17 13:53:29 2015

showing how the time is formatted both as local time and UTC, one for
each handler.


File: python.info,  Node: Using a context manager for selective logging,  Prev: Formatting times using UTC GMT via configuration,  Up: Logging Cookbook

10.7.27 Using a context manager for selective logging
-----------------------------------------------------

There are times when it would be useful to temporarily change the
logging configuration and revert it back after doing something.  For
this, a context manager is the most obvious way of saving and restoring
the logging context.  Here is a simple example of such a context
manager, which allows you to optionally change the logging level and add
a logging handler purely in the scope of the context manager:

     import logging
     import sys

     class LoggingContext(object):
         def __init__(self, logger, level=None, handler=None, close=True):
             self.logger = logger
             self.level = level
             self.handler = handler
             self.close = close

         def __enter__(self):
             if self.level is not None:
                 self.old_level = self.logger.level
                 self.logger.setLevel(self.level)
             if self.handler:
                 self.logger.addHandler(self.handler)

         def __exit__(self, et, ev, tb):
             if self.level is not None:
                 self.logger.setLevel(self.old_level)
             if self.handler:
                 self.logger.removeHandler(self.handler)
             if self.handler and self.close:
                 self.handler.close()
             # implicit return of None => don't swallow exceptions

If you specify a level value, the logger’s level is set to that value in
the scope of the with block covered by the context manager.  If you
specify a handler, it is added to the logger on entry to the block and
removed on exit from the block.  You can also ask the manager to close
the handler for you on block exit - you could do this if you don’t need
the handler any more.

To illustrate how it works, we can add the following block of code to
the above:

     if __name__ == '__main__':
         logger = logging.getLogger('foo')
         logger.addHandler(logging.StreamHandler())
         logger.setLevel(logging.INFO)
         logger.info('1. This should appear just once on stderr.')
         logger.debug('2. This should not appear.')
         with LoggingContext(logger, level=logging.DEBUG):
             logger.debug('3. This should appear once on stderr.')
         logger.debug('4. This should not appear.')
         h = logging.StreamHandler(sys.stdout)
         with LoggingContext(logger, level=logging.DEBUG, handler=h, close=True):
             logger.debug('5. This should appear twice - once on stderr and once on stdout.')
         logger.info('6. This should appear just once on stderr.')
         logger.debug('7. This should not appear.')

We initially set the logger’s level to ‘INFO’, so message #1 appears and
message #2 doesn’t.  We then change the level to ‘DEBUG’ temporarily in
the following ‘with’ block, and so message #3 appears.  After the block
exits, the logger’s level is restored to ‘INFO’ and so message #4
doesn’t appear.  In the next ‘with’ block, we set the level to ‘DEBUG’
again but also add a handler writing to ‘sys.stdout’.  Thus, message #5
appears twice on the console (once via ‘stderr’ and once via ‘stdout’).
After the ‘with’ statement’s completion, the status is as it was before
so message #6 appears (like message #1) whereas message #7 doesn’t (just
like message #2).

If we run the resulting script, the result is as follows:

     $ python logctx.py
     1. This should appear just once on stderr.
     3. This should appear once on stderr.
     5. This should appear twice - once on stderr and once on stdout.
     5. This should appear twice - once on stderr and once on stdout.
     6. This should appear just once on stderr.

If we run it again, but pipe ‘stderr’ to ‘/dev/null’, we see the
following, which is the only message written to ‘stdout’:

     $ python logctx.py 2>/dev/null
     5. This should appear twice - once on stderr and once on stdout.

Once again, but piping ‘stdout’ to ‘/dev/null’, we get:

     $ python logctx.py >/dev/null
     1. This should appear just once on stderr.
     3. This should appear once on stderr.
     5. This should appear twice - once on stderr and once on stdout.
     6. This should appear just once on stderr.

In this case, the message #5 printed to ‘stdout’ doesn’t appear, as
expected.

Of course, the approach described here can be generalised, for example
to attach logging filters temporarily.  Note that the above code works
in Python 2 as well as Python 3.


File: python.info,  Node: Regular Expression HOWTO,  Next: Socket Programming HOWTO,  Prev: Logging Cookbook,  Up: Python HOWTOs

10.8 Regular Expression HOWTO
=============================


Author: A.M. Kuchling <<amk@amk.ca>>

Abstract
........

This document is an introductory tutorial to using regular expressions
in Python with the *note re: db. module.  It provides a gentler
introduction than the corresponding section in the Library Reference.

* Menu:

* Introduction: Introduction<15>. 
* Simple Patterns:: 
* Using Regular Expressions:: 
* More Pattern Power:: 
* Modifying Strings:: 
* Common Problems:: 
* Feedback:: 


File: python.info,  Node: Introduction<15>,  Next: Simple Patterns,  Up: Regular Expression HOWTO

10.8.1 Introduction
-------------------

Regular expressions (called REs, or regexes, or regex patterns) are
essentially a tiny, highly specialized programming language embedded
inside Python and made available through the *note re: db. module.
Using this little language, you specify the rules for the set of
possible strings that you want to match; this set might contain English
sentences, or e-mail addresses, or TeX commands, or anything you like.
You can then ask questions such as "Does this string match the
pattern?", or "Is there a match for the pattern anywhere in this
string?".  You can also use REs to modify a string or to split it apart
in various ways.

Regular expression patterns are compiled into a series of bytecodes
which are then executed by a matching engine written in C. For advanced
use, it may be necessary to pay careful attention to how the engine will
execute a given RE, and write the RE in a certain way in order to
produce bytecode that runs faster.  Optimization isn’t covered in this
document, because it requires that you have a good understanding of the
matching engine’s internals.

The regular expression language is relatively small and restricted, so
not all possible string processing tasks can be done using regular
expressions.  There are also tasks that `can' be done with regular
expressions, but the expressions turn out to be very complicated.  In
these cases, you may be better off writing Python code to do the
processing; while Python code will be slower than an elaborate regular
expression, it will also probably be more understandable.


File: python.info,  Node: Simple Patterns,  Next: Using Regular Expressions,  Prev: Introduction<15>,  Up: Regular Expression HOWTO

10.8.2 Simple Patterns
----------------------

We’ll start by learning about the simplest possible regular expressions.
Since regular expressions are used to operate on strings, we’ll begin
with the most common task: matching characters.

For a detailed explanation of the computer science underlying regular
expressions (deterministic and non-deterministic finite automata), you
can refer to almost any textbook on writing compilers.

* Menu:

* Matching Characters:: 
* Repeating Things:: 


File: python.info,  Node: Matching Characters,  Next: Repeating Things,  Up: Simple Patterns

10.8.2.1 Matching Characters
............................

Most letters and characters will simply match themselves.  For example,
the regular expression ‘test’ will match the string ‘test’ exactly.
(You can enable a case-insensitive mode that would let this RE match
‘Test’ or ‘TEST’ as well; more about this later.)

There are exceptions to this rule; some characters are special
`metacharacters', and don’t match themselves.  Instead, they signal that
some out-of-the-ordinary thing should be matched, or they affect other
portions of the RE by repeating them or changing their meaning.  Much of
this document is devoted to discussing various metacharacters and what
they do.

Here’s a complete list of the metacharacters; their meanings will be
discussed in the rest of this HOWTO.

     . ^ $ * + ? { } [ ] \ | ( )

The first metacharacters we’ll look at are ‘[’ and ‘]’.  They’re used
for specifying a character class, which is a set of characters that you
wish to match.  Characters can be listed individually, or a range of
characters can be indicated by giving two characters and separating them
by a ‘'-'’.  For example, ‘[abc]’ will match any of the characters ‘a’,
‘b’, or ‘c’; this is the same as ‘[a-c]’, which uses a range to express
the same set of characters.  If you wanted to match only lowercase
letters, your RE would be ‘[a-z]’.

Metacharacters are not active inside classes.  For example, ‘[akm$]’
will match any of the characters ‘'a'’, ‘'k'’, ‘'m'’, or ‘'$'’; ‘'$'’ is
usually a metacharacter, but inside a character class it’s stripped of
its special nature.

You can match the characters not listed within the class by
`complementing' the set.  This is indicated by including a ‘'^'’ as the
first character of the class; ‘'^'’ outside a character class will
simply match the ‘'^'’ character.  For example, ‘[^5]’ will match any
character except ‘'5'’.

Perhaps the most important metacharacter is the backslash, ‘\’.  As in
Python string literals, the backslash can be followed by various
characters to signal various special sequences.  It’s also used to
escape all the metacharacters so you can still match them in patterns;
for example, if you need to match a ‘[’ or ‘\’, you can precede them
with a backslash to remove their special meaning: ‘\[’ or ‘\\’.

Some of the special sequences beginning with ‘'\'’ represent predefined
sets of characters that are often useful, such as the set of digits, the
set of letters, or the set of anything that isn’t whitespace.

Let’s take an example: ‘\w’ matches any alphanumeric character.  If the
regex pattern is expressed in bytes, this is equivalent to the class
‘[a-zA-Z0-9_]’.  If the regex pattern is a string, ‘\w’ will match all
the characters marked as letters in the Unicode database provided by the
*note unicodedata: 117. module.  You can use the more restricted
definition of ‘\w’ in a string pattern by supplying the *note re.ASCII:
3a1. flag when compiling the regular expression.

The following list of special sequences isn’t complete.  For a complete
list of sequences and expanded class definitions for Unicode string
patterns, see the last part of *note Regular Expression Syntax: 1101. in
the Standard Library reference.  In general, the Unicode versions match
any character that’s in the appropriate category in the Unicode
database.

‘\d’

     Matches any decimal digit; this is equivalent to the class ‘[0-9]’.

‘\D’

     Matches any non-digit character; this is equivalent to the class
     ‘[^0-9]’.

‘\s’

     Matches any whitespace character; this is equivalent to the class
     ‘[ \t\n\r\f\v]’.

‘\S’

     Matches any non-whitespace character; this is equivalent to the
     class ‘[^ \t\n\r\f\v]’.

‘\w’

     Matches any alphanumeric character; this is equivalent to the class
     ‘[a-zA-Z0-9_]’.

‘\W’

     Matches any non-alphanumeric character; this is equivalent to the
     class ‘[^a-zA-Z0-9_]’.

These sequences can be included inside a character class.  For example,
‘[\s,.]’ is a character class that will match any whitespace character,
or ‘','’ or ‘'.'’.

The final metacharacter in this section is ‘.’.  It matches anything
except a newline character, and there’s an alternate mode (‘re.DOTALL’)
where it will match even a newline.  ‘'.'’ is often used where you want
to match "any character".


File: python.info,  Node: Repeating Things,  Prev: Matching Characters,  Up: Simple Patterns

10.8.2.2 Repeating Things
.........................

Being able to match varying sets of characters is the first thing
regular expressions can do that isn’t already possible with the methods
available on strings.  However, if that was the only additional
capability of regexes, they wouldn’t be much of an advance.  Another
capability is that you can specify that portions of the RE must be
repeated a certain number of times.

The first metacharacter for repeating things that we’ll look at is ‘*’.
‘*’ doesn’t match the literal character ‘*’; instead, it specifies that
the previous character can be matched zero or more times, instead of
exactly once.

For example, ‘ca*t’ will match ‘ct’ (0 ‘a’ characters), ‘cat’ (1 ‘a’),
‘caaat’ (3 ‘a’ characters), and so forth.  The RE engine has various
internal limitations stemming from the size of C’s ‘int’ type that will
prevent it from matching over 2 billion ‘a’ characters; patterns are
usually not written to match that much data.

Repetitions such as ‘*’ are `greedy'; when repeating a RE, the matching
engine will try to repeat it as many times as possible.  If later
portions of the pattern don’t match, the matching engine will then back
up and try again with fewer repetitions.

A step-by-step example will make this more obvious.  Let’s consider the
expression ‘a[bcd]*b’.  This matches the letter ‘'a'’, zero or more
letters from the class ‘[bcd]’, and finally ends with a ‘'b'’.  Now
imagine matching this RE against the string ‘abcbd’.

Step       Matched         Explanation
                           
-----------------------------------------------------------------
                           
1          ‘a’             The ‘a’ in the RE matches.
                           
                           
2          ‘abcbd’         The engine matches ‘[bcd]*’, going
                           as far as it can, which is to the
                           end of the string.
                           
                           
3          `Failure'       The engine tries to match ‘b’, but
                           the current position is at the end
                           of the string, so it fails.
                           
                           
4          ‘abcb’          Back up, so that ‘[bcd]*’ matches
                           one less character.
                           
                           
5          `Failure'       Try ‘b’ again, but the current
                           position is at the last character,
                           which is a ‘'d'’.
                           
                           
6          ‘abc’           Back up again, so that ‘[bcd]*’ is
                           only matching ‘bc’.
                           
                           
6          ‘abcb’          Try ‘b’ again.  This time the
                           character at the current position
                           is ‘'b'’, so it succeeds.
                           

The end of the RE has now been reached, and it has matched ‘abcb’.  This
demonstrates how the matching engine goes as far as it can at first, and
if no match is found it will then progressively back up and retry the
rest of the RE again and again.  It will back up until it has tried zero
matches for ‘[bcd]*’, and if that subsequently fails, the engine will
conclude that the string doesn’t match the RE at all.

Another repeating metacharacter is ‘+’, which matches one or more times.
Pay careful attention to the difference between ‘*’ and ‘+’; ‘*’ matches
`zero' or more times, so whatever’s being repeated may not be present at
all, while ‘+’ requires at least `one' occurrence.  To use a similar
example, ‘ca+t’ will match ‘cat’ (1 ‘a’), ‘caaat’ (3 ‘a’’s), but won’t
match ‘ct’.

There are two more repeating qualifiers.  The question mark character,
‘?’, matches either once or zero times; you can think of it as marking
something as being optional.  For example, ‘home-?brew’ matches either
‘homebrew’ or ‘home-brew’.

The most complicated repeated qualifier is ‘{m,n}’, where `m' and `n'
are decimal integers.  This qualifier means there must be at least `m'
repetitions, and at most `n'.  For example, ‘a/{1,3}b’ will match ‘a/b’,
‘a//b’, and ‘a///b’.  It won’t match ‘ab’, which has no slashes, or
‘a////b’, which has four.

You can omit either `m' or `n'; in that case, a reasonable value is
assumed for the missing value.  Omitting `m' is interpreted as a lower
limit of 0, while omitting `n' results in an upper bound of infinity —
actually, the upper bound is the 2-billion limit mentioned earlier, but
that might as well be infinity.

Readers of a reductionist bent may notice that the three other
qualifiers can all be expressed using this notation.  ‘{0,}’ is the same
as ‘*’, ‘{1,}’ is equivalent to ‘+’, and ‘{0,1}’ is the same as ‘?’.
It’s better to use ‘*’, ‘+’, or ‘?’ when you can, simply because they’re
shorter and easier to read.


File: python.info,  Node: Using Regular Expressions,  Next: More Pattern Power,  Prev: Simple Patterns,  Up: Regular Expression HOWTO

10.8.3 Using Regular Expressions
--------------------------------

Now that we’ve looked at some simple regular expressions, how do we
actually use them in Python?  The *note re: db. module provides an
interface to the regular expression engine, allowing you to compile REs
into objects and then perform matches with them.

* Menu:

* Compiling Regular Expressions:: 
* The Backslash Plague:: 
* Performing Matches:: 
* Module-Level Functions: Module-Level Functions<2>. 
* Compilation Flags:: 


File: python.info,  Node: Compiling Regular Expressions,  Next: The Backslash Plague,  Up: Using Regular Expressions

10.8.3.1 Compiling Regular Expressions
......................................

Regular expressions are compiled into pattern objects, which have
methods for various operations such as searching for pattern matches or
performing string substitutions.

     >>> import re
     >>> p = re.compile('ab*')
     >>> p
     re.compile('ab*')

*note re.compile(): 110d. also accepts an optional `flags' argument,
used to enable various special features and syntax variations.  We’ll go
over the available settings later, but for now a single example will do:

     >>> p = re.compile('ab*', re.IGNORECASE)

The RE is passed to *note re.compile(): 110d. as a string.  REs are
handled as strings because regular expressions aren’t part of the core
Python language, and no special syntax was created for expressing them.
(There are applications that don’t need REs at all, so there’s no need
to bloat the language specification by including them.)  Instead, the
*note re: db. module is simply a C extension module included with
Python, just like the *note socket: ed. or *note zlib: 141. modules.

Putting REs in strings keeps the Python language simpler, but has one
disadvantage which is the topic of the next section.


File: python.info,  Node: The Backslash Plague,  Next: Performing Matches,  Prev: Compiling Regular Expressions,  Up: Using Regular Expressions

10.8.3.2 The Backslash Plague
.............................

As stated earlier, regular expressions use the backslash character
(‘'\'’) to indicate special forms or to allow special characters to be
used without invoking their special meaning.  This conflicts with
Python’s usage of the same character for the same purpose in string
literals.

Let’s say you want to write a RE that matches the string ‘\section’,
which might be found in a LaTeX file.  To figure out what to write in
the program code, start with the desired string to be matched.  Next,
you must escape any backslashes and other metacharacters by preceding
them with a backslash, resulting in the string ‘\\section’.  The
resulting string that must be passed to *note re.compile(): 110d. must
be ‘\\section’.  However, to express this as a Python string literal,
both backslashes must be escaped `again'.

Characters              Stage
                        
-----------------------------------------------------------------------
                        
‘\section’              Text string to be matched
                        
                        
‘\\section’             Escaped backslash for
                        *note re.compile(): 110d.
                        
                        
‘"\\\\section"’         Escaped backslashes for a string literal
                        

In short, to match a literal backslash, one has to write ‘'\\\\'’ as the
RE string, because the regular expression must be ‘\\’, and each
backslash must be expressed as ‘\\’ inside a regular Python string
literal.  In REs that feature backslashes repeatedly, this leads to lots
of repeated backslashes and makes the resulting strings difficult to
understand.

The solution is to use Python’s raw string notation for regular
expressions; backslashes are not handled in any special way in a string
literal prefixed with ‘'r'’, so ‘r"\n"’ is a two-character string
containing ‘'\'’ and ‘'n'’, while ‘"\n"’ is a one-character string
containing a newline.  Regular expressions will often be written in
Python code using this raw string notation.

Regular String          Raw string
                        
-----------------------------------------------
                        
‘"ab*"’                 ‘r"ab*"’
                        
                        
‘"\\\\section"’         ‘r"\\section"’
                        
                        
‘"\\w+\\s+\\1"’         ‘r"\w+\s+\1"’
                        


File: python.info,  Node: Performing Matches,  Next: Module-Level Functions<2>,  Prev: The Backslash Plague,  Up: Using Regular Expressions

10.8.3.3 Performing Matches
...........................

Once you have an object representing a compiled regular expression, what
do you do with it?  Pattern objects have several methods and attributes.
Only the most significant ones will be covered here; consult the *note
re: db. docs for a complete listing.

Method/Attribute       Purpose
                       
---------------------------------------------------------------------------
                       
‘match()’              Determine if the RE matches at the beginning of
                       the string.
                       
                       
‘search()’             Scan through a string, looking for any location
                       where this RE matches.
                       
                       
‘findall()’            Find all substrings where the RE matches, and
                       returns them as a list.
                       
                       
‘finditer()’           Find all substrings where the RE matches, and
                       returns them as an *note iterator: e4f.
                       

*note match(): 110f. and *note search(): 1110. return ‘None’ if no match
can be found.  If they’re successful, a *note match object: 49a.
instance is returned, containing information about the match: where it
starts and ends, the substring it matched, and more.

You can learn about this by interactively experimenting with the *note
re: db. module.  If you have *note tkinter: 109. available, you may also
want to look at Tools/demo/redemo.py(1), a demonstration program
included with the Python distribution.  It allows you to enter REs and
strings, and displays whether the RE matches or fails.  ‘redemo.py’ can
be quite useful when trying to debug a complicated RE. Phil Schwartz’s
Kodos(2) is also an interactive tool for developing and testing RE
patterns.

This HOWTO uses the standard Python interpreter for its examples.
First, run the Python interpreter, import the *note re: db. module, and
compile a RE:

     >>> import re
     >>> p = re.compile('[a-z]+')
     >>> p
     re.compile('[a-z]+')

Now, you can try matching various strings against the RE ‘[a-z]+’.  An
empty string shouldn’t match at all, since ‘+’ means ’one or more
repetitions’.  ‘match()’ should return ‘None’ in this case, which will
cause the interpreter to print no output.  You can explicitly print the
result of ‘match()’ to make this clear.

     >>> p.match("")
     >>> print(p.match(""))
     None

Now, let’s try it on a string that it should match, such as ‘tempo’.  In
this case, ‘match()’ will return a *note match object: 49a, so you
should store the result in a variable for later use.

     >>> m = p.match('tempo')
     >>> m  #doctest: +ELLIPSIS
     <_sre.SRE_Match object; span=(0, 5), match='tempo'>

Now you can query the *note match object: 49a. for information about the
matching string.  *note match object: 49a. instances also have several
methods and attributes; the most important ones are:

Method/Attribute       Purpose
                       
------------------------------------------------------------------------
                       
‘group()’              Return the string matched by the RE
                       
                       
‘start()’              Return the starting position of the match
                       
                       
‘end()’                Return the ending position of the match
                       
                       
‘span()’               Return a tuple containing the (start, end)
                       positions of the match
                       

Trying these methods will soon clarify their meaning:

     >>> m.group()
     'tempo'
     >>> m.start(), m.end()
     (0, 5)
     >>> m.span()
     (0, 5)

*note group(): 57e. returns the substring that was matched by the RE.
*note start(): 1124. and *note end(): 1125. return the starting and
ending index of the match.  *note span(): 1126. returns both start and
end indexes in a single tuple.  Since the ‘match()’ method only checks
if the RE matches at the start of a string, ‘start()’ will always be
zero.  However, the ‘search()’ method of patterns scans through the
string, so the match may not start at zero in that case.

     >>> print(p.match('::: message'))
     None
     >>> m = p.search('::: message'); print(m)  #doctest: +ELLIPSIS
     <_sre.SRE_Match object; span=(4, 11), match='message'>
     >>> m.group()
     'message'
     >>> m.span()
     (4, 11)

In actual programs, the most common style is to store the *note match
object: 49a. in a variable, and then check if it was ‘None’.  This
usually looks like:

     p = re.compile( ... )
     m = p.match( 'string goes here' )
     if m:
         print('Match found: ', m.group())
     else:
         print('No match')

Two pattern methods return all of the matches for a pattern.  *note
findall(): 1119. returns a list of matching strings:

     >>> p = re.compile('\d+')
     >>> p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')
     ['12', '11', '10']

‘findall()’ has to create the entire list before it can be returned as
the result.  The *note finditer(): 111a. method returns a sequence of
*note match object: 49a. instances as an *note iterator: e4f.:

     >>> iterator = p.finditer('12 drummers drumming, 11 ... 10 ...')
     >>> iterator  #doctest: +ELLIPSIS
     <callable_iterator object at 0x...>
     >>> for match in iterator:
     ...     print(match.span())
     ...
     (0, 2)
     (22, 24)
     (29, 31)

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Tools/demo/redemo.py

   (2) http://kodos.sourceforge.net/


File: python.info,  Node: Module-Level Functions<2>,  Next: Compilation Flags,  Prev: Performing Matches,  Up: Using Regular Expressions

10.8.3.4 Module-Level Functions
...............................

You don’t have to create a pattern object and call its methods; the
*note re: db. module also provides top-level functions called *note
match(): 811, *note search(): 810, *note findall(): 57d, *note sub():
2f5, and so forth.  These functions take the same arguments as the
corresponding pattern method with the RE string added as the first
argument, and still return either ‘None’ or a *note match object: 49a.
instance.

     >>> print(re.match(r'From\s+', 'Fromage amk'))
     None
     >>> re.match(r'From\s+', 'From amk Thu May 14 19:12:10 1998')  #doctest: +ELLIPSIS
     <_sre.SRE_Match object; span=(0, 5), match='From '>

Under the hood, these functions simply create a pattern object for you
and call the appropriate method on it.  They also store the compiled
object in a cache, so future calls using the same RE won’t need to parse
the pattern again and again.

Should you use these module-level functions, or should you get the
pattern and call its methods yourself?  If you’re accessing a regex
within a loop, pre-compiling it will save a few function calls.  Outside
of loops, there’s not much difference thanks to the internal cache.


File: python.info,  Node: Compilation Flags,  Prev: Module-Level Functions<2>,  Up: Using Regular Expressions

10.8.3.5 Compilation Flags
..........................

Compilation flags let you modify some aspects of how regular expressions
work.  Flags are available in the *note re: db. module under two names,
a long name such as ‘IGNORECASE’ and a short, one-letter form such as
‘I’.  (If you’re familiar with Perl’s pattern modifiers, the one-letter
forms use the same letters; the short form of *note re.VERBOSE: 1113. is
*note re.X: 110b, for example.)  Multiple flags can be specified by
bitwise OR-ing them; ‘re.I | re.M’ sets both the ‘I’ and ‘M’ flags, for
example.

Here’s a table of the available flags, followed by a more detailed
explanation of each one.

Flag                                  Meaning
                                      
---------------------------------------------------------------------------------------
                                      
‘ASCII’, ‘A’                          Makes several escapes like ‘\w’, ‘\b’, ‘\s’
                                      and ‘\d’ match only on ASCII characters with
                                      the respective property.
                                      
                                      
‘DOTALL’, ‘S’                         Make ‘.’ match any character, including
                                      newlines
                                      
                                      
‘IGNORECASE’, ‘I’                     Do case-insensitive matches
                                      
                                      
‘LOCALE’, ‘L’                         Do a locale-aware match
                                      
                                      
‘MULTILINE’, ‘M’                      Multi-line matching, affecting ‘^’ and ‘$’
                                      
                                      
‘VERBOSE’, ‘X’ (for ’extended’)       Enable verbose REs, which can be organized
                                      more cleanly and understandably.
                                      

 -- Data: I

 -- Data: IGNORECASE

     Perform case-insensitive matching; character class and literal
     strings will match letters by ignoring case.  For example, ‘[A-Z]’
     will match lowercase letters, too, and ‘Spam’ will match ‘Spam’,
     ‘spam’, or ‘spAM’.  This lowercasing doesn’t take the current
     locale into account; it will if you also set the ‘LOCALE’ flag.

 -- Data: L

 -- Data: LOCALE

     Make ‘\w’, ‘\W’, ‘\b’, and ‘\B’, dependent on the current locale
     instead of the Unicode database.

     Locales are a feature of the C library intended to help in writing
     programs that take account of language differences.  For example,
     if you’re processing French text, you’d want to be able to write
     ‘\w+’ to match words, but ‘\w’ only matches the character class
     ‘[A-Za-z]’; it won’t match ‘'é'’ or ‘'ç'’.  If your system is
     configured properly and a French locale is selected, certain C
     functions will tell the program that ‘'é'’ should also be
     considered a letter.  Setting the ‘LOCALE’ flag when compiling a
     regular expression will cause the resulting compiled object to use
     these C functions for ‘\w’; this is slower, but also enables ‘\w+’
     to match French words as you’d expect.

 -- Data: M

 -- Data: MULTILINE

     (‘^’ and ‘$’ haven’t been explained yet; they’ll be introduced in
     section *note More Metacharacters: 38d9.)

     Usually ‘^’ matches only at the beginning of the string, and ‘$’
     matches only at the end of the string and immediately before the
     newline (if any) at the end of the string.  When this flag is
     specified, ‘^’ matches at the beginning of the string and at the
     beginning of each line within the string, immediately following
     each newline.  Similarly, the ‘$’ metacharacter matches either at
     the end of the string and at the end of each line (immediately
     preceding each newline).

 -- Data: S

 -- Data: DOTALL

     Makes the ‘'.'’ special character match any character at all,
     including a newline; without this flag, ‘'.'’ will match anything
     `except' a newline.

 -- Data: A

 -- Data: ASCII

     Make ‘\w’, ‘\W’, ‘\b’, ‘\B’, ‘\s’ and ‘\S’ perform ASCII-only
     matching instead of full Unicode matching.  This is only meaningful
     for Unicode patterns, and is ignored for byte patterns.

 -- Data: X

 -- Data: VERBOSE

     This flag allows you to write regular expressions that are more
     readable by granting you more flexibility in how you can format
     them.  When this flag has been specified, whitespace within the RE
     string is ignored, except when the whitespace is in a character
     class or preceded by an unescaped backslash; this lets you organize
     and indent the RE more clearly.  This flag also lets you put
     comments within a RE that will be ignored by the engine; comments
     are marked by a ‘'#'’ that’s neither in a character class or
     preceded by an unescaped backslash.

     For example, here’s a RE that uses *note re.VERBOSE: 1113.; see how
     much easier it is to read?

          charref = re.compile(r"""
           &[#]                # Start of a numeric entity reference
           (
               0[0-7]+         # Octal form
             | [0-9]+          # Decimal form
             | x[0-9a-fA-F]+   # Hexadecimal form
           )
           ;                   # Trailing semicolon
          """, re.VERBOSE)

     Without the verbose setting, the RE would look like this:

          charref = re.compile("&#(0[0-7]+"
                               "|[0-9]+"
                               "|x[0-9a-fA-F]+);")

     In the above example, Python’s automatic concatenation of string
     literals has been used to break up the RE into smaller pieces, but
     it’s still more difficult to understand than the version using
     *note re.VERBOSE: 1113.


File: python.info,  Node: More Pattern Power,  Next: Modifying Strings,  Prev: Using Regular Expressions,  Up: Regular Expression HOWTO

10.8.4 More Pattern Power
-------------------------

So far we’ve only covered a part of the features of regular expressions.
In this section, we’ll cover some new metacharacters, and how to use
groups to retrieve portions of the text that was matched.

* Menu:

* More Metacharacters:: 
* Grouping:: 
* Non-capturing and Named Groups:: 
* Lookahead Assertions:: 


File: python.info,  Node: More Metacharacters,  Next: Grouping,  Up: More Pattern Power

10.8.4.1 More Metacharacters
............................

There are some metacharacters that we haven’t covered yet.  Most of them
will be covered in this section.

Some of the remaining metacharacters to be discussed are `zero-width
assertions'.  They don’t cause the engine to advance through the string;
instead, they consume no characters at all, and simply succeed or fail.
For example, ‘\b’ is an assertion that the current position is located
at a word boundary; the position isn’t changed by the ‘\b’ at all.  This
means that zero-width assertions should never be repeated, because if
they match once at a given location, they can obviously be matched an
infinite number of times.

‘|’

     Alternation, or the "or" operator.  If A and B are regular
     expressions, ‘A|B’ will match any string that matches either ‘A’ or
     ‘B’.  ‘|’ has very low precedence in order to make it work
     reasonably when you’re alternating multi-character strings.
     ‘Crow|Servo’ will match either ‘Crow’ or ‘Servo’, not ‘Cro’, a
     ‘'w'’ or an ‘'S'’, and ‘ervo’.

     To match a literal ‘'|'’, use ‘\|’, or enclose it inside a
     character class, as in ‘[|]’.

‘^’

     Matches at the beginning of lines.  Unless the ‘MULTILINE’ flag has
     been set, this will only match at the beginning of the string.  In
     ‘MULTILINE’ mode, this also matches immediately after each newline
     within the string.

     For example, if you wish to match the word ‘From’ only at the
     beginning of a line, the RE to use is ‘^From’.

          >>> print(re.search('^From', 'From Here to Eternity'))  #doctest: +ELLIPSIS
          <_sre.SRE_Match object; span=(0, 4), match='From'>
          >>> print(re.search('^From', 'Reciting From Memory'))
          None

‘$’

     Matches at the end of a line, which is defined as either the end of
     the string, or any location followed by a newline character.

          >>> print(re.search('}$', '{block}'))  #doctest: +ELLIPSIS
          <_sre.SRE_Match object; span=(6, 7), match='}'>
          >>> print(re.search('}$', '{block} '))
          None
          >>> print(re.search('}$', '{block}\n'))  #doctest: +ELLIPSIS
          <_sre.SRE_Match object; span=(6, 7), match='}'>

     To match a literal ‘'$'’, use ‘\$’ or enclose it inside a character
     class, as in ‘[$]’.

‘\A’

     Matches only at the start of the string.  When not in ‘MULTILINE’
     mode, ‘\A’ and ‘^’ are effectively the same.  In ‘MULTILINE’ mode,
     they’re different: ‘\A’ still matches only at the beginning of the
     string, but ‘^’ may match at any location inside the string that
     follows a newline character.

‘\Z’

     Matches only at the end of the string.

‘\b’

     Word boundary.  This is a zero-width assertion that matches only at
     the beginning or end of a word.  A word is defined as a sequence of
     alphanumeric characters, so the end of a word is indicated by
     whitespace or a non-alphanumeric character.

     The following example matches ‘class’ only when it’s a complete
     word; it won’t match when it’s contained inside another word.

          >>> p = re.compile(r'\bclass\b')
          >>> print(p.search('no class at all'))  #doctest: +ELLIPSIS
          <_sre.SRE_Match object; span=(3, 8), match='class'>
          >>> print(p.search('the declassified algorithm'))
          None
          >>> print(p.search('one subclass is'))
          None

     There are two subtleties you should remember when using this
     special sequence.  First, this is the worst collision between
     Python’s string literals and regular expression sequences.  In
     Python’s string literals, ‘\b’ is the backspace character, ASCII
     value 8.  If you’re not using raw strings, then Python will convert
     the ‘\b’ to a backspace, and your RE won’t match as you expect it
     to.  The following example looks the same as our previous RE, but
     omits the ‘'r'’ in front of the RE string.

          >>> p = re.compile('\bclass\b')
          >>> print(p.search('no class at all'))
          None
          >>> print(p.search('\b' + 'class' + '\b'))  #doctest: +ELLIPSIS
          <_sre.SRE_Match object; span=(0, 7), match='\x08class\x08'>

     Second, inside a character class, where there’s no use for this
     assertion, ‘\b’ represents the backspace character, for
     compatibility with Python’s string literals.

‘\B’

     Another zero-width assertion, this is the opposite of ‘\b’, only
     matching when the current position is not at a word boundary.


File: python.info,  Node: Grouping,  Next: Non-capturing and Named Groups,  Prev: More Metacharacters,  Up: More Pattern Power

10.8.4.2 Grouping
.................

Frequently you need to obtain more information than just whether the RE
matched or not.  Regular expressions are often used to dissect strings
by writing a RE divided into several subgroups which match different
components of interest.  For example, an RFC-822 header line is divided
into a header name and a value, separated by a ‘':'’, like this:

     From: author@example.com
     User-Agent: Thunderbird 1.5.0.9 (X11/20061227)
     MIME-Version: 1.0
     To: editor@example.com

This can be handled by writing a regular expression which matches an
entire header line, and has one group which matches the header name, and
another group which matches the header’s value.

Groups are marked by the ‘'('’, ‘')'’ metacharacters.  ‘'('’ and ‘')'’
have much the same meaning as they do in mathematical expressions; they
group together the expressions contained inside them, and you can repeat
the contents of a group with a repeating qualifier, such as ‘*’, ‘+’,
‘?’, or ‘{m,n}’.  For example, ‘(ab)*’ will match zero or more
repetitions of ‘ab’.

     >>> p = re.compile('(ab)*')
     >>> print(p.match('ababababab').span())
     (0, 10)

Groups indicated with ‘'('’, ‘')'’ also capture the starting and ending
index of the text that they match; this can be retrieved by passing an
argument to ‘group()’, ‘start()’, ‘end()’, and ‘span()’.  Groups are
numbered starting with 0.  Group 0 is always present; it’s the whole RE,
so *note match object: 49a. methods all have group 0 as their default
argument.  Later we’ll see how to express groups that don’t capture the
span of text that they match.

     >>> p = re.compile('(a)b')
     >>> m = p.match('ab')
     >>> m.group()
     'ab'
     >>> m.group(0)
     'ab'

Subgroups are numbered from left to right, from 1 upward.  Groups can be
nested; to determine the number, just count the opening parenthesis
characters, going from left to right.

     >>> p = re.compile('(a(b)c)d')
     >>> m = p.match('abcd')
     >>> m.group(0)
     'abcd'
     >>> m.group(1)
     'abc'
     >>> m.group(2)
     'b'

‘group()’ can be passed multiple group numbers at a time, in which case
it will return a tuple containing the corresponding values for those
groups.

     >>> m.group(2,1,2)
     ('b', 'abc', 'b')

The ‘groups()’ method returns a tuple containing the strings for all the
subgroups, from 1 up to however many there are.

     >>> m.groups()
     ('abc', 'b')

Backreferences in a pattern allow you to specify that the contents of an
earlier capturing group must also be found at the current location in
the string.  For example, ‘\1’ will succeed if the exact contents of
group 1 can be found at the current position, and fails otherwise.
Remember that Python’s string literals also use a backslash followed by
numbers to allow including arbitrary characters in a string, so be sure
to use a raw string when incorporating backreferences in a RE.

For example, the following RE detects doubled words in a string.

     >>> p = re.compile(r'(\b\w+)\s+\1')
     >>> p.search('Paris in the the spring').group()
     'the the'

Backreferences like this aren’t often useful for just searching through
a string — there are few text formats which repeat data in this way —
but you’ll soon find out that they’re `very' useful when performing
string substitutions.


File: python.info,  Node: Non-capturing and Named Groups,  Next: Lookahead Assertions,  Prev: Grouping,  Up: More Pattern Power

10.8.4.3 Non-capturing and Named Groups
.......................................

Elaborate REs may use many groups, both to capture substrings of
interest, and to group and structure the RE itself.  In complex REs, it
becomes difficult to keep track of the group numbers.  There are two
features which help with this problem.  Both of them use a common syntax
for regular expression extensions, so we’ll look at that first.

Perl 5 is well known for its powerful additions to standard regular
expressions.  For these new features the Perl developers couldn’t choose
new single-keystroke metacharacters or new special sequences beginning
with ‘\’ without making Perl’s regular expressions confusingly different
from standard REs.  If they chose ‘&’ as a new metacharacter, for
example, old expressions would be assuming that ‘&’ was a regular
character and wouldn’t have escaped it by writing ‘\&’ or ‘[&]’.

The solution chosen by the Perl developers was to use ‘(?...)’ as the
extension syntax.  ‘?’ immediately after a parenthesis was a syntax
error because the ‘?’ would have nothing to repeat, so this didn’t
introduce any compatibility problems.  The characters immediately after
the ‘?’ indicate what extension is being used, so ‘(?=foo)’ is one thing
(a positive lookahead assertion) and ‘(?:foo)’ is something else (a
non-capturing group containing the subexpression ‘foo’).

Python supports several of Perl’s extensions and adds an extension
syntax to Perl’s extension syntax.  If the first character after the
question mark is a ‘P’, you know that it’s an extension that’s specific
to Python.

Now that we’ve looked at the general extension syntax, we can return to
the features that simplify working with groups in complex REs.

Sometimes you’ll want to use a group to denote a part of a regular
expression, but aren’t interested in retrieving the group’s contents.
You can make this fact explicit by using a non-capturing group:
‘(?:...)’, where you can replace the ‘...’ with any other regular
expression.

     >>> m = re.match("([abc])+", "abc")
     >>> m.groups()
     ('c',)
     >>> m = re.match("(?:[abc])+", "abc")
     >>> m.groups()
     ()

Except for the fact that you can’t retrieve the contents of what the
group matched, a non-capturing group behaves exactly the same as a
capturing group; you can put anything inside it, repeat it with a
repetition metacharacter such as ‘*’, and nest it within other groups
(capturing or non-capturing).  ‘(?:...)’ is particularly useful when
modifying an existing pattern, since you can add new groups without
changing how all the other groups are numbered.  It should be mentioned
that there’s no performance difference in searching between capturing
and non-capturing groups; neither form is any faster than the other.

A more significant feature is named groups: instead of referring to them
by numbers, groups can be referenced by a name.

The syntax for a named group is one of the Python-specific extensions:
‘(?P<name>...)’.  `name' is, obviously, the name of the group.  Named
groups behave exactly like capturing groups, and additionally associate
a name with a group.  The *note match object: 49a. methods that deal
with capturing groups all accept either integers that refer to the group
by number or strings that contain the desired group’s name.  Named
groups are still given numbers, so you can retrieve information about a
group in two ways:

     >>> p = re.compile(r'(?P<word>\b\w+\b)')
     >>> m = p.search( '(((( Lots of punctuation )))' )
     >>> m.group('word')
     'Lots'
     >>> m.group(1)
     'Lots'

Named groups are handy because they let you use easily-remembered names,
instead of having to remember numbers.  Here’s an example RE from the
*note imaplib: 97. module:

     InternalDate = re.compile(r'INTERNALDATE "'
             r'(?P<day>[ 123][0-9])-(?P<mon>[A-Z][a-z][a-z])-'
             r'(?P<year>[0-9][0-9][0-9][0-9])'
             r' (?P<hour>[0-9][0-9]):(?P<min>[0-9][0-9]):(?P<sec>[0-9][0-9])'
             r' (?P<zonen>[-+])(?P<zoneh>[0-9][0-9])(?P<zonem>[0-9][0-9])'
             r'"')

It’s obviously much easier to retrieve ‘m.group('zonem')’, instead of
having to remember to retrieve group 9.

The syntax for backreferences in an expression such as ‘(...)\1’ refers
to the number of the group.  There’s naturally a variant that uses the
group name instead of the number.  This is another Python extension:
‘(?P=name)’ indicates that the contents of the group called `name'
should again be matched at the current point.  The regular expression
for finding doubled words, ‘(\b\w+)\s+\1’ can also be written as
‘(?P<word>\b\w+)\s+(?P=word)’:

     >>> p = re.compile(r'(?P<word>\b\w+)\s+(?P=word)')
     >>> p.search('Paris in the the spring').group()
     'the the'

