This is python.info, produced by makeinfo version 6.0 from python.texi.

     Python 3.6.0a0, May 06, 2016

     Georg Brandl

     Copyright © 2001-2016, Python Software Foundation

INFO-DIR-SECTION Documentation tools
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY


   Generated by Sphinx 1.4.1.


File: python.info,  Node: os path --- Common pathname manipulations,  Next: fileinput --- Iterate over lines from multiple input streams,  Prev: pathlib --- Object-oriented filesystem paths,  Up: File and Directory Access

5.11.2 ‘os.path’ — Common pathname manipulations
------------------------------------------------

This module implements some useful functions on pathnames.  To read or
write files see *note open(): 1e8, and for accessing the filesystem see
the *note os: c2. module.  The path parameters can be passed as either
strings, or bytes.  Applications are encouraged to represent file names
as (Unicode) character strings.  Unfortunately, some file names may not
be representable as strings on Unix, so applications that need to
support arbitrary file names on Unix should use bytes objects to
represent path names.  Vice versa, using bytes objects cannot represent
all file names on Windows (in the standard ‘mbcs’ encoding), hence
Windows applications should use string objects to access all files.

Unlike a unix shell, Python does not do any `automatic' path expansions.
Functions such as *note expanduser(): 1569. and *note expandvars(): 9ba.
can be invoked explicitly when an application desires shell-like path
expansion.  (See also the *note glob: 89. module.)

See also
........

The *note pathlib: c6. module offers high-level path objects.

     Note: All of these functions accept either only bytes or only
     string objects as their parameters.  The result is an object of the
     same type, if a path or file name is returned.

     Note: Since different operating systems have different path name
     conventions, there are several versions of this module in the
     standard library.  The *note os.path: c3. module is always the path
     module suitable for the operating system Python is running on, and
     therefore usable for local paths.  However, you can also import and
     use the individual modules if you want to manipulate a path that is
     `always' in one of the different formats.  They all have the same
     interface:

        * ‘posixpath’ for UNIX-style paths

        * ‘ntpath’ for Windows paths

        * *note macpath: ac. for old-style MacOS paths

 -- Function: os.path.abspath (path)

     Return a normalized absolutized version of the pathname `path'.  On
     most platforms, this is equivalent to calling the function *note
     normpath(): 923. as follows: ‘normpath(join(os.getcwd(), path))’.

 -- Function: os.path.basename (path)

     Return the base name of pathname `path'.  This is the second
     element of the pair returned by passing `path' to the function
     *note split(): 1585.  Note that the result of this function is
     different from the Unix ‘basename’ program; where ‘basename’ for
     ‘'/foo/bar/'’ returns ‘'bar'’, the *note basename(): 1584. function
     returns an empty string (‘''’).

 -- Function: os.path.commonpath (paths)

     Return the longest common sub-path of each pathname in the sequence
     `paths'.  Raise ValueError if `paths' contains both absolute and
     relative pathnames, or if `paths' is empty.  Unlike *note
     commonprefix(): 2e5, this returns a valid path.

     Availability: Unix, Windows

     New in version 3.5.

 -- Function: os.path.commonprefix (list)

     Return the longest path prefix (taken character-by-character) that
     is a prefix of all paths in `list'.  If `list' is empty, return the
     empty string (‘''’).

          Note: This function may return invalid paths because it works
          a character at a time.  To obtain a valid path, see *note
          commonpath(): 2e4.

               >>> os.path.commonprefix(['/usr/lib', '/usr/local/lib'])
               '/usr/l'

               >>> os.path.commonpath(['/usr/lib', '/usr/local/lib'])
               '/usr'

 -- Function: os.path.dirname (path)

     Return the directory name of pathname `path'.  This is the first
     element of the pair returned by passing `path' to the function
     *note split(): 1585.

 -- Function: os.path.exists (path)

     Return ‘True’ if `path' refers to an existing path or an open file
     descriptor.  Returns ‘False’ for broken symbolic links.  On some
     platforms, this function may return ‘False’ if permission is not
     granted to execute *note os.stat(): 1e2. on the requested file,
     even if the `path' physically exists.

     Changed in version 3.3: `path' can now be an integer: ‘True’ is
     returned if it is an open file descriptor, ‘False’ otherwise.

 -- Function: os.path.lexists (path)

     Return ‘True’ if `path' refers to an existing path.  Returns ‘True’
     for broken symbolic links.  Equivalent to *note exists(): 677. on
     platforms lacking *note os.lstat(): 668.

 -- Function: os.path.expanduser (path)

     On Unix and Windows, return the argument with an initial component
     of ‘~’ or ‘~user’ replaced by that `user'’s home directory.

     On Unix, an initial ‘~’ is replaced by the environment variable
     ‘HOME’ if it is set; otherwise the current user’s home directory is
     looked up in the password directory through the built-in module
     *note pwd: d4.  An initial ‘~user’ is looked up directly in the
     password directory.

     On Windows, ‘HOME’ and ‘USERPROFILE’ will be used if set, otherwise
     a combination of ‘HOMEPATH’ and ‘HOMEDRIVE’ will be used.  An
     initial ‘~user’ is handled by stripping the last directory
     component from the created user path derived above.

     If the expansion fails or if the path does not begin with a tilde,
     the path is returned unchanged.

 -- Function: os.path.expandvars (path)

     Return the argument with environment variables expanded.
     Substrings of the form ‘$name’ or ‘${name}’ are replaced by the
     value of environment variable `name'.  Malformed variable names and
     references to non-existing variables are left unchanged.

     On Windows, ‘%name%’ expansions are supported in addition to
     ‘$name’ and ‘${name}’.

 -- Function: os.path.getatime (path)

     Return the time of last access of `path'.  The return value is a
     number giving the number of seconds since the epoch (see the *note
     time: 107. module).  Raise *note OSError: 4b6. if the file does not
     exist or is inaccessible.

     If *note os.stat_float_times(): 72b. returns ‘True’, the result is
     a floating point number.

 -- Function: os.path.getmtime (path)

     Return the time of last modification of `path'.  The return value
     is a number giving the number of seconds since the epoch (see the
     *note time: 107. module).  Raise *note OSError: 4b6. if the file
     does not exist or is inaccessible.

     If *note os.stat_float_times(): 72b. returns ‘True’, the result is
     a floating point number.

 -- Function: os.path.getctime (path)

     Return the system’s ctime which, on some systems (like Unix) is the
     time of the last metadata change, and, on others (like Windows), is
     the creation time for `path'.  The return value is a number giving
     the number of seconds since the epoch (see the *note time: 107.
     module).  Raise *note OSError: 4b6. if the file does not exist or
     is inaccessible.

 -- Function: os.path.getsize (path)

     Return the size, in bytes, of `path'.  Raise *note OSError: 4b6. if
     the file does not exist or is inaccessible.

 -- Function: os.path.isabs (path)

     Return ‘True’ if `path' is an absolute pathname.  On Unix, that
     means it begins with a slash, on Windows that it begins with a
     (back)slash after chopping off a potential drive letter.

 -- Function: os.path.isfile (path)

     Return ‘True’ if `path' is an existing regular file.  This follows
     symbolic links, so both *note islink(): 158e. and *note isfile():
     158d. can be true for the same path.

 -- Function: os.path.isdir (path)

     Return ‘True’ if `path' is an existing directory.  This follows
     symbolic links, so both *note islink(): 158e. and *note isdir():
     158f. can be true for the same path.

 -- Function: os.path.islink (path)

     Return ‘True’ if `path' refers to a directory entry that is a
     symbolic link.  Always ‘False’ if symbolic links are not supported
     by the python runtime.

 -- Function: os.path.ismount (path)

     Return ‘True’ if pathname `path' is a `mount point': a point in a
     file system where a different file system has been mounted.  On
     POSIX, the function checks whether `path'’s parent, ‘path/..’, is
     on a different device than `path', or whether ‘path/..’ and `path'
     point to the same i-node on the same device — this should detect
     mount points for all Unix and POSIX variants.  On Windows, a drive
     letter root and a share UNC are always mount points, and for any
     other path ‘GetVolumePathName’ is called to see if it is different
     from the input path.

     New in version 3.4: Support for detecting non-root mount points on
     Windows.

 -- Function: os.path.join (path, *paths)

     Join one or more path components intelligently.  The return value
     is the concatenation of `path' and any members of `*paths' with
     exactly one directory separator (‘os.sep’) following each non-empty
     part except the last, meaning that the result will only end in a
     separator if the last part is empty.  If a component is an absolute
     path, all previous components are thrown away and joining continues
     from the absolute path component.

     On Windows, the drive letter is not reset when an absolute path
     component (e.g., ‘r'\foo'’) is encountered.  If a component
     contains a drive letter, all previous components are thrown away
     and the drive letter is reset.  Note that since there is a current
     directory for each drive, ‘os.path.join("c:", "foo")’ represents a
     path relative to the current directory on drive ‘C:’ (‘c:foo’), not
     ‘c:\foo’.

 -- Function: os.path.normcase (path)

     Normalize the case of a pathname.  On Unix and Mac OS X, this
     returns the path unchanged; on case-insensitive filesystems, it
     converts the path to lowercase.  On Windows, it also converts
     forward slashes to backward slashes.  Raise a TypeError if the type
     of `path' is not ‘str’ or ‘bytes’.

 -- Function: os.path.normpath (path)

     Normalize a pathname by collapsing redundant separators and
     up-level references so that ‘A//B’, ‘A/B/’, ‘A/./B’ and
     ‘A/foo/../B’ all become ‘A/B’.  This string manipulation may change
     the meaning of a path that contains symbolic links.  On Windows, it
     converts forward slashes to backward slashes.  To normalize case,
     use *note normcase(): 1590.

 -- Function: os.path.realpath (path)

     Return the canonical path of the specified filename, eliminating
     any symbolic links encountered in the path (if they are supported
     by the operating system).

 -- Function: os.path.relpath (path, start=os.curdir)

     Return a relative filepath to `path' either from the current
     directory or from an optional `start' directory.  This is a path
     computation: the filesystem is not accessed to confirm the
     existence or nature of `path' or `start'.

     `start' defaults to *note os.curdir: 1593.

     Availability: Unix, Windows.

 -- Function: os.path.samefile (path1, path2)

     Return ‘True’ if both pathname arguments refer to the same file or
     directory.  This is determined by the device number and i-node
     number and raises an exception if an *note os.stat(): 1e2. call on
     either pathname fails.

     Availability: Unix, Windows.

     Changed in version 3.2: Added Windows support.

     Changed in version 3.4: Windows now uses the same implementation as
     all other platforms.

 -- Function: os.path.sameopenfile (fp1, fp2)

     Return ‘True’ if the file descriptors `fp1' and `fp2' refer to the
     same file.

     Availability: Unix, Windows.

     Changed in version 3.2: Added Windows support.

 -- Function: os.path.samestat (stat1, stat2)

     Return ‘True’ if the stat tuples `stat1' and `stat2' refer to the
     same file.  These structures may have been returned by *note
     os.fstat(): 1ee, *note os.lstat(): 668, or *note os.stat(): 1e2.
     This function implements the underlying comparison used by *note
     samefile(): 47b. and *note sameopenfile(): 1594.

     Availability: Unix, Windows.

     Changed in version 3.4: Added Windows support.

 -- Function: os.path.split (path)

     Split the pathname `path' into a pair, ‘(head, tail)’ where `tail'
     is the last pathname component and `head' is everything leading up
     to that.  The `tail' part will never contain a slash; if `path'
     ends in a slash, `tail' will be empty.  If there is no slash in
     `path', `head' will be empty.  If `path' is empty, both `head' and
     `tail' are empty.  Trailing slashes are stripped from `head' unless
     it is the root (one or more slashes only).  In all cases,
     ‘join(head, tail)’ returns a path to the same location as `path'
     (but the strings may differ).  Also see the functions *note
     dirname(): 1586. and *note basename(): 1584.

 -- Function: os.path.splitdrive (path)

     Split the pathname `path' into a pair ‘(drive, tail)’ where `drive'
     is either a mount point or the empty string.  On systems which do
     not use drive specifications, `drive' will always be the empty
     string.  In all cases, ‘drive + tail’ will be the same as `path'.

     On Windows, splits a pathname into drive/UNC sharepoint and
     relative path.

     If the path contains a drive letter, drive will contain everything
     up to and including the colon.  e.g.  ‘splitdrive("c:/dir")’
     returns ‘("c:", "/dir")’

     If the path contains a UNC path, drive will contain the host name
     and share, up to but not including the fourth separator.  e.g.
     ‘splitdrive("//host/computer/dir")’ returns ‘("//host/computer",
     "/dir")’

 -- Function: os.path.splitext (path)

     Split the pathname `path' into a pair ‘(root, ext)’ such that ‘root
     + ext == path’, and `ext' is empty or begins with a period and
     contains at most one period.  Leading periods on the basename are
     ignored; ‘splitext('.cshrc')’ returns ‘('.cshrc', '')’.

 -- Function: os.path.splitunc (path)

     Deprecated since version 3.1: Use `splitdrive' instead.

     Split the pathname `path' into a pair ‘(unc, rest)’ so that `unc'
     is the UNC mount point (such as ‘r'\\host\mount'’), if present, and
     `rest' the rest of the path (such as ‘r'\path\file.ext'’).  For
     paths containing drive letters, `unc' will always be the empty
     string.

     Availability: Windows.

 -- Data: os.path.supports_unicode_filenames

     ‘True’ if arbitrary Unicode strings can be used as file names
     (within limitations imposed by the file system).


File: python.info,  Node: fileinput --- Iterate over lines from multiple input streams,  Next: stat --- Interpreting stat results,  Prev: os path --- Common pathname manipulations,  Up: File and Directory Access

5.11.3 ‘fileinput’ — Iterate over lines from multiple input streams
-------------------------------------------------------------------

`Source code:' Lib/fileinput.py(1)

__________________________________________________________________

This module implements a helper class and functions to quickly write a
loop over standard input or a list of files.  If you just want to read
or write one file see *note open(): 1e8.

The typical use is:

     import fileinput
     for line in fileinput.input():
         process(line)

This iterates over the lines of all files listed in ‘sys.argv[1:]’,
defaulting to ‘sys.stdin’ if the list is empty.  If a filename is ‘'-'’,
it is also replaced by ‘sys.stdin’.  To specify an alternative list of
filenames, pass it as the first argument to *note input(): 7d5.  A
single file name is also allowed.

All files are opened in text mode by default, but you can override this
by specifying the `mode' parameter in the call to *note input(): 7d5. or
*note FileInput: 159a.  If an I/O error occurs during opening or reading
a file, *note OSError: 4b6. is raised.

Changed in version 3.3: *note IOError: 5b0. used to be raised; it is now
an alias of *note OSError: 4b6.

If ‘sys.stdin’ is used more than once, the second and further use will
return no lines, except perhaps for interactive use, or if it has been
explicitly reset (e.g.  using ‘sys.stdin.seek(0)’).

Empty files are opened and immediately closed; the only time their
presence in the list of filenames is noticeable at all is when the last
file opened is empty.

Lines are returned with any newlines intact, which means that the last
line in a file may not have one.

You can control how files are opened by providing an opening hook via
the `openhook' parameter to *note fileinput.input(): 7d5. or *note
FileInput(): 159a.  The hook must be a function that takes two
arguments, `filename' and `mode', and returns an accordingly opened
file-like object.  Two useful hooks are already provided by this module.

The following function is the primary interface of this module:

 -- Function: fileinput.input (files=None, inplace=False, backup='',
          bufsize=0, mode='r', openhook=None)

     Create an instance of the *note FileInput: 159a. class.  The
     instance will be used as global state for the functions of this
     module, and is also returned to use during iteration.  The
     parameters to this function will be passed along to the constructor
     of the *note FileInput: 159a. class.

     The *note FileInput: 159a. instance can be used as a context
     manager in the *note with: 29d. statement.  In this example,
     `input' is closed after the *note with: 29d. statement is exited,
     even if an exception occurs:

          with fileinput.input(files=('spam.txt', 'eggs.txt')) as f:
              for line in f:
                  process(line)

     Changed in version 3.2: Can be used as a context manager.

     Deprecated since version 3.6, will be removed in version 3.8: The
     `bufsize' parameter.

The following functions use the global state created by *note
fileinput.input(): 7d5.; if there is no active state, *note
RuntimeError: 193. is raised.

 -- Function: fileinput.filename ()

     Return the name of the file currently being read.  Before the first
     line has been read, returns ‘None’.

 -- Function: fileinput.fileno ()

     Return the integer "file descriptor" for the current file.  When no
     file is opened (before the first line and between files), returns
     ‘-1’.

 -- Function: fileinput.lineno ()

     Return the cumulative line number of the line that has just been
     read.  Before the first line has been read, returns ‘0’.  After the
     last line of the last file has been read, returns the line number
     of that line.

 -- Function: fileinput.filelineno ()

     Return the line number in the current file.  Before the first line
     has been read, returns ‘0’.  After the last line of the last file
     has been read, returns the line number of that line within the
     file.

 -- Function: fileinput.isfirstline ()

     Returns true if the line just read is the first line of its file,
     otherwise returns false.

 -- Function: fileinput.isstdin ()

     Returns true if the last line was read from ‘sys.stdin’, otherwise
     returns false.

 -- Function: fileinput.nextfile ()

     Close the current file so that the next iteration will read the
     first line from the next file (if any); lines not read from the
     file will not count towards the cumulative line count.  The
     filename is not changed until after the first line of the next file
     has been read.  Before the first line has been read, this function
     has no effect; it cannot be used to skip the first file.  After the
     last line of the last file has been read, this function has no
     effect.

 -- Function: fileinput.close ()

     Close the sequence.

The class which implements the sequence behavior provided by the module
is available for subclassing as well:

 -- Class: fileinput.FileInput (files=None, inplace=False, backup='',
          bufsize=0, mode='r', openhook=None)

     Class *note FileInput: 159a. is the implementation; its methods
     *note filename(): 159b, *note fileno(): 159c, *note lineno(): 159d,
     *note filelineno(): 159e, *note isfirstline(): 159f, *note
     isstdin(): 15a0, *note nextfile(): 15a1. and *note close(): 15a2.
     correspond to the functions of the same name in the module.  In
     addition it has a *note readline(): faa. method which returns the
     next input line, and a *note __getitem__(): a84. method which
     implements the sequence behavior.  The sequence must be accessed in
     strictly sequential order; random access and *note readline(): faa.
     cannot be mixed.

     With `mode' you can specify which file mode will be passed to *note
     open(): 1e8.  It must be one of ‘'r'’, ‘'rU'’, ‘'U'’ and ‘'rb'’.

     The `openhook', when given, must be a function that takes two
     arguments, `filename' and `mode', and returns an accordingly opened
     file-like object.  You cannot use `inplace' and `openhook'
     together.

     A *note FileInput: 159a. instance can be used as a context manager
     in the *note with: 29d. statement.  In this example, `input' is
     closed after the *note with: 29d. statement is exited, even if an
     exception occurs:

          with FileInput(files=('spam.txt', 'eggs.txt')) as input:
              process(input)

     Changed in version 3.2: Can be used as a context manager.

     Deprecated since version 3.4: The ‘'rU'’ and ‘'U'’ modes.

     Deprecated since version 3.6, will be removed in version 3.8: The
     `bufsize' parameter.

`Optional in-place filtering:' if the keyword argument ‘inplace=True’ is
passed to *note fileinput.input(): 7d5. or to the *note FileInput: 159a.
constructor, the file is moved to a backup file and standard output is
directed to the input file (if a file of the same name as the backup
file already exists, it will be replaced silently).  This makes it
possible to write a filter that rewrites its input file in place.  If
the `backup' parameter is given (typically as ‘backup='.<some
extension>'’), it specifies the extension for the backup file, and the
backup file remains around; by default, the extension is ‘'.bak'’ and it
is deleted when the output file is closed.  In-place filtering is
disabled when standard input is read.

The two following opening hooks are provided by this module:

 -- Function: fileinput.hook_compressed (filename, mode)

     Transparently opens files compressed with gzip and bzip2
     (recognized by the extensions ‘'.gz'’ and ‘'.bz2'’) using the *note
     gzip: 8b. and *note bz2: 14. modules.  If the filename extension is
     not ‘'.gz'’ or ‘'.bz2'’, the file is opened normally (ie, using
     *note open(): 1e8. without any decompression).

     Usage example: ‘fi =
     fileinput.FileInput(openhook=fileinput.hook_compressed)’

 -- Function: fileinput.hook_encoded (encoding, errors=None)

     Returns a hook which opens each file with *note open(): 1e8, using
     the given `encoding' and `errors' to read the file.

     Usage example: ‘fi =
     fileinput.FileInput(openhook=fileinput.hook_encoded("utf-8",
     "surrogateescape"))’

     Changed in version 3.6: Added the optional `errors' parameter.

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/fileinput.py


File: python.info,  Node: stat --- Interpreting stat results,  Next: filecmp --- File and Directory Comparisons,  Prev: fileinput --- Iterate over lines from multiple input streams,  Up: File and Directory Access

5.11.4 ‘stat’ — Interpreting ‘stat()’ results
---------------------------------------------

`Source code:' Modules/_stat.c(1)

     Lib/stat.py(2)

__________________________________________________________________

The *note stat: f2. module defines constants and functions for
interpreting the results of *note os.stat(): 1e2, *note os.fstat(): 1ee.
and *note os.lstat(): 668. (if they exist).  For complete details about
the ‘stat()’, ‘fstat()’ and ‘lstat()’ calls, consult the documentation
for your system.

Changed in version 3.4: The stat module is backed by a C implementation.

The *note stat: f2. module defines the following functions to test for
specific file types:

 -- Function: stat.S_ISDIR (mode)

     Return non-zero if the mode is from a directory.

 -- Function: stat.S_ISCHR (mode)

     Return non-zero if the mode is from a character special device
     file.

 -- Function: stat.S_ISBLK (mode)

     Return non-zero if the mode is from a block special device file.

 -- Function: stat.S_ISREG (mode)

     Return non-zero if the mode is from a regular file.

 -- Function: stat.S_ISFIFO (mode)

     Return non-zero if the mode is from a FIFO (named pipe).

 -- Function: stat.S_ISLNK (mode)

     Return non-zero if the mode is from a symbolic link.

 -- Function: stat.S_ISSOCK (mode)

     Return non-zero if the mode is from a socket.

 -- Function: stat.S_ISDOOR (mode)

     Return non-zero if the mode is from a door.

     New in version 3.4.

 -- Function: stat.S_ISPORT (mode)

     Return non-zero if the mode is from an event port.

     New in version 3.4.

 -- Function: stat.S_ISWHT (mode)

     Return non-zero if the mode is from a whiteout.

     New in version 3.4.

Two additional functions are defined for more general manipulation of
the file’s mode:

 -- Function: stat.S_IMODE (mode)

     Return the portion of the file’s mode that can be set by *note
     os.chmod(): 665.—that is, the file’s permission bits, plus the
     sticky bit, set-group-id, and set-user-id bits (on systems that
     support them).

 -- Function: stat.S_IFMT (mode)

     Return the portion of the file’s mode that describes the file type
     (used by the ‘S_IS*()’ functions above).

Normally, you would use the ‘os.path.is*()’ functions for testing the
type of a file; the functions here are useful when you are doing
multiple tests of the same file and wish to avoid the overhead of the
‘stat()’ system call for each test.  These are also useful when checking
for information about a file that isn’t handled by *note os.path: c3,
like the tests for block and character devices.

Example:

     import os, sys
     from stat import *

     def walktree(top, callback):
         '''recursively descend the directory tree rooted at top,
            calling the callback function for each regular file'''

         for f in os.listdir(top):
             pathname = os.path.join(top, f)
             mode = os.stat(pathname).st_mode
             if S_ISDIR(mode):
                 # It's a directory, recurse into it
                 walktree(pathname, callback)
             elif S_ISREG(mode):
                 # It's a file, call the callback function
                 callback(pathname)
             else:
                 # Unknown file type, print a message
                 print('Skipping %s' % pathname)

     def visitfile(file):
         print('visiting', file)

     if __name__ == '__main__':
         walktree(sys.argv[1], visitfile)

An additional utility function is provided to convert a file’s mode in a
human readable string:

 -- Function: stat.filemode (mode)

     Convert a file’s mode to a string of the form ’-rwxrwxrwx’.

     New in version 3.3.

     Changed in version 3.4: The function supports *note S_IFDOOR: 4d4,
     *note S_IFPORT: 4d5. and *note S_IFWHT: 4d6.

All the variables below are simply symbolic indexes into the 10-tuple
returned by *note os.stat(): 1e2, *note os.fstat(): 1ee. or *note
os.lstat(): 668.

 -- Data: stat.ST_MODE

     Inode protection mode.

 -- Data: stat.ST_INO

     Inode number.

 -- Data: stat.ST_DEV

     Device inode resides on.

 -- Data: stat.ST_NLINK

     Number of links to the inode.

 -- Data: stat.ST_UID

     User id of the owner.

 -- Data: stat.ST_GID

     Group id of the owner.

 -- Data: stat.ST_SIZE

     Size in bytes of a plain file; amount of data waiting on some
     special files.

 -- Data: stat.ST_ATIME

     Time of last access.

 -- Data: stat.ST_MTIME

     Time of last modification.

 -- Data: stat.ST_CTIME

     The "ctime" as reported by the operating system.  On some systems
     (like Unix) is the time of the last metadata change, and, on others
     (like Windows), is the creation time (see platform documentation
     for details).

The interpretation of "file size" changes according to the file type.
For plain files this is the size of the file in bytes.  For FIFOs and
sockets under most flavors of Unix (including Linux in particular), the
"size" is the number of bytes waiting to be read at the time of the call
to *note os.stat(): 1e2, *note os.fstat(): 1ee, or *note os.lstat():
668.; this can sometimes be useful, especially for polling one of these
special files after a non-blocking open.  The meaning of the size field
for other character and block devices varies more, depending on the
implementation of the underlying system call.

The variables below define the flags used in the *note ST_MODE: 4d3.
field.

Use of the functions above is more portable than use of the first set of
flags:

 -- Data: stat.S_IFSOCK

     Socket.

 -- Data: stat.S_IFLNK

     Symbolic link.

 -- Data: stat.S_IFREG

     Regular file.

 -- Data: stat.S_IFBLK

     Block device.

 -- Data: stat.S_IFDIR

     Directory.

 -- Data: stat.S_IFCHR

     Character device.

 -- Data: stat.S_IFIFO

     FIFO.

 -- Data: stat.S_IFDOOR

     Door.

     New in version 3.4.

 -- Data: stat.S_IFPORT

     Event port.

     New in version 3.4.

 -- Data: stat.S_IFWHT

     Whiteout.

     New in version 3.4.

     Note: *note S_IFDOOR: 4d4, *note S_IFPORT: 4d5. or *note S_IFWHT:
     4d6. are defined as 0 when the platform does not have support for
     the file types.

The following flags can also be used in the `mode' argument of *note
os.chmod(): 665.:

 -- Data: stat.S_ISUID

     Set UID bit.

 -- Data: stat.S_ISGID

     Set-group-ID bit.  This bit has several special uses.  For a
     directory it indicates that BSD semantics is to be used for that
     directory: files created there inherit their group ID from the
     directory, not from the effective group ID of the creating process,
     and directories created there will also get the *note S_ISGID:
     15c3. bit set.  For a file that does not have the group execution
     bit (*note S_IXGRP: 15c4.) set, the set-group-ID bit indicates
     mandatory file/record locking (see also *note S_ENFMT: 15c5.).

 -- Data: stat.S_ISVTX

     Sticky bit.  When this bit is set on a directory it means that a
     file in that directory can be renamed or deleted only by the owner
     of the file, by the owner of the directory, or by a privileged
     process.

 -- Data: stat.S_IRWXU

     Mask for file owner permissions.

 -- Data: stat.S_IRUSR

     Owner has read permission.

 -- Data: stat.S_IWUSR

     Owner has write permission.

 -- Data: stat.S_IXUSR

     Owner has execute permission.

 -- Data: stat.S_IRWXG

     Mask for group permissions.

 -- Data: stat.S_IRGRP

     Group has read permission.

 -- Data: stat.S_IWGRP

     Group has write permission.

 -- Data: stat.S_IXGRP

     Group has execute permission.

 -- Data: stat.S_IRWXO

     Mask for permissions for others (not in group).

 -- Data: stat.S_IROTH

     Others have read permission.

 -- Data: stat.S_IWOTH

     Others have write permission.

 -- Data: stat.S_IXOTH

     Others have execute permission.

 -- Data: stat.S_ENFMT

     System V file locking enforcement.  This flag is shared with *note
     S_ISGID: 15c3.: file/record locking is enforced on files that do
     not have the group execution bit (*note S_IXGRP: 15c4.) set.

 -- Data: stat.S_IREAD

     Unix V7 synonym for *note S_IRUSR: 15c8.

 -- Data: stat.S_IWRITE

     Unix V7 synonym for *note S_IWUSR: 15c9.

 -- Data: stat.S_IEXEC

     Unix V7 synonym for *note S_IXUSR: 15ca.

The following flags can be used in the `flags' argument of *note
os.chflags(): 664.:

 -- Data: stat.UF_NODUMP

     Do not dump the file.

 -- Data: stat.UF_IMMUTABLE

     The file may not be changed.

 -- Data: stat.UF_APPEND

     The file may only be appended to.

 -- Data: stat.UF_OPAQUE

     The directory is opaque when viewed through a union stack.

 -- Data: stat.UF_NOUNLINK

     The file may not be renamed or deleted.

 -- Data: stat.UF_COMPRESSED

     The file is stored compressed (Mac OS X 10.6+).

 -- Data: stat.UF_HIDDEN

     The file should not be displayed in a GUI (Mac OS X 10.5+).

 -- Data: stat.SF_ARCHIVED

     The file may be archived.

 -- Data: stat.SF_IMMUTABLE

     The file may not be changed.

 -- Data: stat.SF_APPEND

     The file may only be appended to.

 -- Data: stat.SF_NOUNLINK

     The file may not be renamed or deleted.

 -- Data: stat.SF_SNAPSHOT

     The file is a snapshot file.

See the *BSD or Mac OS systems man page ‘chflags(2)’ for more
information.

On Windows, the following file attribute constants are available for use
when testing bits in the ‘st_file_attributes’ member returned by *note
os.stat(): 1e2.  See the Windows API documentation(3) for more detail on
the meaning of these constants.

 -- Data: stat.FILE_ATTRIBUTE_ARCHIVE
 -- Data: stat.FILE_ATTRIBUTE_COMPRESSED
 -- Data: stat.FILE_ATTRIBUTE_DEVICE
 -- Data: stat.FILE_ATTRIBUTE_DIRECTORY
 -- Data: stat.FILE_ATTRIBUTE_ENCRYPTED
 -- Data: stat.FILE_ATTRIBUTE_HIDDEN
 -- Data: stat.FILE_ATTRIBUTE_INTEGRITY_STREAM
 -- Data: stat.FILE_ATTRIBUTE_NORMAL
 -- Data: stat.FILE_ATTRIBUTE_NOT_CONTENT_INDEXED
 -- Data: stat.FILE_ATTRIBUTE_NO_SCRUB_DATA
 -- Data: stat.FILE_ATTRIBUTE_OFFLINE
 -- Data: stat.FILE_ATTRIBUTE_READONLY
 -- Data: stat.FILE_ATTRIBUTE_REPARSE_POINT
 -- Data: stat.FILE_ATTRIBUTE_SPARSE_FILE
 -- Data: stat.FILE_ATTRIBUTE_SYSTEM
 -- Data: stat.FILE_ATTRIBUTE_TEMPORARY
 -- Data: stat.FILE_ATTRIBUTE_VIRTUAL

     New in version 3.5.

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Modules/_stat.c

   (2) https://hg.python.org/cpython/file/default/Lib/stat.py

   (3) 
https://msdn.microsoft.com/en-us/library/windows/desktop/gg258117.aspx


File: python.info,  Node: filecmp --- File and Directory Comparisons,  Next: tempfile --- Generate temporary files and directories,  Prev: stat --- Interpreting stat results,  Up: File and Directory Access

5.11.5 ‘filecmp’ — File and Directory Comparisons
-------------------------------------------------

`Source code:' Lib/filecmp.py(1)

__________________________________________________________________

The *note filecmp: 7d. module defines functions to compare files and
directories, with various optional time/correctness trade-offs.  For
comparing files, see also the *note difflib: 35. module.

The *note filecmp: 7d. module defines the following functions:

 -- Function: filecmp.cmp (f1, f2, shallow=True)

     Compare the files named `f1' and `f2', returning ‘True’ if they
     seem equal, ‘False’ otherwise.

     If `shallow' is true, files with identical *note os.stat(): 1e2.
     signatures are taken to be equal.  Otherwise, the contents of the
     files are compared.

     Note that no external programs are called from this function,
     giving it portability and efficiency.

     This function uses a cache for past comparisons and the results,
     with cache entries invalidated if the *note os.stat(): 1e2.
     information for the file changes.  The entire cache may be cleared
     using *note clear_cache(): 43a.

 -- Function: filecmp.cmpfiles (dir1, dir2, common, shallow=True)

     Compare the files in the two directories `dir1' and `dir2' whose
     names are given by `common'.

     Returns three lists of file names: `match', `mismatch', `errors'.
     `match' contains the list of files that match, `mismatch' contains
     the names of those that don’t, and `errors' lists the names of
     files which could not be compared.  Files are listed in `errors' if
     they don’t exist in one of the directories, the user lacks
     permission to read them or if the comparison could not be done for
     some other reason.

     The `shallow' parameter has the same meaning and default value as
     for *note filecmp.cmp(): 15f4.

     For example, ‘cmpfiles('a', 'b', ['c', 'd/e'])’ will compare ‘a/c’
     with ‘b/c’ and ‘a/d/e’ with ‘b/d/e’.  ‘'c'’ and ‘'d/e'’ will each
     be in one of the three returned lists.

 -- Function: filecmp.clear_cache ()

     Clear the filecmp cache.  This may be useful if a file is compared
     so quickly after it is modified that it is within the mtime
     resolution of the underlying filesystem.

     New in version 3.4.

* Menu:

* The dircmp class:: 

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/filecmp.py


File: python.info,  Node: The dircmp class,  Up: filecmp --- File and Directory Comparisons

5.11.5.1 The ‘dircmp’ class
...........................

 -- Class: filecmp.dircmp (a, b, ignore=None, hide=None)

     Construct a new directory comparison object, to compare the
     directories `a' and `b'.  `ignore' is a list of names to ignore,
     and defaults to *note filecmp.DEFAULT_IGNORES: 43b.  `hide' is a
     list of names to hide, and defaults to ‘[os.curdir, os.pardir]’.

     The *note dircmp: 43c. class compares files by doing `shallow'
     comparisons as described for *note filecmp.cmp(): 15f4.

     The *note dircmp: 43c. class provides the following methods:

      -- Method: report ()

          Print (to *note sys.stdout: 1ba.) a comparison between `a' and
          `b'.

      -- Method: report_partial_closure ()

          Print a comparison between `a' and `b' and common immediate
          subdirectories.

      -- Method: report_full_closure ()

          Print a comparison between `a' and `b' and common
          subdirectories (recursively).

     The *note dircmp: 43c. class offers a number of interesting
     attributes that may be used to get various bits of information
     about the directory trees being compared.

     Note that via *note __getattr__(): 782. hooks, all attributes are
     computed lazily, so there is no speed penalty if only those
     attributes which are lightweight to compute are used.

      -- Attribute: left

          The directory `a'.

      -- Attribute: right

          The directory `b'.

      -- Attribute: left_list

          Files and subdirectories in `a', filtered by `hide' and
          `ignore'.

      -- Attribute: right_list

          Files and subdirectories in `b', filtered by `hide' and
          `ignore'.

      -- Attribute: common

          Files and subdirectories in both `a' and `b'.

      -- Attribute: left_only

          Files and subdirectories only in `a'.

      -- Attribute: right_only

          Files and subdirectories only in `b'.

      -- Attribute: common_dirs

          Subdirectories in both `a' and `b'.

      -- Attribute: common_files

          Files in both `a' and `b'.

      -- Attribute: common_funny

          Names in both `a' and `b', such that the type differs between
          the directories, or names for which *note os.stat(): 1e2.
          reports an error.

      -- Attribute: same_files

          Files which are identical in both `a' and `b', using the
          class’s file comparison operator.

      -- Attribute: diff_files

          Files which are in both `a' and `b', whose contents differ
          according to the class’s file comparison operator.

      -- Attribute: funny_files

          Files which are in both `a' and `b', but could not be
          compared.

      -- Attribute: subdirs

          A dictionary mapping names in *note common_dirs: 1602. to
          *note dircmp: 43c. objects.

 -- Attribute: filecmp.DEFAULT_IGNORES

     New in version 3.4.

     List of directories ignored by *note dircmp: 43c. by default.

Here is a simplified example of using the ‘subdirs’ attribute to search
recursively through two directories to show common different files:

     >>> from filecmp import dircmp
     >>> def print_diff_files(dcmp):
     ...     for name in dcmp.diff_files:
     ...         print("diff_file %s found in %s and %s" % (name, dcmp.left,
     ...               dcmp.right))
     ...     for sub_dcmp in dcmp.subdirs.values():
     ...         print_diff_files(sub_dcmp)
     ...
     >>> dcmp = dircmp('dir1', 'dir2') # doctest: +SKIP
     >>> print_diff_files(dcmp) # doctest: +SKIP


File: python.info,  Node: tempfile --- Generate temporary files and directories,  Next: glob --- Unix style pathname pattern expansion,  Prev: filecmp --- File and Directory Comparisons,  Up: File and Directory Access

5.11.6 ‘tempfile’ — Generate temporary files and directories
------------------------------------------------------------

`Source code:' Lib/tempfile.py(1)

__________________________________________________________________

This module creates temporary files and directories.  It works on all
supported platforms.  *note TemporaryFile: 160b, *note
NamedTemporaryFile: 9bf, *note TemporaryDirectory: 826, and *note
SpooledTemporaryFile: 6e0. are high-level interfaces which provide
automatic cleanup and can be used as context managers.  *note mkstemp():
160c. and *note mkdtemp(): 160d. are lower-level functions which require
manual cleanup.

All the user-callable functions and constructors take additional
arguments which allow direct control over the location and name of
temporary files and directories.  Files names used by this module
include a string of random characters which allows those files to be
securely created in shared temporary directories.  To maintain backward
compatibility, the argument order is somewhat odd; it is recommended to
use keyword arguments for clarity.

The module defines the following user-callable items:

 -- Function: tempfile.TemporaryFile (mode='w+b', buffering=None,
          encoding=None, newline=None, suffix=None, prefix=None,
          dir=None)

     Return a *note file-like object: 160e. that can be used as a
     temporary storage area.  The file is created securely, using the
     same rules as *note mkstemp(): 160c.  It will be destroyed as soon
     as it is closed (including an implicit close when the object is
     garbage collected).  Under Unix, the directory entry for the file
     is either not created at all or is removed immediately after the
     file is created.  Other platforms do not support this; your code
     should not rely on a temporary file created using this function
     having or not having a visible name in the file system.

     The resulting object can be used as a context manager (see *note
     Examples: 160f.).  On completion of the context or destruction of
     the file object the temporary file will be removed from the
     filesystem.

     The `mode' parameter defaults to ‘'w+b'’ so that the file created
     can be read and written without being closed.  Binary mode is used
     so that it behaves consistently on all platforms without regard for
     the data that is stored.  `buffering', `encoding' and `newline' are
     interpreted as for *note open(): 1e8.

     The `dir', `prefix' and `suffix' parameters have the same meaning
     and defaults as with *note mkstemp(): 160c.

     The returned object is a true file object on POSIX platforms.  On
     other platforms, it is a file-like object whose ‘file’ attribute is
     the underlying true file object.

     The *note os.O_TMPFILE: 47e. flag is used if it is available and
     works (Linux-specific, requires Linux kernel 3.11 or later).

     Changed in version 3.5: The *note os.O_TMPFILE: 47e. flag is now
     used if available.

 -- Function: tempfile.NamedTemporaryFile (mode='w+b', buffering=None,
          encoding=None, newline=None, suffix=None, prefix=None,
          dir=None, delete=True)

     This function operates exactly as *note TemporaryFile(): 160b.
     does, except that the file is guaranteed to have a visible name in
     the file system (on Unix, the directory entry is not unlinked).
     That name can be retrieved from the ‘name’ attribute of the
     returned file-like object.  Whether the name can be used to open
     the file a second time, while the named temporary file is still
     open, varies across platforms (it can be so used on Unix; it cannot
     on Windows NT or later).  If `delete' is true (the default), the
     file is deleted as soon as it is closed.  The returned object is
     always a file-like object whose ‘file’ attribute is the underlying
     true file object.  This file-like object can be used in a *note
     with: 29d. statement, just like a normal file.

 -- Function: tempfile.SpooledTemporaryFile (max_size=0, mode='w+b',
          buffering=None, encoding=None, newline=None, suffix=None,
          prefix=None, dir=None)

     This function operates exactly as *note TemporaryFile(): 160b.
     does, except that data is spooled in memory until the file size
     exceeds `max_size', or until the file’s ‘fileno()’ method is
     called, at which point the contents are written to disk and
     operation proceeds as with *note TemporaryFile(): 160b.

     The resulting file has one additional method, ‘rollover()’, which
     causes the file to roll over to an on-disk file regardless of its
     size.

     The returned object is a file-like object whose ‘_file’ attribute
     is either an *note io.BytesIO: 371. or *note io.StringIO: 41e.
     object (depending on whether binary or text `mode' was specified)
     or a true file object, depending on whether ‘rollover()’ has been
     called.  This file-like object can be used in a *note with: 29d.
     statement, just like a normal file.

     Changed in version 3.3: the truncate method now accepts a ‘size’
     argument.

 -- Function: tempfile.TemporaryDirectory (suffix=None, prefix=None,
          dir=None)

     This function securely creates a temporary directory using the same
     rules as *note mkdtemp(): 160d.  The resulting object can be used
     as a context manager (see *note Examples: 160f.).  On completion of
     the context or destruction of the temporary directory object the
     newly created temporary directory and all its contents are removed
     from the filesystem.

     The directory name can be retrieved from the ‘name’ attribute of
     the returned object.  When the returned object is used as a context
     manager, the ‘name’ will be assigned to the target of the *note as:
     8aa. clause in the *note with: 29d. statement, if there is one.

     The directory can be explicitly cleaned up by calling the
     ‘cleanup()’ method.

     New in version 3.2.

 -- Function: tempfile.mkstemp (suffix=None, prefix=None, dir=None,
          text=False)

     Creates a temporary file in the most secure manner possible.  There
     are no race conditions in the file’s creation, assuming that the
     platform properly implements the *note os.O_EXCL: 1610. flag for
     *note os.open(): 1f4.  The file is readable and writable only by
     the creating user ID. If the platform uses permission bits to
     indicate whether a file is executable, the file is executable by no
     one.  The file descriptor is not inherited by child processes.

     Unlike *note TemporaryFile(): 160b, the user of *note mkstemp():
     160c. is responsible for deleting the temporary file when done with
     it.

     If `suffix' is not ‘None’, the file name will end with that suffix,
     otherwise there will be no suffix.  *note mkstemp(): 160c. does not
     put a dot between the file name and the suffix; if you need one,
     put it at the beginning of `suffix'.

     If `prefix' is not ‘None’, the file name will begin with that
     prefix; otherwise, a default prefix is used.  The default is the
     return value of *note gettempprefix(): 1611. or *note
     gettempprefixb(): 1612, as appropriate.

     If `dir' is not ‘None’, the file will be created in that directory;
     otherwise, a default directory is used.  The default directory is
     chosen from a platform-dependent list, but the user of the
     application can control the directory location by setting the
     `TMPDIR', `TEMP' or `TMP' environment variables.  There is thus no
     guarantee that the generated filename will have any nice
     properties, such as not requiring quoting when passed to external
     commands via ‘os.popen()’.

     If any of `suffix', `prefix', and `dir' are not ‘None’, they must
     be the same type.  If they are bytes, the returned name will be
     bytes instead of str.  If you want to force a bytes return value
     with otherwise default behavior, pass ‘suffix=b''’.

     If `text' is specified, it indicates whether to open the file in
     binary mode (the default) or text mode.  On some platforms, this
     makes no difference.

     *note mkstemp(): 160c. returns a tuple containing an OS-level
     handle to an open file (as would be returned by *note os.open():
     1f4.) and the absolute pathname of that file, in that order.

     Changed in version 3.5: `suffix', `prefix', and `dir' may now be
     supplied in bytes in order to obtain a bytes return value.  Prior
     to this, only str was allowed.  `suffix' and `prefix' now accept
     and default to ‘None’ to cause an appropriate default value to be
     used.

 -- Function: tempfile.mkdtemp (suffix=None, prefix=None, dir=None)

     Creates a temporary directory in the most secure manner possible.
     There are no race conditions in the directory’s creation.  The
     directory is readable, writable, and searchable only by the
     creating user ID.

     The user of *note mkdtemp(): 160d. is responsible for deleting the
     temporary directory and its contents when done with it.

     The `prefix', `suffix', and `dir' arguments are the same as for
     *note mkstemp(): 160c.

     *note mkdtemp(): 160d. returns the absolute pathname of the new
     directory.

     Changed in version 3.5: `suffix', `prefix', and `dir' may now be
     supplied in bytes in order to obtain a bytes return value.  Prior
     to this, only str was allowed.  `suffix' and `prefix' now accept
     and default to ‘None’ to cause an appropriate default value to be
     used.

 -- Function: tempfile.gettempdir ()

     Return the name of the directory used for temporary files.  This
     defines the default value for the `dir' argument to all functions
     in this module.

     Python searches a standard list of directories to find one which
     the calling user can create files in.  The list is:

       1. The directory named by the ‘TMPDIR’ environment variable.

       2. The directory named by the ‘TEMP’ environment variable.

       3. The directory named by the ‘TMP’ environment variable.

       4. A platform-specific location:

             * On Windows, the directories ‘C:\TEMP’, ‘C:\TMP’, ‘\TEMP’,
               and ‘\TMP’, in that order.

             * On all other platforms, the directories ‘/tmp’,
               ‘/var/tmp’, and ‘/usr/tmp’, in that order.

       5. As a last resort, the current working directory.

     The result of this search is cached, see the description of *note
     tempdir: 1614. below.

 -- Function: tempfile.gettempdirb ()

     Same as *note gettempdir(): 1613. but the return value is in bytes.

     New in version 3.5.

 -- Function: tempfile.gettempprefix ()

     Return the filename prefix used to create temporary files.  This
     does not contain the directory component.

 -- Function: tempfile.gettempprefixb ()

     Same as *note gettempprefix(): 1611. but the return value is in
     bytes.

     New in version 3.5.

The module uses a global variable to store the name of the directory
used for temporary files returned by *note gettempdir(): 1613.  It can
be set directly to override the selection process, but this is
discouraged.  All functions in this module take a `dir' argument which
can be used to specify the directory and this is the recommend approach.

 -- Data: tempfile.tempdir

     When set to a value other than ‘None’, this variable defines the
     default value for the `dir' argument to all the functions defined
     in this module.

     If ‘tempdir’ is unset or ‘None’ at any call to any of the above
     functions except *note gettempprefix(): 1611. it is initialized
     following the algorithm described in *note gettempdir(): 1613.

* Menu:

* Examples: Examples<3>. 
* Deprecated functions and variables:: 

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/tempfile.py


File: python.info,  Node: Examples<3>,  Next: Deprecated functions and variables,  Up: tempfile --- Generate temporary files and directories

5.11.6.1 Examples
.................

Here are some examples of typical usage of the *note tempfile: 101.
module:

     >>> import tempfile

     # create a temporary file and write some data to it
     >>> fp = tempfile.TemporaryFile()
     >>> fp.write(b'Hello world!')
     # read data from file
     >>> fp.seek(0)
     >>> fp.read()
     b'Hello world!'
     # close the file, it will be removed
     >>> fp.close()

     # create a temporary file using a context manager
     >>> with tempfile.TemporaryFile() as fp:
     ...     fp.write(b'Hello world!')
     ...     fp.seek(0)
     ...     fp.read()
     b'Hello world!'
     >>>
     # file is now closed and removed

     # create a temporary directory using the context manager
     >>> with tempfile.TemporaryDirectory() as tmpdirname:
     ...     print('created temporary directory', tmpdirname)
     >>>
     # directory and contents have been removed


File: python.info,  Node: Deprecated functions and variables,  Prev: Examples<3>,  Up: tempfile --- Generate temporary files and directories

5.11.6.2 Deprecated functions and variables
...........................................

A historical way to create temporary files was to first generate a file
name with the *note mktemp(): 1618. function and then create a file
using this name.  Unfortunately this is not secure, because a different
process may create a file with this name in the time between the call to
*note mktemp(): 1618. and the subsequent attempt to create the file by
the first process.  The solution is to combine the two steps and create
the file immediately.  This approach is used by *note mkstemp(): 160c.
and the other functions described above.

 -- Function: tempfile.mktemp (suffix='', prefix='tmp', dir=None)

     Deprecated since version 2.3: Use *note mkstemp(): 160c. instead.

     Return an absolute pathname of a file that did not exist at the
     time the call is made.  The `prefix', `suffix', and `dir' arguments
     are similar to those of *note mkstemp(): 160c, except that bytes
     file names, ‘suffix=None’ and ‘prefix=None’ are not supported.

          Warning: Use of this function may introduce a security hole in
          your program.  By the time you get around to doing anything
          with the file name it returns, someone else may have beaten
          you to the punch.  *note mktemp(): 1618. usage can be replaced
          easily with *note NamedTemporaryFile(): 9bf, passing it the
          ‘delete=False’ parameter:

               >>> f = NamedTemporaryFile(delete=False)
               >>> f.name
               '/tmp/tmptjujjt'
               >>> f.write(b"Hello World!\n")
               13
               >>> f.close()
               >>> os.unlink(f.name)
               >>> os.path.exists(f.name)
               False


File: python.info,  Node: glob --- Unix style pathname pattern expansion,  Next: fnmatch --- Unix filename pattern matching,  Prev: tempfile --- Generate temporary files and directories,  Up: File and Directory Access

5.11.7 ‘glob’ — Unix style pathname pattern expansion
-----------------------------------------------------

`Source code:' Lib/glob.py(1)

__________________________________________________________________

The *note glob: 89. module finds all the pathnames matching a specified
pattern according to the rules used by the Unix shell, although results
are returned in arbitrary order.  No tilde expansion is done, but ‘*’,
‘?’, and character ranges expressed with ‘[]’ will be correctly matched.
This is done by using the *note os.listdir(): 675. and *note
fnmatch.fnmatch(): 161b. functions in concert, and not by actually
invoking a subshell.  Note that unlike *note fnmatch.fnmatch(): 161b,
*note glob: 89. treats filenames beginning with a dot (‘.’) as special
cases.  (For tilde and shell variable expansion, use *note
os.path.expanduser(): 1569. and *note os.path.expandvars(): 9ba.)

For a literal match, wrap the meta-characters in brackets.  For example,
‘'[?]'’ matches the character ‘'?'’.

See also
........

The *note pathlib: c6. module offers high-level path objects.

 -- Function: glob.glob (pathname, *, recursive=False)

     Return a possibly-empty list of path names that match `pathname',
     which must be a string containing a path specification.  `pathname'
     can be either absolute (like ‘/usr/src/Python-1.5/Makefile’) or
     relative (like ‘../../Tools/*/*.gif’), and can contain shell-style
     wildcards.  Broken symlinks are included in the results (as in the
     shell).

     If `recursive' is true, the pattern "‘**’" will match any files and
     zero or more directories and subdirectories.  If the pattern is
     followed by an ‘os.sep’, only directories and subdirectories match.

          Note: Using the "‘**’" pattern in large directory trees may
          consume an inordinate amount of time.

     Changed in version 3.5: Support for recursive globs using "‘**’".

 -- Function: glob.iglob (pathname, recursive=False)

     Return an *note iterator: e4f. which yields the same values as
     *note glob(): 89. without actually storing them all simultaneously.

 -- Function: glob.escape (pathname)

     Escape all special characters (‘'?'’, ‘'*'’ and ‘'['’).  This is
     useful if you want to match an arbitrary literal string that may
     have special characters in it.  Special characters in drive/UNC
     sharepoints are not escaped, e.g.  on Windows ‘escape('//?/c:/Quo
     vadis?.txt')’ returns ‘'//?/c:/Quo vadis[?].txt'’.

     New in version 3.4.

For example, consider a directory containing the following files:
‘1.gif’, ‘2.txt’, ‘card.gif’ and a subdirectory ‘sub’ which contains
only the file ‘3.txt’.  *note glob(): 89. will produce the following
results.  Notice how any leading components of the path are preserved.

     >>> import glob
     >>> glob.glob('./[0-9].*')
     ['./1.gif', './2.txt']
     >>> glob.glob('*.gif')
     ['1.gif', 'card.gif']
     >>> glob.glob('?.gif')
     ['1.gif']
     >>> glob.glob('**/*.txt', recursive=True)
     ['2.txt', 'sub/3.txt']
     >>> glob.glob('./**/', recursive=True)
     ['./', './sub/']

If the directory contains files starting with ‘.’ they won’t be matched
by default.  For example, consider a directory containing ‘card.gif’ and
‘.card.gif’:

     >>> import glob
     >>> glob.glob('*.gif')
     ['card.gif']
     >>> glob.glob('.c*')
     ['.card.gif']

See also
........

Module *note fnmatch: 7f.

     Shell-style filename (not path) expansion

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/glob.py


File: python.info,  Node: fnmatch --- Unix filename pattern matching,  Next: linecache --- Random access to text lines,  Prev: glob --- Unix style pathname pattern expansion,  Up: File and Directory Access

5.11.8 ‘fnmatch’ — Unix filename pattern matching
-------------------------------------------------

`Source code:' Lib/fnmatch.py(1)

__________________________________________________________________

This module provides support for Unix shell-style wildcards, which are
`not' the same as regular expressions (which are documented in the *note
re: db. module).  The special characters used in shell-style wildcards
are:

Pattern          Meaning
                 
----------------------------------------------------------
                 
‘*’              matches everything
                 
                 
‘?’              matches any single character
                 
                 
‘[seq]’          matches any character in `seq'
                 
                 
‘[!seq]’         matches any character not in `seq'
                 

For a literal match, wrap the meta-characters in brackets.  For example,
‘'[?]'’ matches the character ‘'?'’.

Note that the filename separator (‘'/'’ on Unix) is `not' special to
this module.  See module *note glob: 89. for pathname expansion (*note
glob: 89. uses *note fnmatch(): 7f. to match pathname segments).
Similarly, filenames starting with a period are not special for this
module, and are matched by the ‘*’ and ‘?’ patterns.

 -- Function: fnmatch.fnmatch (filename, pattern)

     Test whether the `filename' string matches the `pattern' string,
     returning *note True: 9ff. or *note False: 60d.  If the operating
     system is case-insensitive, then both parameters will be normalized
     to all lower- or upper-case before the comparison is performed.
     *note fnmatchcase(): 161e. can be used to perform a case-sensitive
     comparison, regardless of whether that’s standard for the operating
     system.

     This example will print all file names in the current directory
     with the extension ‘.txt’:

          import fnmatch
          import os

          for file in os.listdir('.'):
              if fnmatch.fnmatch(file, '*.txt'):
                  print(file)

 -- Function: fnmatch.fnmatchcase (filename, pattern)

     Test whether `filename' matches `pattern', returning *note True:
     9ff. or *note False: 60d.; the comparison is case-sensitive.

 -- Function: fnmatch.filter (names, pattern)

     Return the subset of the list of `names' that match `pattern'.  It
     is the same as ‘[n for n in names if fnmatch(n, pattern)]’, but
     implemented more efficiently.

 -- Function: fnmatch.translate (pattern)

     Return the shell-style `pattern' converted to a regular expression.

     Example:

          >>> import fnmatch, re
          >>>
          >>> regex = fnmatch.translate('*.txt')
          >>> regex
          '.*\\.txt\\Z(?ms)'
          >>> reobj = re.compile(regex)
          >>> reobj.match('foobar.txt')
          <_sre.SRE_Match object; span=(0, 10), match='foobar.txt'>

See also
........

Module *note glob: 89.

     Unix shell-style path expansion.

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/fnmatch.py


File: python.info,  Node: linecache --- Random access to text lines,  Next: shutil --- High-level file operations,  Prev: fnmatch --- Unix filename pattern matching,  Up: File and Directory Access

5.11.9 ‘linecache’ — Random access to text lines
------------------------------------------------

`Source code:' Lib/linecache.py(1)

__________________________________________________________________

The *note linecache: a6. module allows one to get any line from a Python
source file, while attempting to optimize internally, using a cache, the
common case where many lines are read from a single file.  This is used
by the *note traceback: 110. module to retrieve source lines for
inclusion in the formatted traceback.

The *note tokenize.open(): 1623. function is used to open files.  This
function uses *note tokenize.detect_encoding(): 1624. to get the
encoding of the file; in the absence of an encoding token, the file
encoding defaults to UTF-8.

The *note linecache: a6. module defines the following functions:

 -- Function: linecache.getline (filename, lineno, module_globals=None)

     Get line `lineno' from file named `filename'.  This function will
     never raise an exception — it will return ‘''’ on errors (the
     terminating newline character will be included for lines that are
     found).

     If a file named `filename' is not found, the function will look for
     it in the module search path, ‘sys.path’, after first checking for
     a PEP 302(2) ‘__loader__’ in `module_globals', in case the module
     was imported from a zipfile or other non-filesystem import source.

 -- Function: linecache.clearcache ()

     Clear the cache.  Use this function if you no longer need lines
     from files previously read using *note getline(): 2c2.

 -- Function: linecache.checkcache (filename=None)

     Check the cache for validity.  Use this function if files in the
     cache may have changed on disk, and you require the updated
     version.  If `filename' is omitted, it will check all the entries
     in the cache.

 -- Function: linecache.lazycache (filename, module_globals)

     Capture enough detail about a non-file-based module to permit
     getting its lines later via *note getline(): 2c2. even if
     `module_globals' is None in the later call.  This avoids doing I/O
     until a line is actually needed, without having to carry the module
     globals around indefinitely.

     New in version 3.5.

Example:

     >>> import linecache
     >>> linecache.getline(linecache.__file__, 8)
     'import sys\n'

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/linecache.py

   (2) https://www.python.org/dev/peps/pep-0302


File: python.info,  Node: shutil --- High-level file operations,  Next: macpath --- Mac OS 9 path manipulation functions,  Prev: linecache --- Random access to text lines,  Up: File and Directory Access

5.11.10 ‘shutil’ — High-level file operations
---------------------------------------------

`Source code:' Lib/shutil.py(1)

__________________________________________________________________

The *note shutil: e7. module offers a number of high-level operations on
files and collections of files.  In particular, functions are provided
which support file copying and removal.  For operations on individual
files, see also the *note os: c2. module.

     Warning: Even the higher-level file copying functions (*note
     shutil.copy(): 303, *note shutil.copy2(): 304.) cannot copy all
     file metadata.

     On POSIX platforms, this means that file owner and group are lost
     as well as ACLs.  On Mac OS, the resource fork and other metadata
     are not used.  This means that resources will be lost and file type
     and creator codes will not be correct.  On Windows, file owners,
     ACLs and alternate data streams are not copied.

* Menu:

* Directory and files operations:: 
* Archiving operations:: 
* Querying the size of the output terminal:: 

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/shutil.py


File: python.info,  Node: Directory and files operations,  Next: Archiving operations,  Up: shutil --- High-level file operations

5.11.10.1 Directory and files operations
........................................

 -- Function: shutil.copyfileobj (fsrc, fdst[, length])

     Copy the contents of the file-like object `fsrc' to the file-like
     object `fdst'.  The integer `length', if given, is the buffer size.
     In particular, a negative `length' value means to copy the data
     without looping over the source data in chunks; by default the data
     is read in chunks to avoid uncontrolled memory consumption.  Note
     that if the current file position of the `fsrc' object is not 0,
     only the contents from the current file position to the end of the
     file will be copied.

 -- Function: shutil.copyfile (src, dst, *, follow_symlinks=True)

     Copy the contents (no metadata) of the file named `src' to a file
     named `dst' and return `dst'.  `src' and `dst' are path names given
     as strings.  `dst' must be the complete target file name; look at
     *note shutil.copy(): 303. for a copy that accepts a target
     directory path.  If `src' and `dst' specify the same file, *note
     SameFileError: 4b1. is raised.

     The destination location must be writable; otherwise, an *note
     OSError: 4b6. exception will be raised.  If `dst' already exists,
     it will be replaced.  Special files such as character or block
     devices and pipes cannot be copied with this function.

     If `follow_symlinks' is false and `src' is a symbolic link, a new
     symbolic link will be created instead of copying the file `src'
     points to.

     Changed in version 3.3: *note IOError: 5b0. used to be raised
     instead of *note OSError: 4b6.  Added `follow_symlinks' argument.
     Now returns `dst'.

     Changed in version 3.4: Raise *note SameFileError: 4b1. instead of
     *note Error: 4b0.  Since the former is a subclass of the latter,
     this change is backward compatible.

 -- Exception: shutil.SameFileError

     This exception is raised if source and destination in *note
     copyfile(): 4af. are the same file.

     New in version 3.4.

 -- Function: shutil.copymode (src, dst, *, follow_symlinks=True)

     Copy the permission bits from `src' to `dst'.  The file contents,
     owner, and group are unaffected.  `src' and `dst' are path names
     given as strings.  If `follow_symlinks' is false, and both `src'
     and `dst' are symbolic links, *note copymode(): 162c. will attempt
     to modify the mode of `dst' itself (rather than the file it points
     to).  This functionality is not available on every platform; please
     see *note copystat(): 6ae. for more information.  If *note
     copymode(): 162c. cannot modify symbolic links on the local
     platform, and it is asked to do so, it will do nothing and return.

     Changed in version 3.3: Added `follow_symlinks' argument.

 -- Function: shutil.copystat (src, dst, *, follow_symlinks=True)

     Copy the permission bits, last access time, last modification time,
     and flags from `src' to `dst'.  On Linux, *note copystat(): 6ae.
     also copies the "extended attributes" where possible.  The file
     contents, owner, and group are unaffected.  `src' and `dst' are
     path names given as strings.

     If `follow_symlinks' is false, and `src' and `dst' both refer to
     symbolic links, *note copystat(): 6ae. will operate on the symbolic
     links themselves rather than the files the symbolic links refer
     to–reading the information from the `src' symbolic link, and
     writing the information to the `dst' symbolic link.

          Note: Not all platforms provide the ability to examine and
          modify symbolic links.  Python itself can tell you what
          functionality is locally available.

             * If ‘os.chmod in os.supports_follow_symlinks’ is ‘True’,
               *note copystat(): 6ae. can modify the permission bits of
               a symbolic link.

             * If ‘os.utime in os.supports_follow_symlinks’ is ‘True’,
               *note copystat(): 6ae. can modify the last access and
               modification times of a symbolic link.

             * If ‘os.chflags in os.supports_follow_symlinks’ is ‘True’,
               *note copystat(): 6ae. can modify the flags of a symbolic
               link.  (‘os.chflags’ is not available on all platforms.)

          On platforms where some or all of this functionality is
          unavailable, when asked to modify a symbolic link, *note
          copystat(): 6ae. will copy everything it can.  *note
          copystat(): 6ae. never returns failure.

          Please see *note os.supports_follow_symlinks: 162d. for more
          information.

     Changed in version 3.3: Added `follow_symlinks' argument and
     support for Linux extended attributes.

 -- Function: shutil.copy (src, dst, *, follow_symlinks=True)

     Copies the file `src' to the file or directory `dst'.  `src' and
     `dst' should be strings.  If `dst' specifies a directory, the file
     will be copied into `dst' using the base filename from `src'.
     Returns the path to the newly created file.

     If `follow_symlinks' is false, and `src' is a symbolic link, `dst'
     will be created as a symbolic link.  If `follow_symlinks' is true
     and `src' is a symbolic link, `dst' will be a copy of the file
     `src' refers to.

     *note copy(): 25. copies the file data and the file’s permission
     mode (see *note os.chmod(): 665.).  Other metadata, like the file’s
     creation and modification times, is not preserved.  To preserve all
     file metadata from the original, use *note copy2(): 304. instead.

     Changed in version 3.3: Added `follow_symlinks' argument.  Now
     returns path to the newly created file.

 -- Function: shutil.copy2 (src, dst, *, follow_symlinks=True)

     Identical to *note copy(): 303. except that *note copy2(): 304.
     also attempts to preserve all file metadata.

     When `follow_symlinks' is false, and `src' is a symbolic link,
     *note copy2(): 304. attempts to copy all metadata from the `src'
     symbolic link to the newly-created `dst' symbolic link.  However,
     this functionality is not available on all platforms.  On platforms
     where some or all of this functionality is unavailable, *note
     copy2(): 304. will preserve all the metadata it can; *note copy2():
     304. never returns failure.

     *note copy2(): 304. uses *note copystat(): 6ae. to copy the file
     metadata.  Please see *note copystat(): 6ae. for more information
     about platform support for modifying symbolic link metadata.

     Changed in version 3.3: Added `follow_symlinks' argument, try to
     copy extended file system attributes too (currently Linux only).
     Now returns path to the newly created file.

 -- Function: shutil.ignore_patterns (*patterns)

     This factory function creates a function that can be used as a
     callable for *note copytree(): 7f0.’s `ignore' argument, ignoring
     files and directories that match one of the glob-style `patterns'
     provided.  See the example below.

 -- Function: shutil.copytree (src, dst, symlinks=False, ignore=None,
          copy_function=copy2, ignore_dangling_symlinks=False)

     Recursively copy an entire directory tree rooted at `src',
     returning the destination directory.  The destination directory,
     named by `dst', must not already exist; it will be created as well
     as missing parent directories.  Permissions and times of
     directories are copied with *note copystat(): 6ae, individual files
     are copied using *note shutil.copy2(): 304.

     If `symlinks' is true, symbolic links in the source tree are
     represented as symbolic links in the new tree and the metadata of
     the original links will be copied as far as the platform allows; if
     false or omitted, the contents and metadata of the linked files are
     copied to the new tree.

     When `symlinks' is false, if the file pointed by the symlink
     doesn’t exist, an exception will be added in the list of errors
     raised in an *note Error: 4b0. exception at the end of the copy
     process.  You can set the optional `ignore_dangling_symlinks' flag
     to true if you want to silence this exception.  Notice that this
     option has no effect on platforms that don’t support *note
     os.symlink(): 66f.

     If `ignore' is given, it must be a callable that will receive as
     its arguments the directory being visited by *note copytree(): 7f0,
     and a list of its contents, as returned by *note os.listdir(): 675.
     Since *note copytree(): 7f0. is called recursively, the `ignore'
     callable will be called once for each directory that is copied.
     The callable must return a sequence of directory and file names
     relative to the current directory (i.e.  a subset of the items in
     its second argument); these names will then be ignored in the copy
     process.  *note ignore_patterns(): 162e. can be used to create such
     a callable that ignores names based on glob-style patterns.

     If exception(s) occur, an *note Error: 4b0. is raised with a list
     of reasons.

     If `copy_function' is given, it must be a callable that will be
     used to copy each file.  It will be called with the source path and
     the destination path as arguments.  By default, *note
     shutil.copy2(): 304. is used, but any function that supports the
     same signature (like *note shutil.copy(): 303.) can be used.

     Changed in version 3.3: Copy metadata when `symlinks' is false.
     Now returns `dst'.

     Changed in version 3.2: Added the `copy_function' argument to be
     able to provide a custom copy function.  Added the
     `ignore_dangling_symlinks' argument to silent dangling symlinks
     errors when `symlinks' is false.

 -- Function: shutil.rmtree (path, ignore_errors=False, onerror=None)

     Delete an entire directory tree; `path' must point to a directory
     (but not a symbolic link to a directory).  If `ignore_errors' is
     true, errors resulting from failed removals will be ignored; if
     false or omitted, such errors are handled by calling a handler
     specified by `onerror' or, if that is omitted, they raise an
     exception.

          Note: On platforms that support the necessary fd-based
          functions a symlink attack resistant version of *note
          rmtree(): 6af. is used by default.  On other platforms, the
          *note rmtree(): 6af. implementation is susceptible to a
          symlink attack: given proper timing and circumstances,
          attackers can manipulate symlinks on the filesystem to delete
          files they wouldn’t be able to access otherwise.  Applications
          can use the *note rmtree.avoids_symlink_attacks: 162f.
          function attribute to determine which case applies.

     If `onerror' is provided, it must be a callable that accepts three
     parameters: `function', `path', and `excinfo'.

     The first parameter, `function', is the function which raised the
     exception; it depends on the platform and implementation.  The
     second parameter, `path', will be the path name passed to
     `function'.  The third parameter, `excinfo', will be the exception
     information returned by *note sys.exc_info(): 8ca.  Exceptions
     raised by `onerror' will not be caught.

     Changed in version 3.3: Added a symlink attack resistant version
     that is used automatically if platform supports fd-based functions.

      -- Attribute: rmtree.avoids_symlink_attacks

          Indicates whether the current platform and implementation
          provides a symlink attack resistant version of *note rmtree():
          6af.  Currently this is only true for platforms supporting
          fd-based directory access functions.

          New in version 3.3.

 -- Function: shutil.move (src, dst, copy_function=copy2)

     Recursively move a file or directory (`src') to another location
     (`dst') and return the destination.

     If the destination is an existing directory, then `src' is moved
     inside that directory.  If the destination already exists but is
     not a directory, it may be overwritten depending on *note
     os.rename(): 66c. semantics.

     If the destination is on the current filesystem, then *note
     os.rename(): 66c. is used.  Otherwise, `src' is copied to `dst'
     using `copy_function' and then removed.  In case of symlinks, a new
     symlink pointing to the target of `src' will be created in or as
     `dst' and `src' will be removed.

     If `copy_function' is given, it must be a callable that takes two
     arguments `src' and `dst', and will be used to copy `src' to `dest'
     if *note os.rename(): 66c. cannot be used.  If the source is a
     directory, *note copytree(): 7f0. is called, passing it the
     ‘copy_function()’.  The default `copy_function' is *note copy2():
     304.  Using *note copy(): 25. as the `copy_function' allows the
     move to succeed when it is not possible to also copy the metadata,
     at the expense of not copying any of the metadata.

     Changed in version 3.3: Added explicit symlink handling for foreign
     filesystems, thus adapting it to the behavior of GNU’s ‘mv’.  Now
     returns `dst'.

     Changed in version 3.5: Added the `copy_function' keyword argument.

 -- Function: shutil.disk_usage (path)

     Return disk usage statistics about the given path as a *note named
     tuple: 787. with the attributes `total', `used' and `free', which
     are the amount of total, used and free space, in bytes.

     New in version 3.3.

     Availability: Unix, Windows.

 -- Function: shutil.chown (path, user=None, group=None)

     Change owner `user' and/or `group' of the given `path'.

     `user' can be a system user name or a uid; the same applies to
     `group'.  At least one argument is required.

     See also *note os.chown(): 666, the underlying function.

     Availability: Unix.

     New in version 3.3.

 -- Function: shutil.which (cmd, mode=os.F_OK | os.X_OK, path=None)

     Return the path to an executable which would be run if the given
     `cmd' was called.  If no `cmd' would be called, return ‘None’.

     `mode' is a permission mask passed to *note os.access(): 663, by
     default determining if the file exists and executable.

     When no `path' is specified, the results of *note os.environ():
     77a. are used, returning either the "PATH" value or a fallback of
     *note os.defpath: 1631.

     On Windows, the current directory is always prepended to the `path'
     whether or not you use the default or provide your own, which is
     the behavior the command shell uses when finding executables.
     Additionally, when finding the `cmd' in the `path', the ‘PATHEXT’
     environment variable is checked.  For example, if you call
     ‘shutil.which("python")’, *note which(): 1630. will search
     ‘PATHEXT’ to know that it should look for ‘python.exe’ within the
     `path' directories.  For example, on Windows:

          >>> shutil.which("python")
          'C:\\Python33\\python.EXE'

     New in version 3.3.

 -- Exception: shutil.Error

     This exception collects exceptions that are raised during a
     multi-file operation.  For *note copytree(): 7f0, the exception
     argument is a list of 3-tuples (`srcname', `dstname', `exception').

* Menu:

* copytree example:: 
* rmtree example:: 


File: python.info,  Node: copytree example,  Next: rmtree example,  Up: Directory and files operations

5.11.10.2 copytree example
..........................

This example is the implementation of the *note copytree(): 7f0.
function, described above, with the docstring omitted.  It demonstrates
many of the other functions provided by this module.

     def copytree(src, dst, symlinks=False):
         names = os.listdir(src)
         os.makedirs(dst)
         errors = []
         for name in names:
             srcname = os.path.join(src, name)
             dstname = os.path.join(dst, name)
             try:
                 if symlinks and os.path.islink(srcname):
                     linkto = os.readlink(srcname)
                     os.symlink(linkto, dstname)
                 elif os.path.isdir(srcname):
                     copytree(srcname, dstname, symlinks)
                 else:
                     copy2(srcname, dstname)
                 # XXX What about devices, sockets etc.?
             except OSError as why:
                 errors.append((srcname, dstname, str(why)))
             # catch the Error from the recursive copytree so that we can
             # continue with other files
             except Error as err:
                 errors.extend(err.args[0])
         try:
             copystat(src, dst)
         except OSError as why:
             # can't copy file access times on Windows
             if why.winerror is None:
                 errors.extend((src, dst, str(why)))
         if errors:
             raise Error(errors)

Another example that uses the *note ignore_patterns(): 162e. helper:

     from shutil import copytree, ignore_patterns

     copytree(source, destination, ignore=ignore_patterns('*.pyc', 'tmp*'))

This will copy everything except ‘.pyc’ files and files or directories
whose name starts with ‘tmp’.

Another example that uses the `ignore' argument to add a logging call:

     from shutil import copytree
     import logging

     def _logpath(path, names):
         logging.info('Working in %s' % path)
         return []   # nothing will be ignored

     copytree(source, destination, ignore=_logpath)


File: python.info,  Node: rmtree example,  Prev: copytree example,  Up: Directory and files operations

5.11.10.3 rmtree example
........................

This example shows how to remove a directory tree on Windows where some
of the files have their read-only bit set.  It uses the onerror callback
to clear the readonly bit and reattempt the remove.  Any subsequent
failure will propagate.

     import os, stat
     import shutil

     def remove_readonly(func, path, _):
         "Clear the readonly bit and reattempt the removal"
         os.chmod(path, stat.S_IWRITE)
         func(path)

     shutil.rmtree(directory, onerror=remove_readonly)


File: python.info,  Node: Archiving operations,  Next: Querying the size of the output terminal,  Prev: Directory and files operations,  Up: shutil --- High-level file operations

5.11.10.4 Archiving operations
..............................

New in version 3.2.

High-level utilities to create and read compressed and archived files
are also provided.  They rely on the *note zipfile: 13f. and *note
tarfile: ff. modules.

 -- Function: shutil.make_archive (base_name, format[, root_dir[,
          base_dir[, verbose[, dry_run[, owner[, group[, logger]]]]]]])

     Create an archive file (such as zip or tar) and return its name.

     `base_name' is the name of the file to create, including the path,
     minus any format-specific extension.  `format' is the archive
     format: one of "zip", "tar", "bztar" (if the *note bz2: 14. module
     is available), "xztar" (if the *note lzma: ab. module is available)
     or "gztar".

     `root_dir' is a directory that will be the root directory of the
     archive; for example, we typically chdir into `root_dir' before
     creating the archive.

     `base_dir' is the directory where we start archiving from; i.e.
     `base_dir' will be the common prefix of all files and directories
     in the archive.

     `root_dir' and `base_dir' both default to the current directory.

     If `dry_run' is true, no archive is created, but the operations
     that would be executed are logged to `logger'.

     `owner' and `group' are used when creating a tar archive.  By
     default, uses the current owner and group.

     `logger' must be an object compatible with PEP 282(1), usually an
     instance of *note logging.Logger: 2c6.

     The `verbose' argument is unused and deprecated.

     Changed in version 3.5: Added support for the `xztar' format.

 -- Function: shutil.get_archive_formats ()

     Return a list of supported formats for archiving.  Each element of
     the returned sequence is a tuple ‘(name, description)’.

     By default *note shutil: e7. provides these formats:

        - `gztar': gzip’ed tar-file

        - `bztar': bzip2’ed tar-file (if the *note bz2: 14. module is
          available.)

        - `xztar': xz’ed tar-file (if the *note lzma: ab. module is
          available.)

        - `tar': uncompressed tar file

        - `zip': ZIP file

     You can register new formats or provide your own archiver for any
     existing formats, by using *note register_archive_format(): 1638.

 -- Function: shutil.register_archive_format (name, function[,
          extra_args[, description]])

     Register an archiver for the format `name'.

     `function' is the callable that will be used to unpack archives.
     The callable will receive the `base_name' of the file to create,
     followed by the `base_dir' (which defaults to *note os.curdir:
     1593.) to start archiving from.  Further arguments are passed as
     keyword arguments: `owner', `group', `dry_run' and `logger' (as
     passed in *note make_archive(): 305.).

     If given, `extra_args' is a sequence of ‘(name, value)’ pairs that
     will be used as extra keywords arguments when the archiver callable
     is used.

     `description' is used by *note get_archive_formats(): 1637. which
     returns the list of archivers.  Defaults to an empty string.

 -- Function: shutil.unregister_archive_format (name)

     Remove the archive format `name' from the list of supported
     formats.

 -- Function: shutil.unpack_archive (filename[, extract_dir[, format]])

     Unpack an archive.  `filename' is the full path of the archive.

     `extract_dir' is the name of the target directory where the archive
     is unpacked.  If not provided, the current working directory is
     used.

     `format' is the archive format: one of "zip", "tar", or "gztar".
     Or any other format registered with *note register_unpack_format():
     163a.  If not provided, *note unpack_archive(): 7f2. will use the
     archive file name extension and see if an unpacker was registered
     for that extension.  In case none is found, a *note ValueError:
     19c. is raised.

 -- Function: shutil.register_unpack_format (name, extensions,
          function[, extra_args[, description]])

     Registers an unpack format.  `name' is the name of the format and
     `extensions' is a list of extensions corresponding to the format,
     like ‘.zip’ for Zip files.

     `function' is the callable that will be used to unpack archives.
     The callable will receive the path of the archive, followed by the
     directory the archive must be extracted to.

     When provided, `extra_args' is a sequence of ‘(name, value)’ tuples
     that will be passed as keywords arguments to the callable.

     `description' can be provided to describe the format, and will be
     returned by the *note get_unpack_formats(): 163b. function.

 -- Function: shutil.unregister_unpack_format (name)

     Unregister an unpack format.  `name' is the name of the format.

 -- Function: shutil.get_unpack_formats ()

     Return a list of all registered formats for unpacking.  Each
     element of the returned sequence is a tuple ‘(name, extensions,
     description)’.

     By default *note shutil: e7. provides these formats:

        - `gztar': gzip’ed tar-file

        - `bztar': bzip2’ed tar-file (if the *note bz2: 14. module is
          available.)

        - `xztar': xz’ed tar-file (if the *note lzma: ab. module is
          available.)

        - `tar': uncompressed tar file

        - `zip': ZIP file

     You can register new formats or provide your own unpacker for any
     existing formats, by using *note register_unpack_format(): 163a.

* Menu:

* Archiving example:: 

   ---------- Footnotes ----------

   (1) https://www.python.org/dev/peps/pep-0282


File: python.info,  Node: Archiving example,  Up: Archiving operations

5.11.10.5 Archiving example
...........................

In this example, we create a gzip’ed tar-file archive containing all
files found in the ‘.ssh’ directory of the user:

     >>> from shutil import make_archive
     >>> import os
     >>> archive_name = os.path.expanduser(os.path.join('~', 'myarchive'))
     >>> root_dir = os.path.expanduser(os.path.join('~', '.ssh'))
     >>> make_archive(archive_name, 'gztar', root_dir)
     '/Users/tarek/myarchive.tar.gz'

The resulting archive contains:

     $ tar -tzvf /Users/tarek/myarchive.tar.gz
     drwx------ tarek/staff       0 2010-02-01 16:23:40 ./
     -rw-r--r-- tarek/staff     609 2008-06-09 13:26:54 ./authorized_keys
     -rwxr-xr-x tarek/staff      65 2008-06-09 13:26:54 ./config
     -rwx------ tarek/staff     668 2008-06-09 13:26:54 ./id_dsa
     -rwxr-xr-x tarek/staff     609 2008-06-09 13:26:54 ./id_dsa.pub
     -rw------- tarek/staff    1675 2008-06-09 13:26:54 ./id_rsa
     -rw-r--r-- tarek/staff     397 2008-06-09 13:26:54 ./id_rsa.pub
     -rw-r--r-- tarek/staff   37192 2010-02-06 18:23:10 ./known_hosts


File: python.info,  Node: Querying the size of the output terminal,  Prev: Archiving operations,  Up: shutil --- High-level file operations

5.11.10.6 Querying the size of the output terminal
..................................................

New in version 3.3.

 -- Function: shutil.get_terminal_size (fallback=(columns, lines))

     Get the size of the terminal window.

     For each of the two dimensions, the environment variable, ‘COLUMNS’
     and ‘LINES’ respectively, is checked.  If the variable is defined
     and the value is a positive integer, it is used.

     When ‘COLUMNS’ or ‘LINES’ is not defined, which is the common case,
     the terminal connected to *note sys.__stdout__: 1640. is queried by
     invoking *note os.get_terminal_size(): 67e.

     If the terminal size cannot be successfully queried, either because
     the system doesn’t support querying, or because we are not
     connected to a terminal, the value given in ‘fallback’ parameter is
     used.  ‘fallback’ defaults to ‘(80, 24)’ which is the default size
     used by many terminal emulators.

     The value returned is a named tuple of type *note os.terminal_size:
     1641.

     See also: The Single UNIX Specification, Version 2, Other
     Environment Variables(1).

   ---------- Footnotes ----------

   (1) 
http://pubs.opengroup.org/onlinepubs/7908799/xbd/envvar.html#tag_002_003


File: python.info,  Node: macpath --- Mac OS 9 path manipulation functions,  Prev: shutil --- High-level file operations,  Up: File and Directory Access

5.11.11 ‘macpath’ — Mac OS 9 path manipulation functions
--------------------------------------------------------

This module is the Mac OS 9 (and earlier) implementation of the *note
os.path: c3. module.  It can be used to manipulate old-style Macintosh
pathnames on Mac OS X (or any other platform).

The following functions are available in this module: ‘normcase()’,
‘normpath()’, ‘isabs()’, ‘join()’, ‘split()’, ‘isdir()’, ‘isfile()’,
‘walk()’, ‘exists()’.  For other functions available in *note os.path:
c3. dummy counterparts are available.

See also
........

Module *note os: c2.

     Operating system interfaces, including functions to work with files
     at a lower level than Python *note file objects: 78b.

Module *note io: 9f.

     Python’s built-in I/O library, including both abstract classes and
     some concrete classes such as file I/O.

Built-in function *note open(): 1e8.

     The standard way to open files for reading and writing with Python.


File: python.info,  Node: Data Persistence,  Next: Data Compression and Archiving,  Prev: File and Directory Access,  Up: The Python Standard Library

5.12 Data Persistence
=====================

The modules described in this chapter support storing Python data in a
persistent form on disk.  The *note pickle: c8. and *note marshal: af.
modules can turn many Python data types into a stream of bytes and then
recreate the objects from the bytes.  The various DBM-related modules
support a family of hash-based file formats that store a mapping of
strings to other strings.

The list of modules described in this chapter is:

* Menu:

* pickle: pickle --- Python object serialization. Python object serialization
* copyreg: copyreg --- Register pickle support functions. Register pickle support functions
* shelve: shelve --- Python object persistence. Python object persistence
* marshal: marshal --- Internal Python object serialization. Internal Python object serialization
* dbm: dbm --- Interfaces to Unix "databases". Interfaces to Unix "databases"
* sqlite3: sqlite3 --- DB-API 2 0 interface for SQLite databases. DB-API 2.0 interface for SQLite databases


File: python.info,  Node: pickle --- Python object serialization,  Next: copyreg --- Register pickle support functions,  Up: Data Persistence

5.12.1 ‘pickle’ — Python object serialization
---------------------------------------------

The *note pickle: c8. module implements binary protocols for serializing
and de-serializing a Python object structure.  `"Pickling"' is the
process whereby a Python object hierarchy is converted into a byte
stream, and `"unpickling"' is the inverse operation, whereby a byte
stream (from a *note binary file: 164a. or *note bytes-like object:
36b.) is converted back into an object hierarchy.  Pickling (and
unpickling) is alternatively known as "serialization", "marshalling,"
(1) or "flattening"; however, to avoid confusion, the terms used here
are "pickling" and "unpickling".

     Warning: The *note pickle: c8. module is not secure against
     erroneous or maliciously constructed data.  Never unpickle data
     received from an untrusted or unauthenticated source.

* Menu:

* Relationship to other Python modules:: 
* Data stream format:: 
* Module Interface:: 
* What can be pickled and unpickled?:: 
* Pickling Class Instances:: 
* Restricting Globals:: 
* Performance: Performance<2>. 
* Examples: Examples<4>. 

   ---------- Footnotes ----------

   (1) Don’t confuse this with the *note marshal: af. module


File: python.info,  Node: Relationship to other Python modules,  Next: Data stream format,  Up: pickle --- Python object serialization

5.12.1.1 Relationship to other Python modules
.............................................

* Menu:

* Comparison with marshal:: 
* Comparison with json:: 


File: python.info,  Node: Comparison with marshal,  Next: Comparison with json,  Up: Relationship to other Python modules

5.12.1.2 Comparison with ‘marshal’
..................................

Python has a more primitive serialization module called *note marshal:
af, but in general *note pickle: c8. should always be the preferred way
to serialize Python objects.  *note marshal: af. exists primarily to
support Python’s ‘.pyc’ files.

The *note pickle: c8. module differs from *note marshal: af. in several
significant ways:

   * The *note pickle: c8. module keeps track of the objects it has
     already serialized, so that later references to the same object
     won’t be serialized again.  *note marshal: af. doesn’t do this.

     This has implications both for recursive objects and object
     sharing.  Recursive objects are objects that contain references to
     themselves.  These are not handled by marshal, and in fact,
     attempting to marshal recursive objects will crash your Python
     interpreter.  Object sharing happens when there are multiple
     references to the same object in different places in the object
     hierarchy being serialized.  *note pickle: c8. stores such objects
     only once, and ensures that all other references point to the
     master copy.  Shared objects remain shared, which can be very
     important for mutable objects.

   * *note marshal: af. cannot be used to serialize user-defined classes
     and their instances.  *note pickle: c8. can save and restore class
     instances transparently, however the class definition must be
     importable and live in the same module as when the object was
     stored.

   * The *note marshal: af. serialization format is not guaranteed to be
     portable across Python versions.  Because its primary job in life
     is to support ‘.pyc’ files, the Python implementers reserve the
     right to change the serialization format in non-backwards
     compatible ways should the need arise.  The *note pickle: c8.
     serialization format is guaranteed to be backwards compatible
     across Python releases.


File: python.info,  Node: Comparison with json,  Prev: Comparison with marshal,  Up: Relationship to other Python modules

5.12.1.3 Comparison with ‘json’
...............................

There are fundamental differences between the pickle protocols and JSON
(JavaScript Object Notation)(1):

   * JSON is a text serialization format (it outputs unicode text,
     although most of the time it is then encoded to ‘utf-8’), while
     pickle is a binary serialization format;

   * JSON is human-readable, while pickle is not;

   * JSON is interoperable and widely used outside of the Python
     ecosystem, while pickle is Python-specific;

   * JSON, by default, can only represent a subset of the Python
     built-in types, and no custom classes; pickle can represent an
     extremely large number of Python types (many of them automatically,
     by clever usage of Python’s introspection facilities; complex cases
     can be tackled by implementing *note specific object APIs: 164e.).

See also
........

The *note json: a2. module: a standard library module allowing JSON
serialization and deserialization.

   ---------- Footnotes ----------

   (1) http://json.org


File: python.info,  Node: Data stream format,  Next: Module Interface,  Prev: Relationship to other Python modules,  Up: pickle --- Python object serialization

5.12.1.4 Data stream format
...........................

The data format used by *note pickle: c8. is Python-specific.  This has
the advantage that there are no restrictions imposed by external
standards such as JSON or XDR (which can’t represent pointer sharing);
however it means that non-Python programs may not be able to reconstruct
pickled Python objects.

By default, the *note pickle: c8. data format uses a relatively compact
binary representation.  If you need optimal size characteristics, you
can efficiently *note compress: 1650. pickled data.

The module *note pickletools: c9. contains tools for analyzing data
streams generated by *note pickle: c8.  *note pickletools: c9. source
code has extensive comments about opcodes used by pickle protocols.

There are currently 5 different protocols which can be used for
pickling.  The higher the protocol used, the more recent the version of
Python needed to read the pickle produced.

   * Protocol version 0 is the original "human-readable" protocol and is
     backwards compatible with earlier versions of Python.

   * Protocol version 1 is an old binary format which is also compatible
     with earlier versions of Python.

   * Protocol version 2 was introduced in Python 2.3.  It provides much
     more efficient pickling of *note new-style class: 1651.es.  Refer
     to PEP 307(1) for information about improvements brought by
     protocol 2.

   * Protocol version 3 was added in Python 3.0.  It has explicit
     support for *note bytes: 1db. objects and cannot be unpickled by
     Python 2.x.  This is the default protocol, and the recommended
     protocol when compatibility with other Python 3 versions is
     required.

   * Protocol version 4 was added in Python 3.4.  It adds support for
     very large objects, pickling more kinds of objects, and some data
     format optimizations.  Refer to PEP 3154(2) for information about
     improvements brought by protocol 4.

     Note: Serialization is a more primitive notion than persistence;
     although *note pickle: c8. reads and writes file objects, it does
     not handle the issue of naming persistent objects, nor the (even
     more complicated) issue of concurrent access to persistent objects.
     The *note pickle: c8. module can transform a complex object into a
     byte stream and it can transform the byte stream into an object
     with the same internal structure.  Perhaps the most obvious thing
     to do with these byte streams is to write them onto a file, but it
     is also conceivable to send them across a network or store them in
     a database.  The *note shelve: e5. module provides a simple
     interface to pickle and unpickle objects on DBM-style database
     files.

   ---------- Footnotes ----------

   (1) https://www.python.org/dev/peps/pep-0307

   (2) https://www.python.org/dev/peps/pep-3154


File: python.info,  Node: Module Interface,  Next: What can be pickled and unpickled?,  Prev: Data stream format,  Up: pickle --- Python object serialization

5.12.1.5 Module Interface
.........................

To serialize an object hierarchy, you simply call the *note dumps():
a7e. function.  Similarly, to de-serialize a data stream, you call the
*note loads(): 1653. function.  However, if you want more control over
serialization and de-serialization, you can create a *note Pickler: 69e.
or an *note Unpickler: 1654. object, respectively.

The *note pickle: c8. module provides the following constants:

 -- Data: pickle.HIGHEST_PROTOCOL

     An integer, the highest *note protocol version: 168. available.
     This value can be passed as a `protocol' value to functions *note
     dump(): 1655. and *note dumps(): a7e. as well as the *note Pickler:
     69e. constructor.

 -- Data: pickle.DEFAULT_PROTOCOL

     An integer, the default *note protocol version: 168. used for
     pickling.  May be less than *note HIGHEST_PROTOCOL: a7f.  Currently
     the default protocol is 3, a new protocol designed for Python 3.

The *note pickle: c8. module provides the following functions to make
the pickling process more convenient:

 -- Function: pickle.dump (obj, file, protocol=None, *,
          fix_imports=True)

     Write a pickled representation of `obj' to the open *note file
     object: 78b. `file'.  This is equivalent to ‘Pickler(file,
     protocol).dump(obj)’.

     The optional `protocol' argument, an integer, tells the pickler to
     use the given protocol; supported protocols are 0 to *note
     HIGHEST_PROTOCOL: a7f.  If not specified, the default is *note
     DEFAULT_PROTOCOL: 1656.  If a negative number is specified, *note
     HIGHEST_PROTOCOL: a7f. is selected.

     The `file' argument must have a write() method that accepts a
     single bytes argument.  It can thus be an on-disk file opened for
     binary writing, an *note io.BytesIO: 371. instance, or any other
     custom object that meets this interface.

     If `fix_imports' is true and `protocol' is less than 3, pickle will
     try to map the new Python 3 names to the old module names used in
     Python 2, so that the pickle data stream is readable with Python 2.

 -- Function: pickle.dumps (obj, protocol=None, *, fix_imports=True)

     Return the pickled representation of the object as a *note bytes:
     1db. object, instead of writing it to a file.

     Arguments `protocol' and `fix_imports' have the same meaning as in
     *note dump(): 1655.

 -- Function: pickle.load (file, *, fix_imports=True, encoding="ASCII",
          errors="strict")

     Read a pickled object representation from the open *note file
     object: 78b. `file' and return the reconstituted object hierarchy
     specified therein.  This is equivalent to ‘Unpickler(file).load()’.

     The protocol version of the pickle is detected automatically, so no
     protocol argument is needed.  Bytes past the pickled object’s
     representation are ignored.

     The argument `file' must have two methods, a read() method that
     takes an integer argument, and a readline() method that requires no
     arguments.  Both methods should return bytes.  Thus `file' can be
     an on-disk file opened for binary reading, an *note io.BytesIO:
     371. object, or any other custom object that meets this interface.

     Optional keyword arguments are `fix_imports', `encoding' and
     `errors', which are used to control compatibility support for
     pickle stream generated by Python 2.  If `fix_imports' is true,
     pickle will try to map the old Python 2 names to the new names used
     in Python 3.  The `encoding' and `errors' tell pickle how to decode
     8-bit string instances pickled by Python 2; these default to
     ’ASCII’ and ’strict’, respectively.  The `encoding' can be ’bytes’
     to read these 8-bit string instances as bytes objects.

 -- Function: pickle.loads (bytes_object, *, fix_imports=True,
          encoding="ASCII", errors="strict")

     Read a pickled object hierarchy from a *note bytes: 1db. object and
     return the reconstituted object hierarchy specified therein.

     The protocol version of the pickle is detected automatically, so no
     protocol argument is needed.  Bytes past the pickled object’s
     representation are ignored.

     Optional keyword arguments are `fix_imports', `encoding' and
     `errors', which are used to control compatibility support for
     pickle stream generated by Python 2.  If `fix_imports' is true,
     pickle will try to map the old Python 2 names to the new names used
     in Python 3.  The `encoding' and `errors' tell pickle how to decode
     8-bit string instances pickled by Python 2; these default to
     ’ASCII’ and ’strict’, respectively.  The `encoding' can be ’bytes’
     to read these 8-bit string instances as bytes objects.

The *note pickle: c8. module defines three exceptions:

 -- Exception: pickle.PickleError

     Common base class for the other pickling exceptions.  It inherits
     *note Exception: 1a1.

 -- Exception: pickle.PicklingError

     Error raised when an unpicklable object is encountered by *note
     Pickler: 69e.  It inherits *note PickleError: 1658.

     Refer to *note What can be pickled and unpickled?: 165a. to learn
     what kinds of objects can be pickled.

 -- Exception: pickle.UnpicklingError

     Error raised when there is a problem unpickling an object, such as
     a data corruption or a security violation.  It inherits *note
     PickleError: 1658.

     Note that other exceptions may also be raised during unpickling,
     including (but not necessarily limited to) AttributeError,
     EOFError, ImportError, and IndexError.

The *note pickle: c8. module exports two classes, *note Pickler: 69e.
and *note Unpickler: 1654.:

 -- Class: pickle.Pickler (file, protocol=None, *, fix_imports=True)

     This takes a binary file for writing a pickle data stream.

     The optional `protocol' argument, an integer, tells the pickler to
     use the given protocol; supported protocols are 0 to *note
     HIGHEST_PROTOCOL: a7f.  If not specified, the default is *note
     DEFAULT_PROTOCOL: 1656.  If a negative number is specified, *note
     HIGHEST_PROTOCOL: a7f. is selected.

     The `file' argument must have a write() method that accepts a
     single bytes argument.  It can thus be an on-disk file opened for
     binary writing, an *note io.BytesIO: 371. instance, or any other
     custom object that meets this interface.

     If `fix_imports' is true and `protocol' is less than 3, pickle will
     try to map the new Python 3 names to the old module names used in
     Python 2, so that the pickle data stream is readable with Python 2.

      -- Method: dump (obj)

          Write a pickled representation of `obj' to the open file
          object given in the constructor.

      -- Method: persistent_id (obj)

          Do nothing by default.  This exists so a subclass can override
          it.

          If *note persistent_id(): 165d. returns ‘None’, `obj' is
          pickled as usual.  Any other value causes *note Pickler: 69e.
          to emit the returned value as a persistent ID for `obj'.  The
          meaning of this persistent ID should be defined by *note
          Unpickler.persistent_load(): 165e.  Note that the value
          returned by *note persistent_id(): 165d. cannot itself have a
          persistent ID.

          See *note Persistence of External Objects: 165f. for details
          and examples of uses.

      -- Attribute: dispatch_table

          A pickler object’s dispatch table is a registry of `reduction
          functions' of the kind which can be declared using *note
          copyreg.pickle(): 1660.  It is a mapping whose keys are
          classes and whose values are reduction functions.  A reduction
          function takes a single argument of the associated class and
          should conform to the same interface as a *note __reduce__():
          a16. method.

          By default, a pickler object will not have a *note
          dispatch_table: 69f. attribute, and it will instead use the
          global dispatch table managed by the *note copyreg: 26.
          module.  However, to customize the pickling for a specific
          pickler object one can set the *note dispatch_table: 69f.
          attribute to a dict-like object.  Alternatively, if a subclass
          of *note Pickler: 69e. has a *note dispatch_table: 69f.
          attribute then this will be used as the default dispatch table
          for instances of that class.

          See *note Dispatch Tables: 1661. for usage examples.

          New in version 3.3.

      -- Attribute: fast

          Deprecated.  Enable fast mode if set to a true value.  The
          fast mode disables the usage of memo, therefore speeding the
          pickling process by not generating superfluous PUT opcodes.
          It should not be used with self-referential objects, doing
          otherwise will cause *note Pickler: 69e. to recurse
          infinitely.

          Use *note pickletools.optimize(): 1663. if you need more
          compact pickles.

 -- Class: pickle.Unpickler (file, *, fix_imports=True,
          encoding="ASCII", errors="strict")

     This takes a binary file for reading a pickle data stream.

     The protocol version of the pickle is detected automatically, so no
     protocol argument is needed.

     The argument `file' must have two methods, a read() method that
     takes an integer argument, and a readline() method that requires no
     arguments.  Both methods should return bytes.  Thus `file' can be
     an on-disk file object opened for binary reading, an *note
     io.BytesIO: 371. object, or any other custom object that meets this
     interface.

     Optional keyword arguments are `fix_imports', `encoding' and
     `errors', which are used to control compatibility support for
     pickle stream generated by Python 2.  If `fix_imports' is true,
     pickle will try to map the old Python 2 names to the new names used
     in Python 3.  The `encoding' and `errors' tell pickle how to decode
     8-bit string instances pickled by Python 2; these default to
     ’ASCII’ and ’strict’, respectively.  The `encoding' can be ’bytes’
     to read these ß8-bit string instances as bytes objects.

      -- Method: load ()

          Read a pickled object representation from the open file object
          given in the constructor, and return the reconstituted object
          hierarchy specified therein.  Bytes past the pickled object’s
          representation are ignored.

      -- Method: persistent_load (pid)

          Raise an *note UnpicklingError: 165b. by default.

          If defined, *note persistent_load(): 165e. should return the
          object specified by the persistent ID `pid'.  If an invalid
          persistent ID is encountered, an *note UnpicklingError: 165b.
          should be raised.

          See *note Persistence of External Objects: 165f. for details
          and examples of uses.

      -- Method: find_class (module, name)

          Import `module' if necessary and return the object called
          `name' from it, where the `module' and `name' arguments are
          *note str: 25a. objects.  Note, unlike its name suggests,
          *note find_class(): 1665. is also used for finding functions.

          Subclasses may override this to gain control over what type of
          objects and how they can be loaded, potentially reducing
          security risks.  Refer to *note Restricting Globals: 1666. for
          details.


File: python.info,  Node: What can be pickled and unpickled?,  Next: Pickling Class Instances,  Prev: Module Interface,  Up: pickle --- Python object serialization

5.12.1.6 What can be pickled and unpickled?
...........................................

The following types can be pickled:

   * ‘None’, ‘True’, and ‘False’

   * integers, floating point numbers, complex numbers

   * strings, bytes, bytearrays

   * tuples, lists, sets, and dictionaries containing only picklable
     objects

   * functions defined at the top level of a module (using *note def:
     a3a, not *note lambda: 894.)

   * built-in functions defined at the top level of a module

   * classes that are defined at the top level of a module

   * instances of such classes whose *note __dict__: df4. or the result
     of calling *note __getstate__(): a80. is picklable (see section
     *note Pickling Class Instances: 164e. for details).

Attempts to pickle unpicklable objects will raise the *note
PicklingError: 1659. exception; when this happens, an unspecified number
of bytes may have already been written to the underlying file.  Trying
to pickle a highly recursive data structure may exceed the maximum
recursion depth, a *note RecursionError: 1b8. will be raised in this
case.  You can carefully raise this limit with *note
sys.setrecursionlimit(): b01.

Note that functions (built-in and user-defined) are pickled by "fully
qualified" name reference, not by value.  (1) This means that only the
function name is pickled, along with the name of the module the function
is defined in.  Neither the function’s code, nor any of its function
attributes are pickled.  Thus the defining module must be importable in
the unpickling environment, and the module must contain the named
object, otherwise an exception will be raised.  (2)

Similarly, classes are pickled by named reference, so the same
restrictions in the unpickling environment apply.  Note that none of the
class’s code or data is pickled, so in the following example the class
attribute ‘attr’ is not restored in the unpickling environment:

     class Foo:
         attr = 'A class attribute'

     picklestring = pickle.dumps(Foo)

These restrictions are why picklable functions and classes must be
defined in the top level of a module.

Similarly, when class instances are pickled, their class’s code and data
are not pickled along with them.  Only the instance data are pickled.
This is done on purpose, so you can fix bugs in a class or add methods
to the class and still load objects that were created with an earlier
version of the class.  If you plan to have long-lived objects that will
see many versions of a class, it may be worthwhile to put a version
number in the objects so that suitable conversions can be made by the
class’s *note __setstate__(): a81. method.

   ---------- Footnotes ----------

   (1) This is why *note lambda: 894. functions cannot be pickled: all
*note lambda: 894. functions share the same name: ‘<lambda>’.

   (2) The exception raised will likely be an *note ImportError: 19f. or
an *note AttributeError: 356. but it could be something else.


File: python.info,  Node: Pickling Class Instances,  Next: Restricting Globals,  Prev: What can be pickled and unpickled?,  Up: pickle --- Python object serialization

5.12.1.7 Pickling Class Instances
.................................

In this section, we describe the general mechanisms available to you to
define, customize, and control how class instances are pickled and
unpickled.

In most cases, no additional code is needed to make instances picklable.
By default, pickle will retrieve the class and the attributes of an
instance via introspection.  When a class instance is unpickled, its
*note __init__(): 9d5. method is usually `not' invoked.  The default
behaviour first creates an uninitialized instance and then restores the
saved attributes.  The following code shows an implementation of this
behaviour:

     def save(obj):
         return (obj.__class__, obj.__dict__)

     def load(cls, attributes):
         obj = cls.__new__(cls)
         obj.__dict__.update(attributes)
         return obj

Classes can alter the default behaviour by providing one or several
special methods:

 -- Method: object.__getnewargs_ex__ ()

     In protocols 2 and newer, classes that implements the *note
     __getnewargs_ex__(): 1669. method can dictate the values passed to
     the *note __new__(): 484. method upon unpickling.  The method must
     return a pair ‘(args, kwargs)’ where `args' is a tuple of
     positional arguments and `kwargs' a dictionary of named arguments
     for constructing the object.  Those will be passed to the *note
     __new__(): 484. method upon unpickling.

     You should implement this method if the *note __new__(): 484.
     method of your class requires keyword-only arguments.  Otherwise,
     it is recommended for compatibility to implement *note
     __getnewargs__(): 254.

     Changed in version 3.6: *note __getnewargs_ex__(): 1669. is now
     used in protocols 2 and 3.

 -- Method: object.__getnewargs__ ()

     This method serve a similar purpose as *note __getnewargs_ex__():
     1669, but supports only positional arguments.  It must return a
     tuple of arguments ‘args’ which will be passed to the *note
     __new__(): 484. method upon unpickling.

     *note __getnewargs__(): 254. will not be called if *note
     __getnewargs_ex__(): 1669. is defined.

     Changed in version 3.6: Before Python 3.6, *note __getnewargs__():
     254. was called instead of *note __getnewargs_ex__(): 1669. in
     protocols 2 and 3.

 -- Method: object.__getstate__ ()

     Classes can further influence how their instances are pickled; if
     the class defines the method *note __getstate__(): a80, it is
     called and the returned object is pickled as the contents for the
     instance, instead of the contents of the instance’s dictionary.  If
     the *note __getstate__(): a80. method is absent, the instance’s
     *note __dict__: df4. is pickled as usual.

 -- Method: object.__setstate__ (state)

     Upon unpickling, if the class defines *note __setstate__(): a81, it
     is called with the unpickled state.  In that case, there is no
     requirement for the state object to be a dictionary.  Otherwise,
     the pickled state must be a dictionary and its items are assigned
     to the new instance’s dictionary.

          Note: If *note __getstate__(): a80. returns a false value, the
          *note __setstate__(): a81. method will not be called upon
          unpickling.

Refer to the section *note Handling Stateful Objects: 166a. for more
information about how to use the methods *note __getstate__(): a80. and
*note __setstate__(): a81.

     Note: At unpickling time, some methods like *note __getattr__():
     782, *note __getattribute__(): 783, or *note __setattr__(): aaf.
     may be called upon the instance.  In case those methods rely on
     some internal invariant being true, the type should implement *note
     __getnewargs__(): 254. or *note __getnewargs_ex__(): 1669. to
     establish such an invariant; otherwise, neither *note __new__():
     484. nor *note __init__(): 9d5. will be called.

As we shall see, pickle does not use directly the methods described
above.  In fact, these methods are part of the copy protocol which
implements the *note __reduce__(): a16. special method.  The copy
protocol provides a unified interface for retrieving the data necessary
for pickling and copying objects.  (1)

Although powerful, implementing *note __reduce__(): a16. directly in
your classes is error prone.  For this reason, class designers should
use the high-level interface (i.e., *note __getnewargs_ex__(): 1669,
*note __getstate__(): a80. and *note __setstate__(): a81.) whenever
possible.  We will show, however, cases where using *note __reduce__():
a16. is the only option or leads to more efficient pickling or both.

 -- Method: object.__reduce__ ()

     The interface is currently defined as follows.  The *note
     __reduce__(): a16. method takes no argument and shall return either
     a string or preferably a tuple (the returned object is often
     referred to as the "reduce value").

     If a string is returned, the string should be interpreted as the
     name of a global variable.  It should be the object’s local name
     relative to its module; the pickle module searches the module
     namespace to determine the object’s module.  This behaviour is
     typically useful for singletons.

     When a tuple is returned, it must be between two and five items
     long.  Optional items can either be omitted, or ‘None’ can be
     provided as their value.  The semantics of each item are in order:

        * A callable object that will be called to create the initial
          version of the object.

        * A tuple of arguments for the callable object.  An empty tuple
          must be given if the callable does not accept any argument.

        * Optionally, the object’s state, which will be passed to the
          object’s *note __setstate__(): a81. method as previously
          described.  If the object has no such method then, the value
          must be a dictionary and it will be added to the object’s
          *note __dict__: df4. attribute.

        * Optionally, an iterator (and not a sequence) yielding
          successive items.  These items will be appended to the object
          either using ‘obj.append(item)’ or, in batch, using
          ‘obj.extend(list_of_items)’.  This is primarily used for list
          subclasses, but may be used by other classes as long as they
          have ‘append()’ and ‘extend()’ methods with the appropriate
          signature.  (Whether ‘append()’ or ‘extend()’ is used depends
          on which pickle protocol version is used as well as the number
          of items to append, so both must be supported.)

        * Optionally, an iterator (not a sequence) yielding successive
          key-value pairs.  These items will be stored to the object
          using ‘obj[key] = value’.  This is primarily used for
          dictionary subclasses, but may be used by other classes as
          long as they implement *note __setitem__(): 8cd.

 -- Method: object.__reduce_ex__ (protocol)

     Alternatively, a *note __reduce_ex__(): 13a3. method may be
     defined.  The only difference is this method should take a single
     integer argument, the protocol version.  When defined, pickle will
     prefer it over the *note __reduce__(): a16. method.  In addition,
     *note __reduce__(): a16. automatically becomes a synonym for the
     extended version.  The main use for this method is to provide
     backwards-compatible reduce values for older Python releases.

* Menu:

* Persistence of External Objects:: 
* Dispatch Tables:: 
* Handling Stateful Objects:: 

   ---------- Footnotes ----------

   (1) The *note copy: 25. module uses this protocol for shallow and
deep copying operations.


File: python.info,  Node: Persistence of External Objects,  Next: Dispatch Tables,  Up: Pickling Class Instances

5.12.1.8 Persistence of External Objects
........................................

For the benefit of object persistence, the *note pickle: c8. module
supports the notion of a reference to an object outside the pickled data
stream.  Such objects are referenced by a persistent ID, which should be
either a string of alphanumeric characters (for protocol 0) (1) or just
an arbitrary object (for any newer protocol).

The resolution of such persistent IDs is not defined by the *note
pickle: c8. module; it will delegate this resolution to the user defined
methods on the pickler and unpickler, *note persistent_id(): 165d. and
*note persistent_load(): 165e. respectively.

To pickle objects that have an external persistent id, the pickler must
have a custom *note persistent_id(): 165d. method that takes an object
as an argument and returns either ‘None’ or the persistent id for that
object.  When ‘None’ is returned, the pickler simply pickles the object
as normal.  When a persistent ID string is returned, the pickler will
pickle that object, along with a marker so that the unpickler will
recognize it as a persistent ID.

To unpickle external objects, the unpickler must have a custom *note
persistent_load(): 165e. method that takes a persistent ID object and
returns the referenced object.

Here is a comprehensive example presenting how persistent ID can be used
to pickle external objects by reference.

     # Simple example presenting how persistent ID can be used to pickle
     # external objects by reference.

     import pickle
     import sqlite3
     from collections import namedtuple

     # Simple class representing a record in our database.
     MemoRecord = namedtuple("MemoRecord", "key, task")

     class DBPickler(pickle.Pickler):

         def persistent_id(self, obj):
             # Instead of pickling MemoRecord as a regular class instance, we emit a
             # persistent ID.
             if isinstance(obj, MemoRecord):
                 # Here, our persistent ID is simply a tuple, containing a tag and a
                 # key, which refers to a specific record in the database.
                 return ("MemoRecord", obj.key)
             else:
                 # If obj does not have a persistent ID, return None. This means obj
                 # needs to be pickled as usual.
                 return None


     class DBUnpickler(pickle.Unpickler):

         def __init__(self, file, connection):
             super().__init__(file)
             self.connection = connection

         def persistent_load(self, pid):
             # This method is invoked whenever a persistent ID is encountered.
             # Here, pid is the tuple returned by DBPickler.
             cursor = self.connection.cursor()
             type_tag, key_id = pid
             if type_tag == "MemoRecord":
                 # Fetch the referenced record from the database and return it.
                 cursor.execute("SELECT * FROM memos WHERE key=?", (str(key_id),))
                 key, task = cursor.fetchone()
                 return MemoRecord(key, task)
             else:
                 # Always raises an error if you cannot return the correct object.
                 # Otherwise, the unpickler will think None is the object referenced
                 # by the persistent ID.
                 raise pickle.UnpicklingError("unsupported persistent object")


     def main():
         import io
         import pprint

         # Initialize and populate our database.
         conn = sqlite3.connect(":memory:")
         cursor = conn.cursor()
         cursor.execute("CREATE TABLE memos(key INTEGER PRIMARY KEY, task TEXT)")
         tasks = (
             'give food to fish',
             'prepare group meeting',
             'fight with a zebra',
             )
         for task in tasks:
             cursor.execute("INSERT INTO memos VALUES(NULL, ?)", (task,))

         # Fetch the records to be pickled.
         cursor.execute("SELECT * FROM memos")
         memos = [MemoRecord(key, task) for key, task in cursor]
         # Save the records using our custom DBPickler.
         file = io.BytesIO()
         DBPickler(file).dump(memos)

         print("Pickled records:")
         pprint.pprint(memos)

         # Update a record, just for good measure.
         cursor.execute("UPDATE memos SET task='learn italian' WHERE key=1")

         # Load the records from the pickle data stream.
         file.seek(0)
         memos = DBUnpickler(file, conn).load()

         print("Unpickled records:")
         pprint.pprint(memos)


     if __name__ == '__main__':
         main()

   ---------- Footnotes ----------

   (1) The limitation on alphanumeric characters is due to the fact the
persistent IDs, in protocol 0, are delimited by the newline character.
Therefore if any kind of newline characters occurs in persistent IDs,
the resulting pickle will become unreadable.


File: python.info,  Node: Dispatch Tables,  Next: Handling Stateful Objects,  Prev: Persistence of External Objects,  Up: Pickling Class Instances

5.12.1.9 Dispatch Tables
........................

If one wants to customize pickling of some classes without disturbing
any other code which depends on pickling, then one can create a pickler
with a private dispatch table.

The global dispatch table managed by the *note copyreg: 26. module is
available as ‘copyreg.dispatch_table’.  Therefore, one may choose to use
a modified copy of ‘copyreg.dispatch_table’ as a private dispatch table.

For example

     f = io.BytesIO()
     p = pickle.Pickler(f)
     p.dispatch_table = copyreg.dispatch_table.copy()
     p.dispatch_table[SomeClass] = reduce_SomeClass

creates an instance of *note pickle.Pickler: 69e. with a private
dispatch table which handles the ‘SomeClass’ class specially.
Alternatively, the code

     class MyPickler(pickle.Pickler):
         dispatch_table = copyreg.dispatch_table.copy()
         dispatch_table[SomeClass] = reduce_SomeClass
     f = io.BytesIO()
     p = MyPickler(f)

does the same, but all instances of ‘MyPickler’ will by default share
the same dispatch table.  The equivalent code using the *note copyreg:
26. module is

     copyreg.pickle(SomeClass, reduce_SomeClass)
     f = io.BytesIO()
     p = pickle.Pickler(f)


File: python.info,  Node: Handling Stateful Objects,  Prev: Dispatch Tables,  Up: Pickling Class Instances

5.12.1.10 Handling Stateful Objects
...................................

Here’s an example that shows how to modify pickling behavior for a
class.  The ‘TextReader’ class opens a text file, and returns the line
number and line contents each time its ‘readline()’ method is called.
If a ‘TextReader’ instance is pickled, all attributes `except' the file
object member are saved.  When the instance is unpickled, the file is
reopened, and reading resumes from the last location.  The *note
__setstate__(): a81. and *note __getstate__(): a80. methods are used to
implement this behavior.

     class TextReader:
         """Print and number lines in a text file."""

         def __init__(self, filename):
             self.filename = filename
             self.file = open(filename)
             self.lineno = 0

         def readline(self):
             self.lineno += 1
             line = self.file.readline()
             if not line:
                 return None
             if line.endswith('\n'):
                 line = line[:-1]
             return "%i: %s" % (self.lineno, line)

         def __getstate__(self):
             # Copy the object's state from self.__dict__ which contains
             # all our instance attributes. Always use the dict.copy()
             # method to avoid modifying the original state.
             state = self.__dict__.copy()
             # Remove the unpicklable entries.
             del state['file']
             return state

         def __setstate__(self, state):
             # Restore instance attributes (i.e., filename and lineno).
             self.__dict__.update(state)
             # Restore the previously opened file's state. To do so, we need to
             # reopen it and read from it until the line count is restored.
             file = open(self.filename)
             for _ in range(self.lineno):
                 file.readline()
             # Finally, save the file.
             self.file = file

A sample usage might be something like this:

     >>> reader = TextReader("hello.txt")
     >>> reader.readline()
     '1: Hello world!'
     >>> reader.readline()
     '2: I am line number two.'
     >>> new_reader = pickle.loads(pickle.dumps(reader))
     >>> new_reader.readline()
     '3: Goodbye!'


File: python.info,  Node: Restricting Globals,  Next: Performance<2>,  Prev: Pickling Class Instances,  Up: pickle --- Python object serialization

5.12.1.11 Restricting Globals
.............................

By default, unpickling will import any class or function that it finds
in the pickle data.  For many applications, this behaviour is
unacceptable as it permits the unpickler to import and invoke arbitrary
code.  Just consider what this hand-crafted pickle data stream does when
loaded:

     >>> import pickle
     >>> pickle.loads(b"cos\nsystem\n(S'echo hello world'\ntR.")
     hello world
     0

In this example, the unpickler imports the *note os.system(): a3d.
function and then apply the string argument "echo hello world".
Although this example is inoffensive, it is not difficult to imagine one
that could damage your system.

For this reason, you may want to control what gets unpickled by
customizing *note Unpickler.find_class(): 1665.  Unlike its name
suggests, *note Unpickler.find_class(): 1665. is called whenever a
global (i.e., a class or a function) is requested.  Thus it is possible
to either completely forbid globals or restrict them to a safe subset.

Here is an example of an unpickler allowing only few safe classes from
the *note builtins: 13. module to be loaded:

     import builtins
     import io
     import pickle

     safe_builtins = {
         'range',
         'complex',
         'set',
         'frozenset',
         'slice',
     }

     class RestrictedUnpickler(pickle.Unpickler):

         def find_class(self, module, name):
             # Only allow safe classes from builtins.
             if module == "builtins" and name in safe_builtins:
                 return getattr(builtins, name)
             # Forbid everything else.
             raise pickle.UnpicklingError("global '%s.%s' is forbidden" %
                                          (module, name))

     def restricted_loads(s):
         """Helper function analogous to pickle.loads()."""
         return RestrictedUnpickler(io.BytesIO(s)).load()

A sample usage of our unpickler working has intended:

     >>> restricted_loads(pickle.dumps([1, 2, range(15)]))
     [1, 2, range(0, 15)]
     >>> restricted_loads(b"cos\nsystem\n(S'echo hello world'\ntR.")
     Traceback (most recent call last):
       ...
     pickle.UnpicklingError: global 'os.system' is forbidden
     >>> restricted_loads(b'cbuiltins\neval\n'
     ...                  b'(S\'getattr(__import__("os"), "system")'
     ...                  b'("echo hello world")\'\ntR.')
     Traceback (most recent call last):
       ...
     pickle.UnpicklingError: global 'builtins.eval' is forbidden

As our examples shows, you have to be careful with what you allow to be
unpickled.  Therefore if security is a concern, you may want to consider
alternatives such as the marshalling API in *note xmlrpc.client: 13c. or
third-party solutions.


File: python.info,  Node: Performance<2>,  Next: Examples<4>,  Prev: Restricting Globals,  Up: pickle --- Python object serialization

5.12.1.12 Performance
.....................

Recent versions of the pickle protocol (from protocol 2 and upwards)
feature efficient binary encodings for several common features and
built-in types.  Also, the *note pickle: c8. module has a transparent
optimizer written in C.


File: python.info,  Node: Examples<4>,  Prev: Performance<2>,  Up: pickle --- Python object serialization

5.12.1.13 Examples
..................

For the simplest code, use the *note dump(): 1655. and *note load():
1657. functions.

     import pickle

     # An arbitrary collection of objects supported by pickle.
     data = {
         'a': [1, 2.0, 3, 4+6j],
         'b': ("character string", b"byte string"),
         'c': {None, True, False}
     }

     with open('data.pickle', 'wb') as f:
         # Pickle the 'data' dictionary using the highest protocol available.
         pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)

The following example reads the resulting pickled data.

     import pickle

     with open('data.pickle', 'rb') as f:
         # The protocol version used is detected automatically, so we do not
         # have to specify it.
         data = pickle.load(f)

See also
........

Module *note copyreg: 26.

     Pickle interface constructor registration for extension types.

Module *note pickletools: c9.

     Tools for working with and analyzing pickled data.

Module *note shelve: e5.

     Indexed databases of objects; uses *note pickle: c8.

Module *note copy: 25.

     Shallow and deep object copying.

Module *note marshal: af.

     High-performance serialization of built-in types.


File: python.info,  Node: copyreg --- Register pickle support functions,  Next: shelve --- Python object persistence,  Prev: pickle --- Python object serialization,  Up: Data Persistence

5.12.2 ‘copyreg’ — Register ‘pickle’ support functions
------------------------------------------------------

The *note copyreg: 26. module offers a way to define functions used
while pickling specific objects.  The *note pickle: c8. and *note copy:
25. modules use those functions when pickling/copying those objects.
The module provides configuration information about object constructors
which are not classes.  Such constructors may be factory functions or
class instances.

 -- Function: copyreg.constructor (object)

     Declares `object' to be a valid constructor.  If `object' is not
     callable (and hence not valid as a constructor), raises *note
     TypeError: 562.

 -- Function: copyreg.pickle (type, function, constructor=None)

     Declares that `function' should be used as a "reduction" function
     for objects of type `type'.  `function' should return either a
     string or a tuple containing two or three elements.

     The optional `constructor' parameter, if provided, is a callable
     object which can be used to reconstruct the object when called with
     the tuple of arguments returned by `function' at pickling time.
     *note TypeError: 562. will be raised if `object' is a class or
     `constructor' is not callable.

     See the *note pickle: c8. module for more details on the interface
     expected of `function' and `constructor'.  Note that the *note
     dispatch_table: 69f. attribute of a pickler object or subclass of
     *note pickle.Pickler: 69e. can also be used for declaring reduction
     functions.

* Menu:

* Example: Example<4>. 


File: python.info,  Node: Example<4>,  Up: copyreg --- Register pickle support functions

5.12.2.1 Example
................

The example below would like to show how to register a pickle function
and how it will be used:

     >>> import copyreg, copy, pickle
     >>> class C(object):
     ...     def __init__(self, a):
     ...         self.a = a
     ...
     >>> def pickle_c(c):
     ...     print("pickling a C instance...")
     ...     return C, (c.a,)
     ...
     >>> copyreg.pickle(C, pickle_c)
     >>> c = C(1)
     >>> d = copy.copy(c)
     pickling a C instance...
     >>> p = pickle.dumps(c)
     pickling a C instance...


File: python.info,  Node: shelve --- Python object persistence,  Next: marshal --- Internal Python object serialization,  Prev: copyreg --- Register pickle support functions,  Up: Data Persistence

5.12.3 ‘shelve’ — Python object persistence
-------------------------------------------

`Source code:' Lib/shelve.py(1)

__________________________________________________________________

A "shelf" is a persistent, dictionary-like object.  The difference with
"dbm" databases is that the values (not the keys!)  in a shelf can be
essentially arbitrary Python objects — anything that the *note pickle:
c8. module can handle.  This includes most class instances, recursive
data types, and objects containing lots of shared sub-objects.  The keys
are ordinary strings.

 -- Function: shelve.open (filename, flag='c', protocol=None,
          writeback=False)

     Open a persistent dictionary.  The filename specified is the base
     filename for the underlying database.  As a side-effect, an
     extension may be added to the filename and more than one file may
     be created.  By default, the underlying database file is opened for
     reading and writing.  The optional `flag' parameter has the same
     interpretation as the `flag' parameter of *note dbm.open(): 421.

     By default, version 3 pickles are used to serialize values.  The
     version of the pickle protocol can be specified with the `protocol'
     parameter.

     Because of Python semantics, a shelf cannot know when a mutable
     persistent-dictionary entry is modified.  By default modified
     objects are written `only' when assigned to the shelf (see *note
     Example: 1679.).  If the optional `writeback' parameter is set to
     `True', all entries accessed are also cached in memory, and written
     back on *note sync(): 167a. and *note close(): 167b.; this can make
     it handier to mutate mutable entries in the persistent dictionary,
     but, if many entries are accessed, it can consume vast amounts of
     memory for the cache, and it can make the close operation very slow
     since all accessed entries are written back (there is no way to
     determine which accessed entries are mutable, nor which ones were
     actually mutated).

          Note: Do not rely on the shelf being closed automatically;
          always call *note close(): 167b. explicitly when you don’t
          need it any more, or use *note shelve.open(): 1678. as a
          context manager:

               with shelve.open('spam') as db:
                   db['eggs'] = 'eggs'

     Warning: Because the *note shelve: e5. module is backed by *note
     pickle: c8, it is insecure to load a shelf from an untrusted
     source.  Like with pickle, loading a shelf can execute arbitrary
     code.

Shelf objects support all methods supported by dictionaries.  This eases
the transition from dictionary based scripts to those requiring
persistent storage.

Two additional methods are supported:

 -- Method: Shelf.sync ()

     Write back all entries in the cache if the shelf was opened with
     `writeback' set to *note True: 9ff.  Also empty the cache and
     synchronize the persistent dictionary on disk, if feasible.  This
     is called automatically when the shelf is closed with *note
     close(): 167b.

 -- Method: Shelf.close ()

     Synchronize and close the persistent `dict' object.  Operations on
     a closed shelf will fail with a *note ValueError: 19c.

See also
........

Persistent dictionary recipe(2) with widely supported storage formats
and having the speed of native dictionaries.

* Menu:

* Restrictions:: 
* Example: Example<5>. 

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/shelve.py

   (2) https://code.activestate.com/recipes/576642/


File: python.info,  Node: Restrictions,  Next: Example<5>,  Up: shelve --- Python object persistence

5.12.3.1 Restrictions
.....................

   * The choice of which database package will be used (such as *note
     dbm.ndbm: 33. or *note dbm.gnu: 32.) depends on which interface is
     available.  Therefore it is not safe to open the database directly
     using *note dbm: 30.  The database is also (unfortunately) subject
     to the limitations of *note dbm: 30, if it is used — this means
     that (the pickled representation of) the objects stored in the
     database should be fairly small, and in rare cases key collisions
     may cause the database to refuse updates.

   * The *note shelve: e5. module does not support `concurrent'
     read/write access to shelved objects.  (Multiple simultaneous read
     accesses are safe.)  When a program has a shelf open for writing,
     no other program should have it open for reading or writing.  Unix
     file locking can be used to solve this, but this differs across
     Unix versions and requires knowledge about the database
     implementation used.

 -- Class: shelve.Shelf (dict, protocol=None, writeback=False,
          keyencoding='utf-8')

     A subclass of *note collections.abc.MutableMapping: 61d. which
     stores pickled values in the `dict' object.

     By default, version 0 pickles are used to serialize values.  The
     version of the pickle protocol can be specified with the `protocol'
     parameter.  See the *note pickle: c8. documentation for a
     discussion of the pickle protocols.

     If the `writeback' parameter is ‘True’, the object will hold a
     cache of all entries accessed and write them back to the `dict' at
     sync and close times.  This allows natural operations on mutable
     entries, but can consume much more memory and make sync and close
     take a long time.

     The `keyencoding' parameter is the encoding used to encode keys
     before they are used with the underlying dict.

     A *note Shelf: 4ad. object can also be used as a context manager,
     in which case it will be automatically closed when the *note with:
     29d. block ends.

     Changed in version 3.2: Added the `keyencoding' parameter;
     previously, keys were always encoded in UTF-8.

     Changed in version 3.4: Added context manager support.

 -- Class: shelve.BsdDbShelf (dict, protocol=None, writeback=False,
          keyencoding='utf-8')

     A subclass of *note Shelf: 4ad. which exposes ‘first()’, ‘next()’,
     ‘previous()’, ‘last()’ and ‘set_location()’ which are available in
     the third-party ‘bsddb’ module from pybsddb(1) but not in other
     database modules.  The `dict' object passed to the constructor must
     support those methods.  This is generally accomplished by calling
     one of ‘bsddb.hashopen()’, ‘bsddb.btopen()’ or ‘bsddb.rnopen()’.
     The optional `protocol', `writeback', and `keyencoding' parameters
     have the same interpretation as for the *note Shelf: 4ad. class.

 -- Class: shelve.DbfilenameShelf (filename, flag='c', protocol=None,
          writeback=False)

     A subclass of *note Shelf: 4ad. which accepts a `filename' instead
     of a dict-like object.  The underlying file will be opened using
     *note dbm.open(): 421.  By default, the file will be created and
     opened for both read and write.  The optional `flag' parameter has
     the same interpretation as for the *note open(): 1678. function.
     The optional `protocol' and `writeback' parameters have the same
     interpretation as for the *note Shelf: 4ad. class.

   ---------- Footnotes ----------

   (1) https://www.jcea.es/programacion/pybsddb.htm


File: python.info,  Node: Example<5>,  Prev: Restrictions,  Up: shelve --- Python object persistence

5.12.3.2 Example
................

To summarize the interface (‘key’ is a string, ‘data’ is an arbitrary
object):

     import shelve

     d = shelve.open(filename) # open -- file may get suffix added by low-level
                               # library

     d[key] = data   # store data at key (overwrites old data if
                     # using an existing key)
     data = d[key]   # retrieve a COPY of data at key (raise KeyError if no
                     # such key)
     del d[key]      # delete data stored at key (raises KeyError
                     # if no such key)
     flag = key in d        # true if the key exists
     klist = list(d.keys()) # a list of all existing keys (slow!)

     # as d was opened WITHOUT writeback=True, beware:
     d['xx'] = [0, 1, 2]    # this works as expected, but...
     d['xx'].append(3)      # *this doesn't!* -- d['xx'] is STILL [0, 1, 2]!

     # having opened d without writeback=True, you need to code carefully:
     temp = d['xx']      # extracts the copy
     temp.append(5)      # mutates the copy
     d['xx'] = temp      # stores the copy right back, to persist it

     # or, d=shelve.open(filename,writeback=True) would let you just code
     # d['xx'].append(5) and have it work as expected, BUT it would also
     # consume more memory and make the d.close() operation slower.

     d.close()       # close it

See also
........

Module *note dbm: 30.

     Generic interface to ‘dbm’-style databases.

Module *note pickle: c8.

     Object serialization used by *note shelve: e5.


File: python.info,  Node: marshal --- Internal Python object serialization,  Next: dbm --- Interfaces to Unix "databases",  Prev: shelve --- Python object persistence,  Up: Data Persistence

5.12.4 ‘marshal’ — Internal Python object serialization
-------------------------------------------------------

This module contains functions that can read and write Python values in
a binary format.  The format is specific to Python, but independent of
machine architecture issues (e.g., you can write a Python value to a
file on a PC, transport the file to a Sun, and read it back there).
Details of the format are undocumented on purpose; it may change between
Python versions (although it rarely does).  (1)

This is not a general "persistence" module.  For general persistence and
transfer of Python objects through RPC calls, see the modules *note
pickle: c8. and *note shelve: e5.  The *note marshal: af. module exists
mainly to support reading and writing the "pseudo-compiled" code for
Python modules of ‘.pyc’ files.  Therefore, the Python maintainers
reserve the right to modify the marshal format in backward incompatible
ways should the need arise.  If you’re serializing and de-serializing
Python objects, use the *note pickle: c8. module instead – the
performance is comparable, version independence is guaranteed, and
pickle supports a substantially wider range of objects than marshal.

     Warning: The *note marshal: af. module is not intended to be secure
     against erroneous or maliciously constructed data.  Never unmarshal
     data received from an untrusted or unauthenticated source.

Not all Python object types are supported; in general, only objects
whose value is independent from a particular invocation of Python can be
written and read by this module.  The following types are supported:
booleans, integers, floating point numbers, complex numbers, strings,
bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, and
code objects, where it should be understood that tuples, lists, sets,
frozensets and dictionaries are only supported as long as the values
contained therein are themselves supported.  singletons *note None: 19d,
*note Ellipsis: fc2. and *note StopIteration: 191. can also be
marshalled and unmarshalled.  For format `version' lower than 3,
recursive lists, sets and dictionaries cannot be written (see below).

There are functions that read/write files as well as functions operating
on strings.

The module defines these functions:

 -- Function: marshal.dump (value, file[, version])

     Write the value on the open file.  The value must be a supported
     type.  The file must be an open file object such as ‘sys.stdout’ or
     returned by *note open(): 1e8. or *note os.popen(): 7d7.  It must
     be opened in binary mode (‘'wb'’ or ‘'w+b'’).

     If the value has (or contains an object that has) an unsupported
     type, a *note ValueError: 19c. exception is raised — but garbage
     data will also be written to the file.  The object will not be
     properly read back by *note load(): 1683.

     The `version' argument indicates the data format that ‘dump’ should
     use (see below).

 -- Function: marshal.load (file)

     Read one value from the open file and return it.  If no valid value
     is read (e.g.  because the data has a different Python version’s
     incompatible marshal format), raise *note EOFError: 8d8, *note
     ValueError: 19c. or *note TypeError: 562.  The file must be an open
     file object opened in binary mode (‘'rb'’ or ‘'r+b'’).

          Note: If an object containing an unsupported type was
          marshalled with *note dump(): 1682, *note load(): 1683. will
          substitute ‘None’ for the unmarshallable type.

 -- Function: marshal.dumps (value[, version])

     Return the string that would be written to a file by ‘dump(value,
     file)’.  The value must be a supported type.  Raise a *note
     ValueError: 19c. exception if value has (or contains an object that
     has) an unsupported type.

     The `version' argument indicates the data format that ‘dumps’
     should use (see below).

 -- Function: marshal.loads (string)

     Convert the string to a value.  If no valid value is found, raise
     *note EOFError: 8d8, *note ValueError: 19c. or *note TypeError:
     562.  Extra characters in the string are ignored.

In addition, the following constants are defined:

 -- Data: marshal.version

     Indicates the format that the module uses.  Version 0 is the
     historical format, version 1 shares interned strings and version 2
     uses a binary format for floating point numbers.  Version 3 adds
     support for object instancing and recursion.  The current version
     is 4.

   ---------- Footnotes ----------

   (1) The name of this module stems from a bit of terminology used by
the designers of Modula-3 (amongst others), who use the term
"marshalling" for shipping of data around in a self-contained form.
Strictly speaking, "to marshal" means to convert some data from internal
to external form (in an RPC buffer for instance) and "unmarshalling" for
the reverse process.


File: python.info,  Node: dbm --- Interfaces to Unix "databases",  Next: sqlite3 --- DB-API 2 0 interface for SQLite databases,  Prev: marshal --- Internal Python object serialization,  Up: Data Persistence

5.12.5 ‘dbm’ — Interfaces to Unix "databases"
---------------------------------------------

*note dbm: 30. is a generic interface to variants of the DBM database —
*note dbm.gnu: 32. or *note dbm.ndbm: 33.  If none of these modules is
installed, the slow-but-simple implementation in module *note dbm.dumb:
31. will be used.  There is a third party interface(1) to the Oracle
Berkeley DB.

 -- Exception: dbm.error

     A tuple containing the exceptions that can be raised by each of the
     supported modules, with a unique exception also named *note
     dbm.error: 1688. as the first item — the latter is used when *note
     dbm.error: 1688. is raised.

 -- Function: dbm.whichdb (filename)

     This function attempts to guess which of the several simple
     database modules available — *note dbm.gnu: 32, *note dbm.ndbm: 33.
     or *note dbm.dumb: 31. — should be used to open a given file.

     Returns one of the following values: ‘None’ if the file can’t be
     opened because it’s unreadable or doesn’t exist; the empty string
     (‘''’) if the file’s format can’t be guessed; or a string
     containing the required module name, such as ‘'dbm.ndbm'’ or
     ‘'dbm.gnu'’.

 -- Function: dbm.open (file, flag='r', mode=0o666)

     Open the database file `file' and return a corresponding object.

     If the database file already exists, the *note whichdb(): 1689.
     function is used to determine its type and the appropriate module
     is used; if it does not exist, the first module listed above that
     can be imported is used.

     The optional `flag' argument can be:

     Value         Meaning
                   
     --------------------------------------------------------------
                   
     ‘'r'’         Open existing database for reading only
                   (default)
                   
                   
     ‘'w'’         Open existing database for reading and
                   writing
                   
                   
     ‘'c'’         Open database for reading and writing,
                   creating it if it doesn’t exist
                   
                   
     ‘'n'’         Always create a new, empty database, open for
                   reading and writing
                   

     The optional `mode' argument is the Unix mode of the file, used
     only when the database has to be created.  It defaults to octal
     ‘0o666’ (and will be modified by the prevailing umask).

The object returned by *note open(): 421. supports the same basic
functionality as dictionaries; keys and their corresponding values can
be stored, retrieved, and deleted, and the *note in: 37d. operator and
the ‘keys()’ method are available, as well as ‘get()’ and
‘setdefault()’.

Changed in version 3.2: ‘get()’ and ‘setdefault()’ are now available in
all database modules.

Key and values are always stored as bytes.  This means that when strings
are used they are implicitly converted to the default encoding before
being stored.

These objects also support being used in a *note with: 29d. statement,
which will automatically close them when done.

Changed in version 3.4: Added native support for the context management
protocol to the objects returned by *note open(): 421.

The following example records some hostnames and a corresponding title,
and then prints out the contents of the database:

     import dbm

     # Open database, creating it if necessary.
     with dbm.open('cache', 'c') as db:

         # Record some values
         db[b'hello'] = b'there'
         db['www.python.org'] = 'Python Website'
         db['www.cnn.com'] = 'Cable News Network'

         # Note that the keys are considered bytes now.
         assert db[b'www.python.org'] == b'Python Website'
         # Notice how the value is now in bytes.
         assert db['www.cnn.com'] == b'Cable News Network'

         # Often-used methods of the dict interface work too.
         print(db.get('python.org', b'not present'))

         # Storing a non-string key or value will raise an exception (most
         # likely a TypeError).
         db['www.yahoo.com'] = 4

     # db is automatically closed when leaving the with statement.

See also
........

Module *note shelve: e5.

     Persistence module which stores non-string data.

The individual submodules are described in the following sections.

* Menu:

* dbm.gnu: dbm gnu --- GNU's reinterpretation of dbm. GNU’s reinterpretation of dbm
* dbm.ndbm: dbm ndbm --- Interface based on ndbm. Interface based on ndbm
* dbm.dumb: dbm dumb --- Portable DBM implementation. Portable DBM implementation

   ---------- Footnotes ----------

   (1) https://www.jcea.es/programacion/pybsddb.htm


File: python.info,  Node: dbm gnu --- GNU's reinterpretation of dbm,  Next: dbm ndbm --- Interface based on ndbm,  Up: dbm --- Interfaces to Unix "databases"

5.12.5.1 ‘dbm.gnu’ — GNU’s reinterpretation of dbm
..................................................

This module is quite similar to the *note dbm: 30. module, but uses the
GNU library ‘gdbm’ instead to provide some additional functionality.
Please note that the file formats created by *note dbm.gnu: 32. and
*note dbm.ndbm: 33. are incompatible.

The *note dbm.gnu: 32. module provides an interface to the GNU DBM
library.  ‘dbm.gnu.gdbm’ objects behave like mappings (dictionaries),
except that keys and values are always converted to bytes before
storing.  Printing a ‘gdbm’ object doesn’t print the keys and values,
and the ‘items()’ and ‘values()’ methods are not supported.

 -- Exception: dbm.gnu.error

     Raised on *note dbm.gnu: 32.-specific errors, such as I/O errors.
     *note KeyError: 1a7. is raised for general mapping errors like
     specifying an incorrect key.

 -- Function: dbm.gnu.open (filename[, flag[, mode]])

     Open a ‘gdbm’ database and return a ‘gdbm’ object.  The `filename'
     argument is the name of the database file.

     The optional `flag' argument can be:

     Value         Meaning
                   
     --------------------------------------------------------------
                   
     ‘'r'’         Open existing database for reading only
                   (default)
                   
                   
     ‘'w'’         Open existing database for reading and
                   writing
                   
                   
     ‘'c'’         Open database for reading and writing,
                   creating it if it doesn’t exist
                   
                   
     ‘'n'’         Always create a new, empty database, open for
                   reading and writing
                   

     The following additional characters may be appended to the flag to
     control how the database is opened:

     Value         Meaning
                   
     ---------------------------------------------------------------
                   
     ‘'f'’         Open the database in fast mode.  Writes to the
                   database will not be synchronized.
                   
                   
     ‘'s'’         Synchronized mode.  This will cause changes to
                   the database to be immediately written to the
                   file.
                   
                   
     ‘'u'’         Do not lock database.
                   

     Not all flags are valid for all versions of ‘gdbm’.  The module
     constant ‘open_flags’ is a string of supported flag characters.
     The exception *note error: 168b. is raised if an invalid flag is
     specified.

     The optional `mode' argument is the Unix mode of the file, used
     only when the database has to be created.  It defaults to octal
     ‘0o666’.

     In addition to the dictionary-like methods, ‘gdbm’ objects have the
     following methods:

      -- Method: gdbm.firstkey ()

          It’s possible to loop over every key in the database using
          this method and the *note nextkey(): 168e. method.  The
          traversal is ordered by ‘gdbm’’s internal hash values, and
          won’t be sorted by the key values.  This method returns the
          starting key.

      -- Method: gdbm.nextkey (key)

          Returns the key that follows `key' in the traversal.  The
          following code prints every key in the database ‘db’, without
          having to create a list in memory that contains them all:

               k = db.firstkey()
               while k != None:
                   print(k)
                   k = db.nextkey(k)

      -- Method: gdbm.reorganize ()

          If you have carried out a lot of deletions and would like to
          shrink the space used by the ‘gdbm’ file, this routine will
          reorganize the database.  ‘gdbm’ objects will not shorten the
          length of a database file except by using this reorganization;
          otherwise, deleted file space will be kept and reused as new
          (key, value) pairs are added.

      -- Method: gdbm.sync ()

          When the database has been opened in fast mode, this method
          forces any unwritten data to be written to the disk.

      -- Method: gdbm.close ()

          Close the ‘gdbm’ database.


File: python.info,  Node: dbm ndbm --- Interface based on ndbm,  Next: dbm dumb --- Portable DBM implementation,  Prev: dbm gnu --- GNU's reinterpretation of dbm,  Up: dbm --- Interfaces to Unix "databases"

5.12.5.2 ‘dbm.ndbm’ — Interface based on ndbm
.............................................

The *note dbm.ndbm: 33. module provides an interface to the Unix
"(n)dbm" library.  Dbm objects behave like mappings (dictionaries),
except that keys and values are always stored as bytes.  Printing a
‘dbm’ object doesn’t print the keys and values, and the ‘items()’ and
‘values()’ methods are not supported.

This module can be used with the "classic" ndbm interface or the GNU
GDBM compatibility interface.  On Unix, the ‘configure’ script will
attempt to locate the appropriate header file to simplify building this
module.

 -- Exception: dbm.ndbm.error

     Raised on *note dbm.ndbm: 33.-specific errors, such as I/O errors.
     *note KeyError: 1a7. is raised for general mapping errors like
     specifying an incorrect key.

 -- Data: dbm.ndbm.library

     Name of the ‘ndbm’ implementation library used.

 -- Function: dbm.ndbm.open (filename[, flag[, mode]])

     Open a dbm database and return a ‘ndbm’ object.  The `filename'
     argument is the name of the database file (without the ‘.dir’ or
     ‘.pag’ extensions).

     The optional `flag' argument must be one of these values:

     Value         Meaning
                   
     --------------------------------------------------------------
                   
     ‘'r'’         Open existing database for reading only
                   (default)
                   
                   
     ‘'w'’         Open existing database for reading and
                   writing
                   
                   
     ‘'c'’         Open database for reading and writing,
                   creating it if it doesn’t exist
                   
                   
     ‘'n'’         Always create a new, empty database, open for
                   reading and writing
                   

     The optional `mode' argument is the Unix mode of the file, used
     only when the database has to be created.  It defaults to octal
     ‘0o666’ (and will be modified by the prevailing umask).

     In addition to the dictionary-like methods, ‘ndbm’ objects provide
     the following method:

      -- Method: ndbm.close ()

          Close the ‘ndbm’ database.


File: python.info,  Node: dbm dumb --- Portable DBM implementation,  Prev: dbm ndbm --- Interface based on ndbm,  Up: dbm --- Interfaces to Unix "databases"

5.12.5.3 ‘dbm.dumb’ — Portable DBM implementation
.................................................

     Note: The *note dbm.dumb: 31. module is intended as a last resort
     fallback for the *note dbm: 30. module when a more robust module is
     not available.  The *note dbm.dumb: 31. module is not written for
     speed and is not nearly as heavily used as the other database
     modules.

The *note dbm.dumb: 31. module provides a persistent dictionary-like
interface which is written entirely in Python.  Unlike other modules
such as *note dbm.gnu: 32. no external library is required.  As with
other persistent mappings, the keys and values are always stored as
bytes.

The module defines the following:

 -- Exception: dbm.dumb.error

     Raised on *note dbm.dumb: 31.-specific errors, such as I/O errors.
     *note KeyError: 1a7. is raised for general mapping errors like
     specifying an incorrect key.

 -- Function: dbm.dumb.open (filename[, flag[, mode]])

     Open a ‘dumbdbm’ database and return a dumbdbm object.  The
     `filename' argument is the basename of the database file (without
     any specific extensions).  When a dumbdbm database is created,
     files with ‘.dat’ and ‘.dir’ extensions are created.

     The optional `flag' argument supports only the semantics of ‘'c'’
     and ‘'n'’ values.  Other values will default to database being
     always opened for update, and will be created if it does not exist.

     The optional `mode' argument is the Unix mode of the file, used
     only when the database has to be created.  It defaults to octal
     ‘0o666’ (and will be modified by the prevailing umask).

     Changed in version 3.5: *note open(): 276. always creates a new
     database when the flag has the value ‘'n'’.

     In addition to the methods provided by the *note
     collections.abc.MutableMapping: 61d. class, ‘dumbdbm’ objects
     provide the following methods:

      -- Method: dumbdbm.sync ()

          Synchronize the on-disk directory and data files.  This method
          is called by the ‘Shelve.sync()’ method.

      -- Method: dumbdbm.close ()

          Close the ‘dumbdbm’ database.


File: python.info,  Node: sqlite3 --- DB-API 2 0 interface for SQLite databases,  Prev: dbm --- Interfaces to Unix "databases",  Up: Data Persistence

5.12.6 ‘sqlite3’ — DB-API 2.0 interface for SQLite databases
------------------------------------------------------------

SQLite is a C library that provides a lightweight disk-based database
that doesn’t require a separate server process and allows accessing the
database using a nonstandard variant of the SQL query language.  Some
applications can use SQLite for internal data storage.  It’s also
possible to prototype an application using SQLite and then port the code
to a larger database such as PostgreSQL or Oracle.

The sqlite3 module was written by Gerhard Häring.  It provides a SQL
interface compliant with the DB-API 2.0 specification described by PEP
249(1).

To use the module, you must first create a *note Connection: 6c6. object
that represents the database.  Here the data will be stored in the
‘example.db’ file:

     import sqlite3
     conn = sqlite3.connect('example.db')

You can also supply the special name ‘:memory:’ to create a database in
RAM.

Once you have a *note Connection: 6c6, you can create a *note Cursor:
169d. object and call its *note execute(): 169e. method to perform SQL
commands:

     c = conn.cursor()

     # Create table
     c.execute('''CREATE TABLE stocks
                  (date text, trans text, symbol text, qty real, price real)''')

     # Insert a row of data
     c.execute("INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)")

     # Save (commit) the changes
     conn.commit()

     # We can also close the connection if we are done with it.
     # Just be sure any changes have been committed or they will be lost.
     conn.close()

The data you’ve saved is persistent and is available in subsequent
sessions:

     import sqlite3
     conn = sqlite3.connect('example.db')
     c = conn.cursor()

Usually your SQL operations will need to use values from Python
variables.  You shouldn’t assemble your query using Python’s string
operations because doing so is insecure; it makes your program
vulnerable to an SQL injection attack (see ‘http://xkcd.com/327/’ for
humorous example of what can go wrong).

Instead, use the DB-API’s parameter substitution.  Put ‘?’ as a
placeholder wherever you want to use a value, and then provide a tuple
of values as the second argument to the cursor’s *note execute(): 169e.
method.  (Other database modules may use a different placeholder, such
as ‘%s’ or ‘:1’.)  For example:

     # Never do this -- insecure!
     symbol = 'RHAT'
     c.execute("SELECT * FROM stocks WHERE symbol = '%s'" % symbol)

     # Do this instead
     t = ('RHAT',)
     c.execute('SELECT * FROM stocks WHERE symbol=?', t)
     print(c.fetchone())

     # Larger example that inserts many records at a time
     purchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),
                  ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),
                  ('2006-04-06', 'SELL', 'IBM', 500, 53.00),
                 ]
     c.executemany('INSERT INTO stocks VALUES (?,?,?,?,?)', purchases)

To retrieve data after executing a SELECT statement, you can either
treat the cursor as an *note iterator: e4f, call the cursor’s *note
fetchone(): 169f. method to retrieve a single matching row, or call
*note fetchall(): 16a0. to get a list of the matching rows.

This example uses the iterator form:

     >>> for row in c.execute('SELECT * FROM stocks ORDER BY price'):
             print(row)

     ('2006-01-05', 'BUY', 'RHAT', 100, 35.14)
     ('2006-03-28', 'BUY', 'IBM', 1000, 45.0)
     ('2006-04-06', 'SELL', 'IBM', 500, 53.0)
     ('2006-04-05', 'BUY', 'MSFT', 1000, 72.0)

See also
........

‘https://github.com/ghaering/pysqlite’

     The pysqlite web page – sqlite3 is developed externally under the
     name "pysqlite".

‘http://www.sqlite.org’

     The SQLite web page; the documentation describes the syntax and the
     available data types for the supported SQL dialect.

‘http://www.w3schools.com/sql/’

     Tutorial, reference and examples for learning SQL syntax.

PEP 249(2) - Database API Specification 2.0

     PEP written by Marc-André Lemburg.

* Menu:

* Module functions and constants:: 
* Connection Objects:: 
* Cursor Objects:: 
* Row Objects:: 
* SQLite and Python types:: 
* Controlling Transactions:: 
* Using sqlite3 efficiently:: 
* Common issues:: 

   ---------- Footnotes ----------

   (1) https://www.python.org/dev/peps/pep-0249

   (2) https://www.python.org/dev/peps/pep-0249


File: python.info,  Node: Module functions and constants,  Next: Connection Objects,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.1 Module functions and constants
.......................................

 -- Data: sqlite3.version

     The version number of this module, as a string.  This is not the
     version of the SQLite library.

 -- Data: sqlite3.version_info

     The version number of this module, as a tuple of integers.  This is
     not the version of the SQLite library.

 -- Data: sqlite3.sqlite_version

     The version number of the run-time SQLite library, as a string.

 -- Data: sqlite3.sqlite_version_info

     The version number of the run-time SQLite library, as a tuple of
     integers.

 -- Data: sqlite3.PARSE_DECLTYPES

     This constant is meant to be used with the `detect_types' parameter
     of the *note connect(): 4bd. function.

     Setting it makes the *note sqlite3: f0. module parse the declared
     type for each column it returns.  It will parse out the first word
     of the declared type, i.  e.  for "integer primary key", it will
     parse out "integer", or for "number(10)" it will parse out
     "number".  Then for that column, it will look into the converters
     dictionary and use the converter function registered for that type
     there.

 -- Data: sqlite3.PARSE_COLNAMES

     This constant is meant to be used with the `detect_types' parameter
     of the *note connect(): 4bd. function.

     Setting this makes the SQLite interface parse the column name for
     each column it returns.  It will look for a string formed [mytype]
     in there, and then decide that ’mytype’ is the type of the column.
     It will try to find an entry of ’mytype’ in the converters
     dictionary and then use the converter function found there to
     return the value.  The column name found in *note
     Cursor.description: 16a9. is only the first word of the column
     name, i.  e.  if you use something like ‘'as "x [datetime]"'’ in
     your SQL, then we will parse out everything until the first blank
     for the column name: the column name would simply be "x".

 -- Function: sqlite3.connect (database[, timeout, detect_types,
          isolation_level, check_same_thread, factory,
          cached_statements, uri])

     Opens a connection to the SQLite database file `database'.  You can
     use ‘":memory:"’ to open a database connection to a database that
     resides in RAM instead of on disk.

     When a database is accessed by multiple connections, and one of the
     processes modifies the database, the SQLite database is locked
     until that transaction is committed.  The `timeout' parameter
     specifies how long the connection should wait for the lock to go
     away until raising an exception.  The default for the timeout
     parameter is 5.0 (five seconds).

     For the `isolation_level' parameter, please see the *note
     Connection.isolation_level: 16aa. property of *note Connection:
     6c6. objects.

     SQLite natively supports only the types TEXT, INTEGER, REAL, BLOB
     and NULL. If you want to use other types you must add support for
     them yourself.  The `detect_types' parameter and the using custom
     `converters' registered with the module-level *note
     register_converter(): 16ab. function allow you to easily do that.

     `detect_types' defaults to 0 (i.  e.  off, no type detection), you
     can set it to any combination of *note PARSE_DECLTYPES: 16a7. and
     *note PARSE_COLNAMES: 16a8. to turn type detection on.

     By default, the *note sqlite3: f0. module uses its *note
     Connection: 6c6. class for the connect call.  You can, however,
     subclass the *note Connection: 6c6. class and make *note connect():
     4bd. use your class instead by providing your class for the
     `factory' parameter.

     Consult the section *note SQLite and Python types: 16ac. of this
     manual for details.

     The *note sqlite3: f0. module internally uses a statement cache to
     avoid SQL parsing overhead.  If you want to explicitly set the
     number of statements that are cached for the connection, you can
     set the `cached_statements' parameter.  The currently implemented
     default is to cache 100 statements.

     If `uri' is true, `database' is interpreted as a URI. This allows
     you to specify options.  For example, to open a database in
     read-only mode you can use:

          db = sqlite3.connect('file:path/to/database?mode=ro', uri=True)

     More information about this feature, including a list of recognized
     options, can be found in the SQLite URI documentation(1).

     Changed in version 3.4: Added the `uri' parameter.

 -- Function: sqlite3.register_converter (typename, callable)

     Registers a callable to convert a bytestring from the database into
     a custom Python type.  The callable will be invoked for all
     database values that are of the type `typename'.  Confer the
     parameter `detect_types' of the *note connect(): 4bd. function for
     how the type detection works.  Note that the case of `typename' and
     the name of the type in your query must match!

 -- Function: sqlite3.register_adapter (type, callable)

     Registers a callable to convert the custom Python type `type' into
     one of SQLite’s supported types.  The callable `callable' accepts
     as single parameter the Python value, and must return a value of
     the following types: int, float, str or bytes.

 -- Function: sqlite3.complete_statement (sql)

     Returns *note True: 9ff. if the string `sql' contains one or more
     complete SQL statements terminated by semicolons.  It does not
     verify that the SQL is syntactically correct, only that there are
     no unclosed string literals and the statement is terminated by a
     semicolon.

     This can be used to build a shell for SQLite, as in the following
     example:

          # A minimal SQLite shell for experiments

          import sqlite3

          con = sqlite3.connect(":memory:")
          con.isolation_level = None
          cur = con.cursor()

          buffer = ""

          print("Enter your SQL commands to execute in sqlite3.")
          print("Enter a blank line to exit.")

          while True:
              line = input()
              if line == "":
                  break
              buffer += line
              if sqlite3.complete_statement(buffer):
                  try:
                      buffer = buffer.strip()
                      cur.execute(buffer)

                      if buffer.lstrip().upper().startswith("SELECT"):
                          print(cur.fetchall())
                  except sqlite3.Error as e:
                      print("An error occurred:", e.args[0])
                  buffer = ""

          con.close()

 -- Function: sqlite3.enable_callback_tracebacks (flag)

     By default you will not get any tracebacks in user-defined
     functions, aggregates, converters, authorizer callbacks etc.  If
     you want to debug them, you can call this function with `flag' set
     to ‘True’.  Afterwards, you will get tracebacks from callbacks on
     ‘sys.stderr’.  Use *note False: 60d. to disable the feature again.

   ---------- Footnotes ----------

   (1) http://www.sqlite.org/uri.html


File: python.info,  Node: Connection Objects,  Next: Cursor Objects,  Prev: Module functions and constants,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.2 Connection Objects
...........................

 -- Class: sqlite3.Connection

     A SQLite database connection has the following attributes and
     methods:

      -- Attribute: isolation_level

          Get or set the current isolation level.  *note None: 19d. for
          autocommit mode or one of "DEFERRED", "IMMEDIATE" or
          "EXCLUSIVE". See section *note Controlling Transactions: 16b2.
          for a more detailed explanation.

      -- Attribute: in_transaction

          *note True: 9ff. if a transaction is active (there are
          uncommitted changes), *note False: 60d. otherwise.  Read-only
          attribute.

          New in version 3.2.

      -- Method: cursor ([cursorClass])

          The cursor method accepts a single optional parameter
          `cursorClass'.  If supplied, this must be a custom cursor
          class that extends *note sqlite3.Cursor: 169d.

      -- Method: commit ()

          This method commits the current transaction.  If you don’t
          call this method, anything you did since the last call to
          ‘commit()’ is not visible from other database connections.  If
          you wonder why you don’t see the data you’ve written to the
          database, please check you didn’t forget to call this method.

      -- Method: rollback ()

          This method rolls back any changes to the database since the
          last call to *note commit(): 16b5.

      -- Method: close ()

          This closes the database connection.  Note that this does not
          automatically call *note commit(): 16b5.  If you just close
          your database connection without calling *note commit(): 16b5.
          first, your changes will be lost!

      -- Method: execute (sql[, parameters])

          This is a nonstandard shortcut that creates an intermediate
          cursor object by calling the cursor method, then calls the
          cursor’s *note execute: 169e. method with the parameters
          given.

      -- Method: executemany (sql[, parameters])

          This is a nonstandard shortcut that creates an intermediate
          cursor object by calling the cursor method, then calls the
          cursor’s *note executemany: 16ba. method with the parameters
          given.

      -- Method: executescript (sql_script)

          This is a nonstandard shortcut that creates an intermediate
          cursor object by calling the cursor method, then calls the
          cursor’s *note executescript: 16bc. method with the parameters
          given.

      -- Method: create_function (name, num_params, func)

          Creates a user-defined function that you can later use from
          within SQL statements under the function name `name'.
          `num_params' is the number of parameters the function accepts
          (if `num_params' is -1, the function may take any number of
          arguments), and `func' is a Python callable that is called as
          the SQL function.

          The function can return any of the types supported by SQLite:
          bytes, str, int, float and None.

          Example:

               import sqlite3
               import hashlib

               def md5sum(t):
                   return hashlib.md5(t).hexdigest()

               con = sqlite3.connect(":memory:")
               con.create_function("md5", 1, md5sum)
               cur = con.cursor()
               cur.execute("select md5(?)", (b"foo",))
               print(cur.fetchone()[0])

      -- Method: create_aggregate (name, num_params, aggregate_class)

          Creates a user-defined aggregate function.

          The aggregate class must implement a ‘step’ method, which
          accepts the number of parameters `num_params' (if `num_params'
          is -1, the function may take any number of arguments), and a
          ‘finalize’ method which will return the final result of the
          aggregate.

          The ‘finalize’ method can return any of the types supported by
          SQLite: bytes, str, int, float and None.

          Example:

               import sqlite3

               class MySum:
                   def __init__(self):
                       self.count = 0

                   def step(self, value):
                       self.count += value

                   def finalize(self):
                       return self.count

               con = sqlite3.connect(":memory:")
               con.create_aggregate("mysum", 1, MySum)
               cur = con.cursor()
               cur.execute("create table test(i)")
               cur.execute("insert into test(i) values (1)")
               cur.execute("insert into test(i) values (2)")
               cur.execute("select mysum(i) from test")
               print(cur.fetchone()[0])

      -- Method: create_collation (name, callable)

          Creates a collation with the specified `name' and `callable'.
          The callable will be passed two string arguments.  It should
          return -1 if the first is ordered lower than the second, 0 if
          they are ordered equal and 1 if the first is ordered higher
          than the second.  Note that this controls sorting (ORDER BY in
          SQL) so your comparisons don’t affect other SQL operations.

          Note that the callable will get its parameters as Python
          bytestrings, which will normally be encoded in UTF-8.

          The following example shows a custom collation that sorts "the
          wrong way":

               import sqlite3

               def collate_reverse(string1, string2):
                   if string1 == string2:
                       return 0
                   elif string1 < string2:
                       return 1
                   else:
                       return -1

               con = sqlite3.connect(":memory:")
               con.create_collation("reverse", collate_reverse)

               cur = con.cursor()
               cur.execute("create table test(x)")
               cur.executemany("insert into test(x) values (?)", [("a",), ("b",)])
               cur.execute("select x from test order by x collate reverse")
               for row in cur:
                   print(row)
               con.close()

          To remove a collation, call ‘create_collation’ with None as
          callable:

               con.create_collation("reverse", None)

      -- Method: interrupt ()

          You can call this method from a different thread to abort any
          queries that might be executing on the connection.  The query
          will then abort and the caller will get an exception.

      -- Method: set_authorizer (authorizer_callback)

          This routine registers a callback.  The callback is invoked
          for each attempt to access a column of a table in the
          database.  The callback should return ‘SQLITE_OK’ if access is
          allowed, ‘SQLITE_DENY’ if the entire SQL statement should be
          aborted with an error and ‘SQLITE_IGNORE’ if the column should
          be treated as a NULL value.  These constants are available in
          the *note sqlite3: f0. module.

          The first argument to the callback signifies what kind of
          operation is to be authorized.  The second and third argument
          will be arguments or *note None: 19d. depending on the first
          argument.  The 4th argument is the name of the database
          ("main", "temp", etc.)  if applicable.  The 5th argument is
          the name of the inner-most trigger or view that is responsible
          for the access attempt or *note None: 19d. if this access
          attempt is directly from input SQL code.

          Please consult the SQLite documentation about the possible
          values for the first argument and the meaning of the second
          and third argument depending on the first one.  All necessary
          constants are available in the *note sqlite3: f0. module.

      -- Method: set_progress_handler (handler, n)

          This routine registers a callback.  The callback is invoked
          for every `n' instructions of the SQLite virtual machine.
          This is useful if you want to get called from SQLite during
          long-running operations, for example to update a GUI.

          If you want to clear any previously installed progress
          handler, call the method with *note None: 19d. for `handler'.

      -- Method: set_trace_callback (trace_callback)

          Registers `trace_callback' to be called for each SQL statement
          that is actually executed by the SQLite backend.

          The only argument passed to the callback is the statement (as
          string) that is being executed.  The return value of the
          callback is ignored.  Note that the backend does not only run
          statements passed to the *note Cursor.execute(): 169e.
          methods.  Other sources include the transaction management of
          the Python module and the execution of triggers defined in the
          current database.

          Passing *note None: 19d. as `trace_callback' will disable the
          trace callback.

          New in version 3.3.

      -- Method: enable_load_extension (enabled)

          This routine allows/disallows the SQLite engine to load SQLite
          extensions from shared libraries.  SQLite extensions can
          define new functions, aggregates or whole new virtual table
          implementations.  One well-known extension is the
          fulltext-search extension distributed with SQLite.

          Loadable extensions are disabled by default.  See (1).

          New in version 3.2.

               import sqlite3

               con = sqlite3.connect(":memory:")

               # enable extension loading
               con.enable_load_extension(True)

               # Load the fulltext search extension
               con.execute("select load_extension('./fts3.so')")

               # alternatively you can load the extension using an API call:
               # con.load_extension("./fts3.so")

               # disable extension laoding again
               con.enable_load_extension(False)

               # example from SQLite wiki
               con.execute("create virtual table recipe using fts3(name, ingredients)")
               con.executescript("""
                   insert into recipe (name, ingredients) values ('broccoli stew', 'broccoli peppers cheese tomatoes');
                   insert into recipe (name, ingredients) values ('pumpkin stew', 'pumpkin onions garlic celery');
                   insert into recipe (name, ingredients) values ('broccoli pie', 'broccoli cheese onions flour');
                   insert into recipe (name, ingredients) values ('pumpkin pie', 'pumpkin sugar flour butter');
                   """)
               for row in con.execute("select rowid, name, ingredients from recipe where name match 'pie'"):
                   print(row)

      -- Method: load_extension (path)

          This routine loads a SQLite extension from a shared library.
          You have to enable extension loading with *note
          enable_load_extension(): 7f4. before you can use this routine.

          Loadable extensions are disabled by default.  See (2).

          New in version 3.2.

      -- Attribute: row_factory

          You can change this attribute to a callable that accepts the
          cursor and the original row as a tuple and will return the
          real result row.  This way, you can implement more advanced
          ways of returning results, such as returning an object that
          can also access columns by name.

          Example:

               import sqlite3

               def dict_factory(cursor, row):
                   d = {}
                   for idx, col in enumerate(cursor.description):
                       d[col[0]] = row[idx]
                   return d

               con = sqlite3.connect(":memory:")
               con.row_factory = dict_factory
               cur = con.cursor()
               cur.execute("select 1 as a")
               print(cur.fetchone()["a"])

          If returning a tuple doesn’t suffice and you want name-based
          access to columns, you should consider setting *note
          row_factory: 16c3. to the highly-optimized *note sqlite3.Row:
          32d. type.  *note Row: 32d. provides both index-based and
          case-insensitive name-based access to columns with almost no
          memory overhead.  It will probably be better than your own
          custom dictionary-based approach or even a db_row based
          solution.

      -- Attribute: text_factory

          Using this attribute you can control what objects are returned
          for the ‘TEXT’ data type.  By default, this attribute is set
          to *note str: 25a. and the *note sqlite3: f0. module will
          return Unicode objects for ‘TEXT’.  If you want to return
          bytestrings instead, you can set it to *note bytes: 1db.

          For efficiency reasons, there’s also a way to return *note
          str: 25a. objects only for non-ASCII data, and *note bytes:
          1db. otherwise.  To activate it, set this attribute to
          ‘sqlite3.OptimizedUnicode’.

          You can also set it to any other callable that accepts a
          single bytestring parameter and returns the resulting object.

          See the following example code for illustration:

               import sqlite3

               con = sqlite3.connect(":memory:")
               cur = con.cursor()

               AUSTRIA = "\xd6sterreich"

               # by default, rows are returned as Unicode
               cur.execute("select ?", (AUSTRIA,))
               row = cur.fetchone()
               assert row[0] == AUSTRIA

               # but we can make sqlite3 always return bytestrings ...
               con.text_factory = bytes
               cur.execute("select ?", (AUSTRIA,))
               row = cur.fetchone()
               assert type(row[0]) is bytes
               # the bytestrings will be encoded in UTF-8, unless you stored garbage in the
               # database ...
               assert row[0] == AUSTRIA.encode("utf-8")

               # we can also implement a custom text_factory ...
               # here we implement one that appends "foo" to all strings
               con.text_factory = lambda x: x.decode("utf-8") + "foo"
               cur.execute("select ?", ("bar",))
               row = cur.fetchone()
               assert row[0] == "barfoo"

      -- Attribute: total_changes

          Returns the total number of database rows that have been
          modified, inserted, or deleted since the database connection
          was opened.

      -- Method: iterdump ()

          Returns an iterator to dump the database in an SQL text
          format.  Useful when saving an in-memory database for later
          restoration.  This function provides the same capabilities as
          the ‘.dump’ command in the ‘sqlite3’ shell.

          Example:

               # Convert file existing_db.db to SQL dump file dump.sql
               import sqlite3

               con = sqlite3.connect('existing_db.db')
               with open('dump.sql', 'w') as f:
                   for line in con.iterdump():
                       f.write('%s\n' % line)

   ---------- Footnotes ----------

   (1) The sqlite3 module is not built with loadable extension support
by default, because some platforms (notably Mac OS X) have SQLite
libraries which are compiled without this feature.  To get loadable
extension support, you must pass –enable-loadable-sqlite-extensions to
configure.

   (2) The sqlite3 module is not built with loadable extension support
by default, because some platforms (notably Mac OS X) have SQLite
libraries which are compiled without this feature.  To get loadable
extension support, you must pass –enable-loadable-sqlite-extensions to
configure.


File: python.info,  Node: Cursor Objects,  Next: Row Objects,  Prev: Connection Objects,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.3 Cursor Objects
.......................

 -- Class: sqlite3.Cursor

     A *note Cursor: 169d. instance has the following attributes and
     methods.

      -- Method: execute (sql[, parameters])

          Executes an SQL statement.  The SQL statement may be
          parameterized (i.  e.  placeholders instead of SQL literals).
          The *note sqlite3: f0. module supports two kinds of
          placeholders: question marks (qmark style) and named
          placeholders (named style).

          Here’s an example of both styles:

               import sqlite3

               con = sqlite3.connect(":memory:")
               cur = con.cursor()
               cur.execute("create table people (name_last, age)")

               who = "Yeltsin"
               age = 72

               # This is the qmark style:
               cur.execute("insert into people values (?, ?)", (who, age))

               # And this is the named style:
               cur.execute("select * from people where name_last=:who and age=:age", {"who": who, "age": age})

               print(cur.fetchone())

          *note execute(): 169e. will only execute a single SQL
          statement.  If you try to execute more than one statement with
          it, it will raise a Warning.  Use *note executescript(): 16bc.
          if you want to execute multiple SQL statements with one call.

      -- Method: executemany (sql, seq_of_parameters)

          Executes an SQL command against all parameter sequences or
          mappings found in the sequence `sql'.  The *note sqlite3: f0.
          module also allows using an *note iterator: e4f. yielding
          parameters instead of a sequence.

               import sqlite3

               class IterChars:
                   def __init__(self):
                       self.count = ord('a')

                   def __iter__(self):
                       return self

                   def __next__(self):
                       if self.count > ord('z'):
                           raise StopIteration
                       self.count += 1
                       return (chr(self.count - 1),) # this is a 1-tuple

               con = sqlite3.connect(":memory:")
               cur = con.cursor()
               cur.execute("create table characters(c)")

               theIter = IterChars()
               cur.executemany("insert into characters(c) values (?)", theIter)

               cur.execute("select c from characters")
               print(cur.fetchall())

          Here’s a shorter example using a *note generator: 5c0.:

               import sqlite3
               import string

               def char_generator():
                   for c in string.ascii_lowercase:
                       yield (c,)

               con = sqlite3.connect(":memory:")
               cur = con.cursor()
               cur.execute("create table characters(c)")

               cur.executemany("insert into characters(c) values (?)", char_generator())

               cur.execute("select c from characters")
               print(cur.fetchall())

      -- Method: executescript (sql_script)

          This is a nonstandard convenience method for executing
          multiple SQL statements at once.  It issues a ‘COMMIT’
          statement first, then executes the SQL script it gets as a
          parameter.

          `sql_script' can be an instance of *note str: 25a. or *note
          bytes: 1db.

          Example:

               import sqlite3

               con = sqlite3.connect(":memory:")
               cur = con.cursor()
               cur.executescript("""
                   create table person(
                       firstname,
                       lastname,
                       age
                   );

                   create table book(
                       title,
                       author,
                       published
                   );

                   insert into book(title, author, published)
                   values (
                       'Dirk Gently''s Holistic Detective Agency',
                       'Douglas Adams',
                       1987
                   );
                   """)

      -- Method: fetchone ()

          Fetches the next row of a query result set, returning a single
          sequence, or *note None: 19d. when no more data is available.

      -- Method: fetchmany (size=cursor.arraysize)

          Fetches the next set of rows of a query result, returning a
          list.  An empty list is returned when no more rows are
          available.

          The number of rows to fetch per call is specified by the
          `size' parameter.  If it is not given, the cursor’s arraysize
          determines the number of rows to be fetched.  The method
          should try to fetch as many rows as indicated by the size
          parameter.  If this is not possible due to the specified
          number of rows not being available, fewer rows may be
          returned.

          Note there are performance considerations involved with the
          `size' parameter.  For optimal performance, it is usually best
          to use the arraysize attribute.  If the `size' parameter is
          used, then it is best for it to retain the same value from one
          *note fetchmany(): 16c9. call to the next.

      -- Method: fetchall ()

          Fetches all (remaining) rows of a query result, returning a
          list.  Note that the cursor’s arraysize attribute can affect
          the performance of this operation.  An empty list is returned
          when no rows are available.

      -- Method: close ()

          Close the cursor now (rather than whenever ‘__del__’ is
          called).

          The cursor will be unusable from this point forward; a
          ‘ProgrammingError’ exception will be raised if any operation
          is attempted with the cursor.

      -- Attribute: rowcount

          Although the *note Cursor: 169d. class of the *note sqlite3:
          f0. module implements this attribute, the database engine’s
          own support for the determination of "rows affected"/"rows
          selected" is quirky.

          For *note executemany(): 16ba. statements, the number of
          modifications are summed up into *note rowcount: 16cb.

          As required by the Python DB API Spec, the *note rowcount:
          16cb. attribute "is -1 in case no ‘executeXX()’ has been
          performed on the cursor or the rowcount of the last operation
          is not determinable by the interface".  This includes ‘SELECT’
          statements because we cannot determine the number of rows a
          query produced until all rows were fetched.

          With SQLite versions before 3.6.5, *note rowcount: 16cb. is
          set to 0 if you make a ‘DELETE FROM table’ without any
          condition.

      -- Attribute: lastrowid

          This read-only attribute provides the rowid of the last
          modified row.  It is only set if you issued an ‘INSERT’
          statement using the *note execute(): 169e. method.  For
          operations other than ‘INSERT’ or when *note executemany():
          16ba. is called, *note lastrowid: 16cc. is set to *note None:
          19d.

      -- Attribute: description

          This read-only attribute provides the column names of the last
          query.  To remain compatible with the Python DB API, it
          returns a 7-tuple for each column where the last six items of
          each tuple are *note None: 19d.

          It is set for ‘SELECT’ statements without any matching rows as
          well.

      -- Attribute: connection

          This read-only attribute provides the SQLite database *note
          Connection: 6c6. used by the *note Cursor: 169d. object.  A
          *note Cursor: 169d. object created by calling *note
          con.cursor(): 16b4. will have a *note connection: 16cd.
          attribute that refers to `con':

               >>> con = sqlite3.connect(":memory:")
               >>> cur = con.cursor()
               >>> cur.connection == con
               True


File: python.info,  Node: Row Objects,  Next: SQLite and Python types,  Prev: Cursor Objects,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.4 Row Objects
....................

 -- Class: sqlite3.Row

     A *note Row: 32d. instance serves as a highly optimized *note
     row_factory: 16c3. for *note Connection: 6c6. objects.  It tries to
     mimic a tuple in most of its features.

     It supports mapping access by column name and index, iteration,
     representation, equality testing and *note len(): 5a8.

     If two *note Row: 32d. objects have exactly the same columns and
     their members are equal, they compare equal.

      -- Method: keys ()

          This method returns a list of column names.  Immediately after
          a query, it is the first member of each tuple in *note
          Cursor.description: 16a9.

     Changed in version 3.5: Added support of slicing.

Let’s assume we initialize a table as in the example given above:

     conn = sqlite3.connect(":memory:")
     c = conn.cursor()
     c.execute('''create table stocks
     (date text, trans text, symbol text,
      qty real, price real)''')
     c.execute("""insert into stocks
               values ('2006-01-05','BUY','RHAT',100,35.14)""")
     conn.commit()
     c.close()

Now we plug *note Row: 32d. in:

     >>> conn.row_factory = sqlite3.Row
     >>> c = conn.cursor()
     >>> c.execute('select * from stocks')
     <sqlite3.Cursor object at 0x7f4e7dd8fa80>
     >>> r = c.fetchone()
     >>> type(r)
     <class 'sqlite3.Row'>
     >>> tuple(r)
     ('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)
     >>> len(r)
     5
     >>> r[2]
     'RHAT'
     >>> r.keys()
     ['date', 'trans', 'symbol', 'qty', 'price']
     >>> r['qty']
     100.0
     >>> for member in r:
     ...     print(member)
     ...
     2006-01-05
     BUY
     RHAT
     100.0
     35.14


File: python.info,  Node: SQLite and Python types,  Next: Controlling Transactions,  Prev: Row Objects,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.5 SQLite and Python types
................................

* Menu:

* Introduction: Introduction<7>. 
* Using adapters to store additional Python types in SQLite databases:: 
* Converting SQLite values to custom Python types:: 
* Default adapters and converters:: 


File: python.info,  Node: Introduction<7>,  Next: Using adapters to store additional Python types in SQLite databases,  Up: SQLite and Python types

5.12.6.6 Introduction
.....................

SQLite natively supports the following types: ‘NULL’, ‘INTEGER’, ‘REAL’,
‘TEXT’, ‘BLOB’.

The following Python types can thus be sent to SQLite without any
problem:

Python type                         SQLite type
                                    
------------------------------------------------------
                                    
*note None: 19d.                    ‘NULL’
                                    
                                    
*note int: 227.                     ‘INTEGER’
                                    
                                    
*note float: 57a.                   ‘REAL’
                                    
                                    
*note str: 25a.                     ‘TEXT’
                                    
                                    
*note bytes: 1db.                   ‘BLOB’
                                    

This is how SQLite types are converted to Python types by default:

SQLite type       Python type
                  
---------------------------------------------------------------------
                  
‘NULL’            *note None: 19d.
                  
                  
‘INTEGER’         *note int: 227.
                  
                  
‘REAL’            *note float: 57a.
                  
                  
‘TEXT’            depends on *note text_factory: 16c4,
                  *note str: 25a. by default
                  
                  
‘BLOB’            *note bytes: 1db.
                  

The type system of the *note sqlite3: f0. module is extensible in two
ways: you can store additional Python types in a SQLite database via
object adaptation, and you can let the *note sqlite3: f0. module convert
SQLite types to different Python types via converters.


File: python.info,  Node: Using adapters to store additional Python types in SQLite databases,  Next: Converting SQLite values to custom Python types,  Prev: Introduction<7>,  Up: SQLite and Python types

5.12.6.7 Using adapters to store additional Python types in SQLite databases
............................................................................

As described before, SQLite supports only a limited set of types
natively.  To use other Python types with SQLite, you must `adapt' them
to one of the sqlite3 module’s supported types for SQLite: one of
NoneType, int, float, str, bytes.

There are two ways to enable the *note sqlite3: f0. module to adapt a
custom Python type to one of the supported ones.

* Menu:

* Letting your object adapt itself:: 
* Registering an adapter callable:: 


File: python.info,  Node: Letting your object adapt itself,  Next: Registering an adapter callable,  Up: Using adapters to store additional Python types in SQLite databases

5.12.6.8 Letting your object adapt itself
.........................................

This is a good approach if you write the class yourself.  Let’s suppose
you have a class like this:

     class Point:
         def __init__(self, x, y):
             self.x, self.y = x, y

Now you want to store the point in a single SQLite column.  First you’ll
have to choose one of the supported types first to be used for
representing the point.  Let’s just use str and separate the coordinates
using a semicolon.  Then you need to give your class a method
‘__conform__(self, protocol)’ which must return the converted value.
The parameter `protocol' will be ‘PrepareProtocol’.

     import sqlite3

     class Point:
         def __init__(self, x, y):
             self.x, self.y = x, y

         def __conform__(self, protocol):
             if protocol is sqlite3.PrepareProtocol:
                 return "%f;%f" % (self.x, self.y)

     con = sqlite3.connect(":memory:")
     cur = con.cursor()

     p = Point(4.0, -3.2)
     cur.execute("select ?", (p,))
     print(cur.fetchone()[0])


File: python.info,  Node: Registering an adapter callable,  Prev: Letting your object adapt itself,  Up: Using adapters to store additional Python types in SQLite databases

5.12.6.9 Registering an adapter callable
........................................

The other possibility is to create a function that converts the type to
the string representation and register the function with *note
register_adapter(): 16ad.

     import sqlite3

     class Point:
         def __init__(self, x, y):
             self.x, self.y = x, y

     def adapt_point(point):
         return "%f;%f" % (point.x, point.y)

     sqlite3.register_adapter(Point, adapt_point)

     con = sqlite3.connect(":memory:")
     cur = con.cursor()

     p = Point(4.0, -3.2)
     cur.execute("select ?", (p,))
     print(cur.fetchone()[0])

The *note sqlite3: f0. module has two default adapters for Python’s
built-in *note datetime.date: 9c0. and *note datetime.datetime: 60c.
types.  Now let’s suppose we want to store *note datetime.datetime: 60c.
objects not in ISO representation, but as a Unix timestamp.

     import sqlite3
     import datetime
     import time

     def adapt_datetime(ts):
         return time.mktime(ts.timetuple())

     sqlite3.register_adapter(datetime.datetime, adapt_datetime)

     con = sqlite3.connect(":memory:")
     cur = con.cursor()

     now = datetime.datetime.now()
     cur.execute("select ?", (now,))
     print(cur.fetchone()[0])


File: python.info,  Node: Converting SQLite values to custom Python types,  Next: Default adapters and converters,  Prev: Using adapters to store additional Python types in SQLite databases,  Up: SQLite and Python types

5.12.6.10 Converting SQLite values to custom Python types
.........................................................

Writing an adapter lets you send custom Python types to SQLite.  But to
make it really useful we need to make the Python to SQLite to Python
roundtrip work.

Enter converters.

Let’s go back to the ‘Point’ class.  We stored the x and y coordinates
separated via semicolons as strings in SQLite.

First, we’ll define a converter function that accepts the string as a
parameter and constructs a ‘Point’ object from it.

     Note: Converter functions `always' get called with a *note bytes:
     1db. object, no matter under which data type you sent the value to
     SQLite.

     def convert_point(s):
         x, y = map(float, s.split(b";"))
         return Point(x, y)

Now you need to make the *note sqlite3: f0. module know that what you
select from the database is actually a point.  There are two ways of
doing this:

   * Implicitly via the declared type

   * Explicitly via the column name

Both ways are described in section *note Module functions and constants:
16a2, in the entries for the constants *note PARSE_DECLTYPES: 16a7. and
*note PARSE_COLNAMES: 16a8.

The following example illustrates both approaches.

     import sqlite3

     class Point:
         def __init__(self, x, y):
             self.x, self.y = x, y

         def __repr__(self):
             return "(%f;%f)" % (self.x, self.y)

     def adapt_point(point):
         return ("%f;%f" % (point.x, point.y)).encode('ascii')

     def convert_point(s):
         x, y = list(map(float, s.split(b";")))
         return Point(x, y)

     # Register the adapter
     sqlite3.register_adapter(Point, adapt_point)

     # Register the converter
     sqlite3.register_converter("point", convert_point)

     p = Point(4.0, -3.2)

     #########################
     # 1) Using declared types
     con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_DECLTYPES)
     cur = con.cursor()
     cur.execute("create table test(p point)")

     cur.execute("insert into test(p) values (?)", (p,))
     cur.execute("select p from test")
     print("with declared types:", cur.fetchone()[0])
     cur.close()
     con.close()

     #######################
     # 1) Using column names
     con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_COLNAMES)
     cur = con.cursor()
     cur.execute("create table test(p)")

     cur.execute("insert into test(p) values (?)", (p,))
     cur.execute('select p as "p [point]" from test')
     print("with column names:", cur.fetchone()[0])
     cur.close()
     con.close()


File: python.info,  Node: Default adapters and converters,  Prev: Converting SQLite values to custom Python types,  Up: SQLite and Python types

5.12.6.11 Default adapters and converters
.........................................

There are default adapters for the date and datetime types in the
datetime module.  They will be sent as ISO dates/ISO timestamps to
SQLite.

The default converters are registered under the name "date" for *note
datetime.date: 9c0. and under the name "timestamp" for *note
datetime.datetime: 60c.

This way, you can use date/timestamps from Python without any additional
fiddling in most cases.  The format of the adapters is also compatible
with the experimental SQLite date/time functions.

The following example demonstrates this.

     import sqlite3
     import datetime

     con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)
     cur = con.cursor()
     cur.execute("create table test(d date, ts timestamp)")

     today = datetime.date.today()
     now = datetime.datetime.now()

     cur.execute("insert into test(d, ts) values (?, ?)", (today, now))
     cur.execute("select d, ts from test")
     row = cur.fetchone()
     print(today, "=>", row[0], type(row[0]))
     print(now, "=>", row[1], type(row[1]))

     cur.execute('select current_date as "d [date]", current_timestamp as "ts [timestamp]"')
     row = cur.fetchone()
     print("current_date", row[0], type(row[0]))
     print("current_timestamp", row[1], type(row[1]))

If a timestamp stored in SQLite has a fractional part longer than 6
numbers, its value will be truncated to microsecond precision by the
timestamp converter.


File: python.info,  Node: Controlling Transactions,  Next: Using sqlite3 efficiently,  Prev: SQLite and Python types,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.12 Controlling Transactions
..................................

By default, the *note sqlite3: f0. module opens transactions implicitly
before a Data Modification Language (DML) statement (i.e.
‘INSERT’/‘UPDATE’/‘DELETE’/‘REPLACE’), and commits transactions
implicitly before a non-DML, non-query statement (i.  e.  anything other
than ‘SELECT’ or the aforementioned).

So if you are within a transaction and issue a command like ‘CREATE
TABLE ...’, ‘VACUUM’, ‘PRAGMA’, the *note sqlite3: f0. module will
commit implicitly before executing that command.  There are two reasons
for doing that.  The first is that some of these commands don’t work
within transactions.  The other reason is that sqlite3 needs to keep
track of the transaction state (if a transaction is active or not).  The
current transaction state is exposed through the *note
Connection.in_transaction: 16b3. attribute of the connection object.

You can control which kind of ‘BEGIN’ statements sqlite3 implicitly
executes (or none at all) via the `isolation_level' parameter to the
*note connect(): 4bd. call, or via the ‘isolation_level’ property of
connections.

If you want `autocommit mode', then set ‘isolation_level’ to None.

Otherwise leave it at its default, which will result in a plain "BEGIN"
statement, or set it to one of SQLite’s supported isolation levels:
"DEFERRED", "IMMEDIATE" or "EXCLUSIVE".


File: python.info,  Node: Using sqlite3 efficiently,  Next: Common issues,  Prev: Controlling Transactions,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.13 Using ‘sqlite3’ efficiently
.....................................

* Menu:

* Using shortcut methods:: 
* Accessing columns by name instead of by index:: 
* Using the connection as a context manager:: 


File: python.info,  Node: Using shortcut methods,  Next: Accessing columns by name instead of by index,  Up: Using sqlite3 efficiently

5.12.6.14 Using shortcut methods
................................

Using the nonstandard ‘execute()’, ‘executemany()’ and ‘executescript()’
methods of the *note Connection: 6c6. object, your code can be written
more concisely because you don’t have to create the (often superfluous)
*note Cursor: 169d. objects explicitly.  Instead, the *note Cursor:
169d. objects are created implicitly and these shortcut methods return
the cursor objects.  This way, you can execute a ‘SELECT’ statement and
iterate over it directly using only a single call on the *note
Connection: 6c6. object.

     import sqlite3

     persons = [
         ("Hugo", "Boss"),
         ("Calvin", "Klein")
         ]

     con = sqlite3.connect(":memory:")

     # Create the table
     con.execute("create table person(firstname, lastname)")

     # Fill the table
     con.executemany("insert into person(firstname, lastname) values (?, ?)", persons)

     # Print the table contents
     for row in con.execute("select firstname, lastname from person"):
         print(row)

     print("I just deleted", con.execute("delete from person").rowcount, "rows")


File: python.info,  Node: Accessing columns by name instead of by index,  Next: Using the connection as a context manager,  Prev: Using shortcut methods,  Up: Using sqlite3 efficiently

5.12.6.15 Accessing columns by name instead of by index
.......................................................

One useful feature of the *note sqlite3: f0. module is the built-in
*note sqlite3.Row: 32d. class designed to be used as a row factory.

Rows wrapped with this class can be accessed both by index (like tuples)
and case-insensitively by name:

     import sqlite3

     con = sqlite3.connect(":memory:")
     con.row_factory = sqlite3.Row

     cur = con.cursor()
     cur.execute("select 'John' as name, 42 as age")
     for row in cur:
         assert row[0] == row["name"]
         assert row["name"] == row["nAmE"]
         assert row[1] == row["age"]
         assert row[1] == row["AgE"]


File: python.info,  Node: Using the connection as a context manager,  Prev: Accessing columns by name instead of by index,  Up: Using sqlite3 efficiently

5.12.6.16 Using the connection as a context manager
...................................................

Connection objects can be used as context managers that automatically
commit or rollback transactions.  In the event of an exception, the
transaction is rolled back; otherwise, the transaction is committed:

     import sqlite3

     con = sqlite3.connect(":memory:")
     con.execute("create table person (id integer primary key, firstname varchar unique)")

     # Successful, con.commit() is called automatically afterwards
     with con:
         con.execute("insert into person(firstname) values (?)", ("Joe",))

     # con.rollback() is called after the with block finishes with an exception, the
     # exception is still raised and must be caught
     try:
         with con:
             con.execute("insert into person(firstname) values (?)", ("Joe",))
     except sqlite3.IntegrityError:
         print("couldn't add Joe twice")


File: python.info,  Node: Common issues,  Prev: Using sqlite3 efficiently,  Up: sqlite3 --- DB-API 2 0 interface for SQLite databases

5.12.6.17 Common issues
.......................

* Menu:

* Multithreading:: 


File: python.info,  Node: Multithreading,  Up: Common issues

5.12.6.18 Multithreading
........................

Older SQLite versions had issues with sharing connections between
threads.  That’s why the Python module disallows sharing connections and
cursors between threads.  If you still try to do so, you will get an
exception at runtime.

The only exception is calling the *note interrupt(): 16c0. method, which
only makes sense to call from a different thread.


File: python.info,  Node: Data Compression and Archiving,  Next: File Formats,  Prev: Data Persistence,  Up: The Python Standard Library

5.13 Data Compression and Archiving
===================================

The modules described in this chapter support data compression with the
zlib, gzip, bzip2 and lzma algorithms, and the creation of ZIP- and
tar-format archives.  See also *note Archiving operations: 7f1. provided
by the *note shutil: e7. module.

* Menu:

* zlib: zlib --- Compression compatible with gzip. Compression compatible with gzip
* gzip: gzip --- Support for gzip files. Support for gzip files
* bz2: bz2 --- Support for bzip2 compression. Support for bzip2 compression
* lzma: lzma --- Compression using the LZMA algorithm. Compression using the LZMA algorithm
* zipfile: zipfile --- Work with ZIP archives. Work with ZIP archives
* tarfile: tarfile --- Read and write tar archive files. Read and write tar archive files


File: python.info,  Node: zlib --- Compression compatible with gzip,  Next: gzip --- Support for gzip files,  Up: Data Compression and Archiving

5.13.1 ‘zlib’ — Compression compatible with ‘gzip’
--------------------------------------------------

For applications that require data compression, the functions in this
module allow compression and decompression, using the zlib library.  The
zlib library has its own home page at ‘http://www.zlib.net’.  There are
known incompatibilities between the Python module and versions of the
zlib library earlier than 1.1.3; 1.1.3 has a security vulnerability, so
we recommend using 1.1.4 or later.

zlib’s functions have many options and often need to be used in a
particular order.  This documentation doesn’t attempt to cover all of
the permutations; consult the zlib manual at
‘http://www.zlib.net/manual.html’ for authoritative information.

For reading and writing ‘.gz’ files see the *note gzip: 8b. module.

The available exception and functions in this module are:

 -- Exception: zlib.error

     Exception raised on compression and decompression errors.

 -- Function: zlib.adler32 (data[, value])

     Computes an Adler-32 checksum of `data'.  (An Adler-32 checksum is
     almost as reliable as a CRC32 but can be computed much more
     quickly.)  The result is an unsigned 32-bit integer.  If `value' is
     present, it is used as the starting value of the checksum;
     otherwise, a default value of 1 is used.  Passing in `value' allows
     computing a running checksum over the concatenation of several
     inputs.  The algorithm is not cryptographically strong, and should
     not be used for authentication or digital signatures.  Since the
     algorithm is designed for use as a checksum algorithm, it is not
     suitable for use as a general hash algorithm.

     Changed in version 3.0: Always returns an unsigned value.  To
     generate the same numeric value across all Python versions and
     platforms, use ‘adler32(data) & 0xffffffff’.

 -- Function: zlib.compress (data, level=-1)

     Compresses the bytes in `data', returning a bytes object containing
     compressed data.  `level' is an integer from ‘0’ to ‘9’ or ‘-1’
     controlling the level of compression; ‘1’ is fastest and produces
     the least compression, ‘9’ is slowest and produces the most.  ‘0’
     is no compression.  The default value is ‘-1’
     (Z_DEFAULT_COMPRESSION). Z_DEFAULT_COMPRESSION represents a default
     compromise between speed and compression (currently equivalent to
     level 6).  Raises the *note error: 16e3. exception if any error
     occurs.

     Changed in version 3.6: Keyword arguments are now supported.

 -- Function: zlib.compressobj (level=-1, method=DEFLATED, wbits=15,
          memLevel=8, strategy=Z_DEFAULT_STRATEGY[, zdict])

     Returns a compression object, to be used for compressing data
     streams that won’t fit into memory at once.

     `level' is the compression level – an integer from ‘0’ to ‘9’ or
     ‘-1’.  A value of ‘1’ is fastest and produces the least
     compression, while a value of ‘9’ is slowest and produces the most.
     ‘0’ is no compression.  The default value is ‘-1’
     (Z_DEFAULT_COMPRESSION). Z_DEFAULT_COMPRESSION represents a default
     compromise between speed and compression (currently equivalent to
     level 6).

     `method' is the compression algorithm.  Currently, the only
     supported value is ‘DEFLATED’.

     `wbits' is the base two logarithm of the size of the window buffer.
     This should be an integer from ‘8’ to ‘15’.  Higher values give
     better compression, but use more memory.

     The `memLevel' argument controls the amount of memory used for the
     internal compression state.  Valid values range from ‘1’ to ‘9’.
     Higher values use more memory, but are faster and produce smaller
     output.

     `strategy' is used to tune the compression algorithm.  Possible
     values are ‘Z_DEFAULT_STRATEGY’, ‘Z_FILTERED’, and
     ‘Z_HUFFMAN_ONLY’.

     `zdict' is a predefined compression dictionary.  This is a sequence
     of bytes (such as a *note bytes: 1db. object) containing
     subsequences that are expected to occur frequently in the data that
     is to be compressed.  Those subsequences that are expected to be
     most common should come at the end of the dictionary.

     Changed in version 3.3: Added the `zdict' parameter and keyword
     argument support.

 -- Function: zlib.crc32 (data[, value])

     Computes a CRC (Cyclic Redundancy Check) checksum of `data'.  The
     result is an unsigned 32-bit integer.  If `value' is present, it is
     used as the starting value of the checksum; otherwise, a default
     value of 0 is used.  Passing in `value' allows computing a running
     checksum over the concatenation of several inputs.  The algorithm
     is not cryptographically strong, and should not be used for
     authentication or digital signatures.  Since the algorithm is
     designed for use as a checksum algorithm, it is not suitable for
     use as a general hash algorithm.

     Changed in version 3.0: Always returns an unsigned value.  To
     generate the same numeric value across all Python versions and
     platforms, use ‘crc32(data) & 0xffffffff’.

 -- Function: zlib.decompress (data[, wbits[, bufsize]])

     Decompresses the bytes in `data', returning a bytes object
     containing the uncompressed data.  The `wbits' parameter controls
     the size of the window buffer, and is discussed further below.  If
     `bufsize' is given, it is used as the initial size of the output
     buffer.  Raises the *note error: 16e3. exception if any error
     occurs.

     The absolute value of `wbits' is the base two logarithm of the size
     of the history buffer (the "window size") used when compressing
     data.  Its absolute value should be between 8 and 15 for the most
     recent versions of the zlib library, larger values resulting in
     better compression at the expense of greater memory usage.  When
     decompressing a stream, `wbits' must not be smaller than the size
     originally used to compress the stream; using a too-small value
     will result in an exception.  The default value is therefore the
     highest value, 15.  When `wbits' is negative, the standard ‘gzip’
     header is suppressed.

     `bufsize' is the initial size of the buffer used to hold
     decompressed data.  If more space is required, the buffer size will
     be increased as needed, so you don’t have to get this value exactly
     right; tuning it will only save a few calls to ‘malloc()’.  The
     default size is 16384.

 -- Function: zlib.decompressobj (wbits=15[, zdict])

     Returns a decompression object, to be used for decompressing data
     streams that won’t fit into memory at once.

     The `wbits' parameter controls the size of the window buffer.

     The `zdict' parameter specifies a predefined compression
     dictionary.  If provided, this must be the same dictionary as was
     used by the compressor that produced the data that is to be
     decompressed.

          Note: If `zdict' is a mutable object (such as a *note
          bytearray: 1dc.), you must not modify its contents between the
          call to *note decompressobj(): 16e7. and the first call to the
          decompressor’s ‘decompress()’ method.

     Changed in version 3.3: Added the `zdict' parameter.

Compression objects support the following methods:

 -- Method: Compress.compress (data)

     Compress `data', returning a bytes object containing compressed
     data for at least part of the data in `data'.  This data should be
     concatenated to the output produced by any preceding calls to the
     *note compress(): 17d. method.  Some input may be kept in internal
     buffers for later processing.

 -- Method: Compress.flush ([mode])

     All pending input is processed, and a bytes object containing the
     remaining compressed output is returned.  `mode' can be selected
     from the constants ‘Z_SYNC_FLUSH’, ‘Z_FULL_FLUSH’, or ‘Z_FINISH’,
     defaulting to ‘Z_FINISH’.  ‘Z_SYNC_FLUSH’ and ‘Z_FULL_FLUSH’ allow
     compressing further bytestrings of data, while ‘Z_FINISH’ finishes
     the compressed stream and prevents compressing any more data.
     After calling *note flush(): 16e9. with `mode' set to ‘Z_FINISH’,
     the *note compress(): 17d. method cannot be called again; the only
     realistic action is to delete the object.

 -- Method: Compress.copy ()

     Returns a copy of the compression object.  This can be used to
     efficiently compress a set of data that share a common initial
     prefix.

Decompression objects support the following methods and attributes:

 -- Attribute: Decompress.unused_data

     A bytes object which contains any bytes past the end of the
     compressed data.  That is, this remains ‘b""’ until the last byte
     that contains compression data is available.  If the whole
     bytestring turned out to contain compressed data, this is ‘b""’, an
     empty bytes object.

 -- Attribute: Decompress.unconsumed_tail

     A bytes object that contains any data that was not consumed by the
     last *note decompress(): 122e. call because it exceeded the limit
     for the uncompressed data buffer.  This data has not yet been seen
     by the zlib machinery, so you must feed it (possibly with further
     data concatenated to it) back to a subsequent *note decompress():
     122e. method call in order to get correct output.

 -- Attribute: Decompress.eof

     A boolean indicating whether the end of the compressed data stream
     has been reached.

     This makes it possible to distinguish between a properly-formed
     compressed stream, and an incomplete or truncated one.

     New in version 3.3.

 -- Method: Decompress.decompress (data[, max_length])

     Decompress `data', returning a bytes object containing the
     uncompressed data corresponding to at least part of the data in
     `string'.  This data should be concatenated to the output produced
     by any preceding calls to the *note decompress(): 122e. method.
     Some of the input data may be preserved in internal buffers for
     later processing.

     If the optional parameter `max_length' is non-zero then the return
     value will be no longer than `max_length'.  This may mean that not
     all of the compressed input can be processed; and unconsumed data
     will be stored in the attribute *note unconsumed_tail: 16ec.  This
     bytestring must be passed to a subsequent call to *note
     decompress(): 122e. if decompression is to continue.  If
     `max_length' is not supplied then the whole input is decompressed,
     and *note unconsumed_tail: 16ec. is empty.

 -- Method: Decompress.flush ([length])

     All pending input is processed, and a bytes object containing the
     remaining uncompressed output is returned.  After calling *note
     flush(): 16ee, the *note decompress(): 122e. method cannot be
     called again; the only realistic action is to delete the object.

     The optional parameter `length' sets the initial size of the output
     buffer.

 -- Method: Decompress.copy ()

     Returns a copy of the decompression object.  This can be used to
     save the state of the decompressor midway through the data stream
     in order to speed up random seeks into the stream at a future
     point.

Information about the version of the zlib library in use is available
through the following constants:

 -- Data: zlib.ZLIB_VERSION

     The version string of the zlib library that was used for building
     the module.  This may be different from the zlib library actually
     used at runtime, which is available as *note ZLIB_RUNTIME_VERSION:
     703.

 -- Data: zlib.ZLIB_RUNTIME_VERSION

     The version string of the zlib library actually loaded by the
     interpreter.

     New in version 3.3.

See also
........

Module *note gzip: 8b.

     Reading and writing ‘gzip’-format files.

‘http://www.zlib.net’

     The zlib library home page.

‘http://www.zlib.net/manual.html’

     The zlib manual explains the semantics and usage of the library’s
     many functions.


File: python.info,  Node: gzip --- Support for gzip files,  Next: bz2 --- Support for bzip2 compression,  Prev: zlib --- Compression compatible with gzip,  Up: Data Compression and Archiving

5.13.2 ‘gzip’ — Support for ‘gzip’ files
----------------------------------------

`Source code:' Lib/gzip.py(1)

__________________________________________________________________

This module provides a simple interface to compress and decompress files
just like the GNU programs ‘gzip’ and ‘gunzip’ would.

The data compression is provided by the *note zlib: 141. module.

The *note gzip: 8b. module provides the *note GzipFile: 290. class, as
well as the *note open(): 16f3, *note compress(): 7dd. and *note
decompress(): 7de. convenience functions.  The *note GzipFile: 290.
class reads and writes ‘gzip’-format files, automatically compressing or
decompressing the data so that it looks like an ordinary *note file
object: 78b.

Note that additional file formats which can be decompressed by the
‘gzip’ and ‘gunzip’ programs, such as those produced by ‘compress’ and
‘pack’, are not supported by this module.

The module defines the following items:

 -- Function: gzip.open (filename, mode='rb', compresslevel=9,
          encoding=None, errors=None, newline=None)

     Open a gzip-compressed file in binary or text mode, returning a
     *note file object: 78b.

     The `filename' argument can be an actual filename (a *note str:
     25a. or *note bytes: 1db. object), or an existing file object to
     read from or write to.

     The `mode' argument can be any of ‘'r'’, ‘'rb'’, ‘'a'’, ‘'ab'’,
     ‘'w'’, ‘'wb'’, ‘'x'’ or ‘'xb'’ for binary mode, or ‘'rt'’, ‘'at'’,
     ‘'wt'’, or ‘'xt'’ for text mode.  The default is ‘'rb'’.

     The `compresslevel' argument is an integer from 0 to 9, as for the
     *note GzipFile: 290. constructor.

     For binary mode, this function is equivalent to the *note GzipFile:
     290. constructor: ‘GzipFile(filename, mode, compresslevel)’.  In
     this case, the `encoding', `errors' and `newline' arguments must
     not be provided.

     For text mode, a *note GzipFile: 290. object is created, and
     wrapped in an *note io.TextIOWrapper: 557. instance with the
     specified encoding, error handling behavior, and line ending(s).

     Changed in version 3.3: Added support for `filename' being a file
     object, support for text mode, and the `encoding', `errors' and
     `newline' arguments.

     Changed in version 3.4: Added support for the ‘'x'’, ‘'xb'’ and
     ‘'xt'’ modes.

 -- Class: gzip.GzipFile (filename=None, mode=None, compresslevel=9,
          fileobj=None, mtime=None)

     Constructor for the *note GzipFile: 290. class, which simulates
     most of the methods of a *note file object: 78b, with the exception
     of the ‘truncate()’ method.  At least one of `fileobj' and
     `filename' must be given a non-trivial value.

     The new class instance is based on `fileobj', which can be a
     regular file, an *note io.BytesIO: 371. object, or any other object
     which simulates a file.  It defaults to ‘None’, in which case
     `filename' is opened to provide a file object.

     When `fileobj' is not ‘None’, the `filename' argument is only used
     to be included in the ‘gzip’ file header, which may include the
     original filename of the uncompressed file.  It defaults to the
     filename of `fileobj', if discernible; otherwise, it defaults to
     the empty string, and in this case the original filename is not
     included in the header.

     The `mode' argument can be any of ‘'r'’, ‘'rb'’, ‘'a'’, ‘'ab'’,
     ‘'w'’, ‘'wb'’, ‘'x'’, or ‘'xb'’, depending on whether the file will
     be read or written.  The default is the mode of `fileobj' if
     discernible; otherwise, the default is ‘'rb'’.

     Note that the file is always opened in binary mode.  To open a
     compressed file in text mode, use *note open(): 16f3. (or wrap your
     *note GzipFile: 290. with an *note io.TextIOWrapper: 557.).

     The `compresslevel' argument is an integer from ‘0’ to ‘9’
     controlling the level of compression; ‘1’ is fastest and produces
     the least compression, and ‘9’ is slowest and produces the most
     compression.  ‘0’ is no compression.  The default is ‘9’.

     The `mtime' argument is an optional numeric timestamp to be written
     to the last modification time field in the stream when compressing.
     It should only be provided in compression mode.  If omitted or
     ‘None’, the current time is used.  See the *note mtime: 16f4.
     attribute for more details.

     Calling a *note GzipFile: 290. object’s ‘close()’ method does not
     close `fileobj', since you might wish to append more material after
     the compressed data.  This also allows you to pass an *note
     io.BytesIO: 371. object opened for writing as `fileobj', and
     retrieve the resulting memory buffer using the *note io.BytesIO:
     371. object’s *note getvalue(): 16f5. method.

     *note GzipFile: 290. supports the *note io.BufferedIOBase: 5fe.
     interface, including iteration and the *note with: 29d. statement.
     Only the ‘truncate()’ method isn’t implemented.

     *note GzipFile: 290. also provides the following method and
     attribute:

      -- Method: peek (n)

          Read `n' uncompressed bytes without advancing the file
          position.  At most one single read on the compressed stream is
          done to satisfy the call.  The number of bytes returned may be
          more or less than requested.

               Note: While calling *note peek(): 7dc. does not change
               the file position of the *note GzipFile: 290, it may
               change the position of the underlying file object (e.g.
               if the *note GzipFile: 290. was constructed with the
               `fileobj' parameter).

          New in version 3.2.

      -- Attribute: mtime

          When decompressing, the value of the last modification time
          field in the most recently read header may be read from this
          attribute, as an integer.  The initial value before reading
          any headers is ‘None’.

          All ‘gzip’ compressed streams are required to contain this
          timestamp field.  Some programs, such as ‘gunzip’, make use of
          the timestamp.  The format is the same as the return value of
          *note time.time(): 6a5. and the *note st_mtime: 16f6.
          attribute of the object returned by *note os.stat(): 1e2.

     Changed in version 3.1: Support for the *note with: 29d. statement
     was added, along with the `mtime' constructor argument and *note
     mtime: 16f4. attribute.

     Changed in version 3.2: Support for zero-padded and unseekable
     files was added.

     Changed in version 3.3: The *note io.BufferedIOBase.read1(): 16f7.
     method is now implemented.

     Changed in version 3.4: Added support for the ‘'x'’ and ‘'xb'’
     modes.

     Changed in version 3.5: Added support for writing arbitrary *note
     bytes-like objects: 36b.  The *note read(): 16f8. method now
     accepts an argument of ‘None’.

 -- Function: gzip.compress (data, compresslevel=9)

     Compress the `data', returning a *note bytes: 1db. object
     containing the compressed data.  `compresslevel' has the same
     meaning as in the *note GzipFile: 290. constructor above.

     New in version 3.2.

 -- Function: gzip.decompress (data)

     Decompress the `data', returning a *note bytes: 1db. object
     containing the uncompressed data.

     New in version 3.2.

* Menu:

* Examples of usage:: 

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/gzip.py


File: python.info,  Node: Examples of usage,  Up: gzip --- Support for gzip files

5.13.2.1 Examples of usage
..........................

Example of how to read a compressed file:

     import gzip
     with gzip.open('/home/joe/file.txt.gz', 'rb') as f:
         file_content = f.read()

Example of how to create a compressed GZIP file:

     import gzip
     content = b"Lots of content here"
     with gzip.open('/home/joe/file.txt.gz', 'wb') as f:
         f.write(content)

Example of how to GZIP compress an existing file:

     import gzip
     import shutil
     with open('/home/joe/file.txt', 'rb') as f_in:
         with gzip.open('/home/joe/file.txt.gz', 'wb') as f_out:
             shutil.copyfileobj(f_in, f_out)

Example of how to GZIP compress a binary string:

     import gzip
     s_in = b"Lots of content here"
     s_out = gzip.compress(s_in)

See also
........

Module *note zlib: 141.

     The basic data compression module needed to support the ‘gzip’ file
     format.


File: python.info,  Node: bz2 --- Support for bzip2 compression,  Next: lzma --- Compression using the LZMA algorithm,  Prev: gzip --- Support for gzip files,  Up: Data Compression and Archiving

5.13.3 ‘bz2’ — Support for ‘bzip2’ compression
----------------------------------------------

This module provides a comprehensive interface for compressing and
decompressing data using the bzip2 compression algorithm.

The *note bz2: 14. module contains:

   * The *note open(): 5fc. function and *note BZ2File: 539. class for
     reading and writing compressed files.

   * The *note BZ2Compressor: 16fd. and *note BZ2Decompressor: 16fe.
     classes for incremental (de)compression.

   * The *note compress(): 1227. and *note decompress(): 5fd. functions
     for one-shot (de)compression.

All of the classes in this module may safely be accessed from multiple
threads.

* Menu:

* (De)compression of files: De compression of files. 
* Incremental (de)compression: Incremental de compression. 
* One-shot (de)compression: One-shot de compression. 


File: python.info,  Node: De compression of files,  Next: Incremental de compression,  Up: bz2 --- Support for bzip2 compression

5.13.3.1 (De)compression of files
.................................

 -- Function: bz2.open (filename, mode='r', compresslevel=9,
          encoding=None, errors=None, newline=None)

     Open a bzip2-compressed file in binary or text mode, returning a
     *note file object: 78b.

     As with the constructor for *note BZ2File: 539, the `filename'
     argument can be an actual filename (a *note str: 25a. or *note
     bytes: 1db. object), or an existing file object to read from or
     write to.

     The `mode' argument can be any of ‘'r'’, ‘'rb'’, ‘'w'’, ‘'wb'’,
     ‘'x'’, ‘'xb'’, ‘'a'’ or ‘'ab'’ for binary mode, or ‘'rt'’, ‘'wt'’,
     ‘'xt'’, or ‘'at'’ for text mode.  The default is ‘'rb'’.

     The `compresslevel' argument is an integer from 1 to 9, as for the
     *note BZ2File: 539. constructor.

     For binary mode, this function is equivalent to the *note BZ2File:
     539. constructor: ‘BZ2File(filename, mode,
     compresslevel=compresslevel)’.  In this case, the `encoding',
     `errors' and `newline' arguments must not be provided.

     For text mode, a *note BZ2File: 539. object is created, and wrapped
     in an *note io.TextIOWrapper: 557. instance with the specified
     encoding, error handling behavior, and line ending(s).

     New in version 3.3.

     Changed in version 3.4: The ‘'x'’ (exclusive creation) mode was
     added.

 -- Class: bz2.BZ2File (filename, mode='r', buffering=None,
          compresslevel=9)

     Open a bzip2-compressed file in binary mode.

     If `filename' is a *note str: 25a. or *note bytes: 1db. object,
     open the named file directly.  Otherwise, `filename' should be a
     *note file object: 78b, which will be used to read or write the
     compressed data.

     The `mode' argument can be either ‘'r'’ for reading (default),
     ‘'w'’ for overwriting, ‘'x'’ for exclusive creation, or ‘'a'’ for
     appending.  These can equivalently be given as ‘'rb'’, ‘'wb'’,
     ‘'xb'’ and ‘'ab'’ respectively.

     If `filename' is a file object (rather than an actual file name), a
     mode of ‘'w'’ does not truncate the file, and is instead equivalent
     to ‘'a'’.

     The `buffering' argument is ignored.  Its use is deprecated.

     If `mode' is ‘'w'’ or ‘'a'’, `compresslevel' can be a number
     between ‘1’ and ‘9’ specifying the level of compression: ‘1’
     produces the least compression, and ‘9’ (default) produces the most
     compression.

     If `mode' is ‘'r'’, the input file may be the concatenation of
     multiple compressed streams.

     *note BZ2File: 539. provides all of the members specified by the
     *note io.BufferedIOBase: 5fe, except for ‘detach()’ and
     ‘truncate()’.  Iteration and the *note with: 29d. statement are
     supported.

     *note BZ2File: 539. also provides the following method:

      -- Method: peek ([n])

          Return buffered data without advancing the file position.  At
          least one byte of data will be returned (unless at EOF). The
          exact number of bytes returned is unspecified.

               Note: While calling *note peek(): 1700. does not change
               the file position of the *note BZ2File: 539, it may
               change the position of the underlying file object (e.g.
               if the *note BZ2File: 539. was constructed by passing a
               file object for `filename').

          New in version 3.3.

     Changed in version 3.1: Support for the *note with: 29d. statement
     was added.

     Changed in version 3.3: The ‘fileno()’, ‘readable()’, ‘seekable()’,
     ‘writable()’, ‘read1()’ and ‘readinto()’ methods were added.

     Changed in version 3.3: Support was added for `filename' being a
     *note file object: 78b. instead of an actual filename.

     Changed in version 3.3: The ‘'a'’ (append) mode was added, along
     with support for reading multi-stream files.

     Changed in version 3.4: The ‘'x'’ (exclusive creation) mode was
     added.

     Changed in version 3.5: The *note read(): 16f8. method now accepts
     an argument of ‘None’.


File: python.info,  Node: Incremental de compression,  Next: One-shot de compression,  Prev: De compression of files,  Up: bz2 --- Support for bzip2 compression

5.13.3.2 Incremental (de)compression
....................................

 -- Class: bz2.BZ2Compressor (compresslevel=9)

     Create a new compressor object.  This object may be used to
     compress data incrementally.  For one-shot compression, use the
     *note compress(): 1227. function instead.

     `compresslevel', if given, must be a number between ‘1’ and ‘9’.
     The default is ‘9’.

      -- Method: compress (data)

          Provide data to the compressor object.  Returns a chunk of
          compressed data if possible, or an empty byte string
          otherwise.

          When you have finished providing data to the compressor, call
          the *note flush(): 1703. method to finish the compression
          process.

      -- Method: flush ()

          Finish the compression process.  Returns the compressed data
          left in internal buffers.

          The compressor object may not be used after this method has
          been called.

 -- Class: bz2.BZ2Decompressor

     Create a new decompressor object.  This object may be used to
     decompress data incrementally.  For one-shot compression, use the
     *note decompress(): 5fd. function instead.

          Note: This class does not transparently handle inputs
          containing multiple compressed streams, unlike *note
          decompress(): 5fd. and *note BZ2File: 539.  If you need to
          decompress a multi-stream input with *note BZ2Decompressor:
          16fe, you must use a new decompressor for each stream.

      -- Method: decompress (data, max_length=-1)

          Decompress `data' (a *note bytes-like object: 36b.), returning
          uncompressed data as bytes.  Some of `data' may be buffered
          internally, for use in later calls to *note decompress(): 5fd.
          The returned data should be concatenated with the output of
          any previous calls to *note decompress(): 5fd.

          If `max_length' is nonnegative, returns at most `max_length'
          bytes of decompressed data.  If this limit is reached and
          further output can be produced, the *note needs_input: 1704.
          attribute will be set to ‘False’.  In this case, the next call
          to *note decompress(): 247. may provide `data' as ‘b''’ to
          obtain more of the output.

          If all of the input data was decompressed and returned (either
          because this was less than `max_length' bytes, or because
          `max_length' was negative), the *note needs_input: 1704.
          attribute will be set to ‘True’.

          Attempting to decompress data after the end of stream is
          reached raises an ‘EOFError’.  Any data found after the end of
          the stream is ignored and saved in the *note unused_data:
          1705. attribute.

          Changed in version 3.5: Added the `max_length' parameter.

      -- Attribute: eof

          ‘True’ if the end-of-stream marker has been reached.

          New in version 3.3.

      -- Attribute: unused_data

          Data found after the end of the compressed stream.

          If this attribute is accessed before the end of the stream has
          been reached, its value will be ‘b''’.

      -- Attribute: needs_input

          ‘False’ if the *note decompress(): 247. method can provide
          more decompressed data before requiring new uncompressed
          input.

          New in version 3.5.


File: python.info,  Node: One-shot de compression,  Prev: Incremental de compression,  Up: bz2 --- Support for bzip2 compression

5.13.3.3 One-shot (de)compression
.................................

 -- Function: bz2.compress (data, compresslevel=9)

     Compress `data'.

     `compresslevel', if given, must be a number between ‘1’ and ‘9’.
     The default is ‘9’.

     For incremental compression, use a *note BZ2Compressor: 16fd.
     instead.

 -- Function: bz2.decompress (data)

     Decompress `data'.

     If `data' is the concatenation of multiple compressed streams,
     decompress all of the streams.

     For incremental decompression, use a *note BZ2Decompressor: 16fe.
     instead.

     Changed in version 3.3: Support for multi-stream inputs was added.


File: python.info,  Node: lzma --- Compression using the LZMA algorithm,  Next: zipfile --- Work with ZIP archives,  Prev: bz2 --- Support for bzip2 compression,  Up: Data Compression and Archiving

5.13.4 ‘lzma’ — Compression using the LZMA algorithm
----------------------------------------------------

New in version 3.3.

This module provides classes and convenience functions for compressing
and decompressing data using the LZMA compression algorithm.  Also
included is a file interface supporting the ‘.xz’ and legacy ‘.lzma’
file formats used by the ‘xz’ utility, as well as raw compressed
streams.

The interface provided by this module is very similar to that of the
*note bz2: 14. module.  However, note that *note LZMAFile: 53a. is `not'
thread-safe, unlike *note bz2.BZ2File: 539, so if you need to use a
single *note LZMAFile: 53a. instance from multiple threads, it is
necessary to protect it with a lock.

 -- Exception: lzma.LZMAError

     This exception is raised when an error occurs during compression or
     decompression, or while initializing the compressor/decompressor
     state.

* Menu:

* Reading and writing compressed files:: 
* Compressing and decompressing data in memory:: 
* Miscellaneous: Miscellaneous<2>. 
* Specifying custom filter chains:: 
* Examples: Examples<5>. 


File: python.info,  Node: Reading and writing compressed files,  Next: Compressing and decompressing data in memory,  Up: lzma --- Compression using the LZMA algorithm

5.13.4.1 Reading and writing compressed files
.............................................

 -- Function: lzma.open (filename, mode="rb", *, format=None, check=-1,
          preset=None, filters=None, encoding=None, errors=None,
          newline=None)

     Open an LZMA-compressed file in binary or text mode, returning a
     *note file object: 78b.

     The `filename' argument can be either an actual file name (given as
     a *note str: 25a. or *note bytes: 1db. object), in which case the
     named file is opened, or it can be an existing file object to read
     from or write to.

     The `mode' argument can be any of ‘"r"’, ‘"rb"’, ‘"w"’, ‘"wb"’,
     ‘"x"’, ‘"xb"’, ‘"a"’ or ‘"ab"’ for binary mode, or ‘"rt"’, ‘"wt"’,
     ‘"xt"’, or ‘"at"’ for text mode.  The default is ‘"rb"’.

     When opening a file for reading, the `format' and `filters'
     arguments have the same meanings as for *note LZMADecompressor:
     170d.  In this case, the `check' and `preset' arguments should not
     be used.

     When opening a file for writing, the `format', `check', `preset'
     and `filters' arguments have the same meanings as for *note
     LZMACompressor: 170e.

     For binary mode, this function is equivalent to the *note LZMAFile:
     53a. constructor: ‘LZMAFile(filename, mode, ...)’.  In this case,
     the `encoding', `errors' and `newline' arguments must not be
     provided.

     For text mode, a *note LZMAFile: 53a. object is created, and
     wrapped in an *note io.TextIOWrapper: 557. instance with the
     specified encoding, error handling behavior, and line ending(s).

     Changed in version 3.4: Added support for the ‘"x"’, ‘"xb"’ and
     ‘"xt"’ modes.

 -- Class: lzma.LZMAFile (filename=None, mode="r", *, format=None,
          check=-1, preset=None, filters=None)

     Open an LZMA-compressed file in binary mode.

     An *note LZMAFile: 53a. can wrap an already-open *note file object:
     78b, or operate directly on a named file.  The `filename' argument
     specifies either the file object to wrap, or the name of the file
     to open (as a *note str: 25a. or *note bytes: 1db. object).  When
     wrapping an existing file object, the wrapped file will not be
     closed when the *note LZMAFile: 53a. is closed.

     The `mode' argument can be either ‘"r"’ for reading (default),
     ‘"w"’ for overwriting, ‘"x"’ for exclusive creation, or ‘"a"’ for
     appending.  These can equivalently be given as ‘"rb"’, ‘"wb"’,
     ‘"xb"’ and ‘"ab"’ respectively.

     If `filename' is a file object (rather than an actual file name), a
     mode of ‘"w"’ does not truncate the file, and is instead equivalent
     to ‘"a"’.

     When opening a file for reading, the input file may be the
     concatenation of multiple separate compressed streams.  These are
     transparently decoded as a single logical stream.

     When opening a file for reading, the `format' and `filters'
     arguments have the same meanings as for *note LZMADecompressor:
     170d.  In this case, the `check' and `preset' arguments should not
     be used.

     When opening a file for writing, the `format', `check', `preset'
     and `filters' arguments have the same meanings as for *note
     LZMACompressor: 170e.

     *note LZMAFile: 53a. supports all the members specified by *note
     io.BufferedIOBase: 5fe, except for ‘detach()’ and ‘truncate()’.
     Iteration and the *note with: 29d. statement are supported.

     The following method is also provided:

      -- Method: peek (size=-1)

          Return buffered data without advancing the file position.  At
          least one byte of data will be returned, unless EOF has been
          reached.  The exact number of bytes returned is unspecified
          (the `size' argument is ignored).

               Note: While calling *note peek(): 170f. does not change
               the file position of the *note LZMAFile: 53a, it may
               change the position of the underlying file object (e.g.
               if the *note LZMAFile: 53a. was constructed by passing a
               file object for `filename').

     Changed in version 3.4: Added support for the ‘"x"’ and ‘"xb"’
     modes.

     Changed in version 3.5: The *note read(): 16f8. method now accepts
     an argument of ‘None’.


File: python.info,  Node: Compressing and decompressing data in memory,  Next: Miscellaneous<2>,  Prev: Reading and writing compressed files,  Up: lzma --- Compression using the LZMA algorithm

5.13.4.2 Compressing and decompressing data in memory
.....................................................

 -- Class: lzma.LZMACompressor (format=FORMAT_XZ, check=-1, preset=None,
          filters=None)

     Create a compressor object, which can be used to compress data
     incrementally.

     For a more convenient way of compressing a single chunk of data,
     see *note compress(): 1711.

     The `format' argument specifies what container format should be
     used.  Possible values are:

        * 
          ‘FORMAT_XZ’: The ‘.xz’ container format.

               This is the default format.

        * 
          ‘FORMAT_ALONE’: The legacy ‘.lzma’ container format.

               This format is more limited than ‘.xz’ – it does not
               support integrity checks or multiple filters.

        * 
          ‘FORMAT_RAW’: A raw data stream, not using any container format.

               This format specifier does not support integrity checks,
               and requires that you always specify a custom filter
               chain (for both compression and decompression).
               Additionally, data compressed in this manner cannot be
               decompressed using ‘FORMAT_AUTO’ (see *note
               LZMADecompressor: 170d.).

     The `check' argument specifies the type of integrity check to
     include in the compressed data.  This check is used when
     decompressing, to ensure that the data has not been corrupted.
     Possible values are:

        * ‘CHECK_NONE’: No integrity check.  This is the default (and
          the only acceptable value) for ‘FORMAT_ALONE’ and
          ‘FORMAT_RAW’.

        * ‘CHECK_CRC32’: 32-bit Cyclic Redundancy Check.

        * ‘CHECK_CRC64’: 64-bit Cyclic Redundancy Check.  This is the
          default for ‘FORMAT_XZ’.

        * ‘CHECK_SHA256’: 256-bit Secure Hash Algorithm.

     If the specified check is not supported, an *note LZMAError: 170a.
     is raised.

     The compression settings can be specified either as a preset
     compression level (with the `preset' argument), or in detail as a
     custom filter chain (with the `filters' argument).

     The `preset' argument (if provided) should be an integer between
     ‘0’ and ‘9’ (inclusive), optionally OR-ed with the constant
     ‘PRESET_EXTREME’.  If neither `preset' nor `filters' are given, the
     default behavior is to use ‘PRESET_DEFAULT’ (preset level ‘6’).
     Higher presets produce smaller output, but make the compression
     process slower.

          Note: In addition to being more CPU-intensive, compression
          with higher presets also requires much more memory (and
          produces output that needs more memory to decompress).  With
          preset ‘9’ for example, the overhead for an *note
          LZMACompressor: 170e. object can be as high as 800 MiB. For
          this reason, it is generally best to stick with the default
          preset.

     The `filters' argument (if provided) should be a filter chain
     specifier.  See *note Specifying custom filter chains: 1712. for
     details.

      -- Method: compress (data)

          Compress `data' (a *note bytes: 1db. object), returning a
          *note bytes: 1db. object containing compressed data for at
          least part of the input.  Some of `data' may be buffered
          internally, for use in later calls to *note compress(): 1711.
          and *note flush(): 1714.  The returned data should be
          concatenated with the output of any previous calls to *note
          compress(): 1711.

      -- Method: flush ()

          Finish the compression process, returning a *note bytes: 1db.
          object containing any data stored in the compressor’s internal
          buffers.

          The compressor cannot be used after this method has been
          called.

 -- Class: lzma.LZMADecompressor (format=FORMAT_AUTO, memlimit=None,
          filters=None)

     Create a decompressor object, which can be used to decompress data
     incrementally.

     For a more convenient way of decompressing an entire compressed
     stream at once, see *note decompress(): 1715.

     The `format' argument specifies the container format that should be
     used.  The default is ‘FORMAT_AUTO’, which can decompress both
     ‘.xz’ and ‘.lzma’ files.  Other possible values are ‘FORMAT_XZ’,
     ‘FORMAT_ALONE’, and ‘FORMAT_RAW’.

     The `memlimit' argument specifies a limit (in bytes) on the amount
     of memory that the decompressor can use.  When this argument is
     used, decompression will fail with an *note LZMAError: 170a. if it
     is not possible to decompress the input within the given memory
     limit.

     The `filters' argument specifies the filter chain that was used to
     create the stream being decompressed.  This argument is required if
     `format' is ‘FORMAT_RAW’, but should not be used for other formats.
     See *note Specifying custom filter chains: 1712. for more
     information about filter chains.

          Note: This class does not transparently handle inputs
          containing multiple compressed streams, unlike *note
          decompress(): 1715. and *note LZMAFile: 53a.  To decompress a
          multi-stream input with *note LZMADecompressor: 170d, you must
          create a new decompressor for each stream.

      -- Method: decompress (data, max_length=-1)

          Decompress `data' (a *note bytes-like object: 36b.), returning
          uncompressed data as bytes.  Some of `data' may be buffered
          internally, for use in later calls to *note decompress():
          1715.  The returned data should be concatenated with the
          output of any previous calls to *note decompress(): 1715.

          If `max_length' is nonnegative, returns at most `max_length'
          bytes of decompressed data.  If this limit is reached and
          further output can be produced, the *note needs_input: 1716.
          attribute will be set to ‘False’.  In this case, the next call
          to *note decompress(): 2ce. may provide `data' as ‘b''’ to
          obtain more of the output.

          If all of the input data was decompressed and returned (either
          because this was less than `max_length' bytes, or because
          `max_length' was negative), the *note needs_input: 1716.
          attribute will be set to ‘True’.

          Attempting to decompress data after the end of stream is
          reached raises an ‘EOFError’.  Any data found after the end of
          the stream is ignored and saved in the *note unused_data:
          1717. attribute.

          Changed in version 3.5: Added the `max_length' parameter.

      -- Attribute: check

          The ID of the integrity check used by the input stream.  This
          may be ‘CHECK_UNKNOWN’ until enough of the input has been
          decoded to determine what integrity check it uses.

      -- Attribute: eof

          ‘True’ if the end-of-stream marker has been reached.

      -- Attribute: unused_data

          Data found after the end of the compressed stream.

          Before the end of the stream is reached, this will be ‘b""’.

      -- Attribute: needs_input

          ‘False’ if the *note decompress(): 2ce. method can provide
          more decompressed data before requiring new uncompressed
          input.

          New in version 3.5.

 -- Function: lzma.compress (data, format=FORMAT_XZ, check=-1,
          preset=None, filters=None)

     Compress `data' (a *note bytes: 1db. object), returning the
     compressed data as a *note bytes: 1db. object.

     See *note LZMACompressor: 170e. above for a description of the
     `format', `check', `preset' and `filters' arguments.

 -- Function: lzma.decompress (data, format=FORMAT_AUTO, memlimit=None,
          filters=None)

     Decompress `data' (a *note bytes: 1db. object), returning the
     uncompressed data as a *note bytes: 1db. object.

     If `data' is the concatenation of multiple distinct compressed
     streams, decompress all of these streams, and return the
     concatenation of the results.

     See *note LZMADecompressor: 170d. above for a description of the
     `format', `memlimit' and `filters' arguments.


File: python.info,  Node: Miscellaneous<2>,  Next: Specifying custom filter chains,  Prev: Compressing and decompressing data in memory,  Up: lzma --- Compression using the LZMA algorithm

5.13.4.3 Miscellaneous
......................

 -- Function: lzma.is_check_supported (check)

     Returns true if the given integrity check is supported on this
     system.

     ‘CHECK_NONE’ and ‘CHECK_CRC32’ are always supported.  ‘CHECK_CRC64’
     and ‘CHECK_SHA256’ may be unavailable if you are using a version of
     ‘liblzma’ that was compiled with a limited feature set.


File: python.info,  Node: Specifying custom filter chains,  Next: Examples<5>,  Prev: Miscellaneous<2>,  Up: lzma --- Compression using the LZMA algorithm

5.13.4.4 Specifying custom filter chains
........................................

A filter chain specifier is a sequence of dictionaries, where each
dictionary contains the ID and options for a single filter.  Each
dictionary must contain the key ‘"id"’, and may contain additional keys
to specify filter-dependent options.  Valid filter IDs are as follows:

   * 
     Compression filters:

             * ‘FILTER_LZMA1’ (for use with ‘FORMAT_ALONE’)

             * ‘FILTER_LZMA2’ (for use with ‘FORMAT_XZ’ and
               ‘FORMAT_RAW’)

   * 
     Delta filter:

             * ‘FILTER_DELTA’

   * 
     Branch-Call-Jump (BCJ) filters:

             * ‘FILTER_X86’

             * ‘FILTER_IA64’

             * ‘FILTER_ARM’

             * ‘FILTER_ARMTHUMB’

             * ‘FILTER_POWERPC’

             * ‘FILTER_SPARC’

A filter chain can consist of up to 4 filters, and cannot be empty.  The
last filter in the chain must be a compression filter, and any other
filters must be delta or BCJ filters.

Compression filters support the following options (specified as
additional entries in the dictionary representing the filter):

        * ‘preset’: A compression preset to use as a source of default
          values for options that are not specified explicitly.

        * ‘dict_size’: Dictionary size in bytes.  This should be between
          4 KiB and 1.5 GiB (inclusive).

        * ‘lc’: Number of literal context bits.

        * ‘lp’: Number of literal position bits.  The sum ‘lc + lp’ must
          be at most 4.

        * ‘pb’: Number of position bits; must be at most 4.

        * ‘mode’: ‘MODE_FAST’ or ‘MODE_NORMAL’.

        * ‘nice_len’: What should be considered a "nice length" for a
          match.  This should be 273 or less.

        * ‘mf’: What match finder to use – ‘MF_HC3’, ‘MF_HC4’, ‘MF_BT2’,
          ‘MF_BT3’, or ‘MF_BT4’.

        * ‘depth’: Maximum search depth used by match finder.  0
          (default) means to select automatically based on other filter
          options.

The delta filter stores the differences between bytes, producing more
repetitive input for the compressor in certain circumstances.  It only
supports a single The delta filter supports only one option, ‘dist’.
This indicates the distance between bytes to be subtracted.  The default
is 1, i.e.  take the differences between adjacent bytes.

The BCJ filters are intended to be applied to machine code.  They
convert relative branches, calls and jumps in the code to use absolute
addressing, with the aim of increasing the redundancy that can be
exploited by the compressor.  These filters support one option,
‘start_offset’.  This specifies the address that should be mapped to the
beginning of the input data.  The default is 0.


File: python.info,  Node: Examples<5>,  Prev: Specifying custom filter chains,  Up: lzma --- Compression using the LZMA algorithm

5.13.4.5 Examples
.................

Reading in a compressed file:

     import lzma
     with lzma.open("file.xz") as f:
         file_content = f.read()

Creating a compressed file:

     import lzma
     data = b"Insert Data Here"
     with lzma.open("file.xz", "w") as f:
         f.write(data)

Compressing data in memory:

     import lzma
     data_in = b"Insert Data Here"
     data_out = lzma.compress(data_in)

Incremental compression:

     import lzma
     lzc = lzma.LZMACompressor()
     out1 = lzc.compress(b"Some data\n")
     out2 = lzc.compress(b"Another piece of data\n")
     out3 = lzc.compress(b"Even more data\n")
     out4 = lzc.flush()
     # Concatenate all the partial results:
     result = b"".join([out1, out2, out3, out4])

Writing compressed data to an already-open file:

     import lzma
     with open("file.xz", "wb") as f:
         f.write(b"This data will not be compressed\n")
         with lzma.open(f, "w") as lzf:
             lzf.write(b"This *will* be compressed\n")
         f.write(b"Not compressed\n")

Creating a compressed file using a custom filter chain:

     import lzma
     my_filters = [
         {"id": lzma.FILTER_DELTA, "dist": 5},
         {"id": lzma.FILTER_LZMA2, "preset": 7 | lzma.PRESET_EXTREME},
     ]
     with lzma.open("file.xz", "w", filters=my_filters) as f:
         f.write(b"blah blah blah")


File: python.info,  Node: zipfile --- Work with ZIP archives,  Next: tarfile --- Read and write tar archive files,  Prev: lzma --- Compression using the LZMA algorithm,  Up: Data Compression and Archiving

5.13.5 ‘zipfile’ — Work with ZIP archives
-----------------------------------------

`Source code:' Lib/zipfile.py(1)

__________________________________________________________________

The ZIP file format is a common archive and compression standard.  This
module provides tools to create, read, write, append, and list a ZIP
file.  Any advanced use of this module will require an understanding of
the format, as defined in PKZIP Application Note(2).

This module does not currently handle multi-disk ZIP files.  It can
handle ZIP files that use the ZIP64 extensions (that is ZIP files that
are more than 4 GiB in size).  It supports decryption of encrypted files
in ZIP archives, but it currently cannot create an encrypted file.
Decryption is extremely slow as it is implemented in native Python
rather than C.

The module defines the following items:

 -- Exception: zipfile.BadZipFile

     The error raised for bad ZIP files.

     New in version 3.2.

 -- Exception: zipfile.BadZipfile

     Alias of *note BadZipFile: 1720, for compatibility with older
     Python versions.

     Deprecated since version 3.2.

 -- Exception: zipfile.LargeZipFile

     The error raised when a ZIP file would require ZIP64 functionality
     but that has not been enabled.

 -- Class: zipfile.ZipFile

     The class for reading and writing ZIP files.  See section *note
     ZipFile Objects: 1723. for constructor details.

 -- Class: zipfile.PyZipFile

     Class for creating ZIP archives containing Python libraries.

 -- Class: zipfile.ZipInfo (filename='NoName', date_time=(1980, 1, 1, 0,
          0, 0))

     Class used to represent information about a member of an archive.
     Instances of this class are returned by the *note getinfo(): 1724.
     and *note infolist(): 1725. methods of *note ZipFile: 521. objects.
     Most users of the *note zipfile: 13f. module will not need to
     create these, but only use those created by this module.
     `filename' should be the full name of the archive member, and
     `date_time' should be a tuple containing six fields which describe
     the time of the last modification to the file; the fields are
     described in section *note ZipInfo Objects: 1726.

 -- Function: zipfile.is_zipfile (filename)

     Returns ‘True’ if `filename' is a valid ZIP file based on its magic
     number, otherwise returns ‘False’.  `filename' may be a file or
     file-like object too.

     Changed in version 3.1: Support for file and file-like objects.

 -- Data: zipfile.ZIP_STORED

     The numeric constant for an uncompressed archive member.

 -- Data: zipfile.ZIP_DEFLATED

     The numeric constant for the usual ZIP compression method.  This
     requires the *note zlib: 141. module.

 -- Data: zipfile.ZIP_BZIP2

     The numeric constant for the BZIP2 compression method.  This
     requires the *note bz2: 14. module.

     New in version 3.3.

 -- Data: zipfile.ZIP_LZMA

     The numeric constant for the LZMA compression method.  This
     requires the *note lzma: ab. module.

     New in version 3.3.

          Note: The ZIP file format specification has included support
          for bzip2 compression since 2001, and for LZMA compression
          since 2006.  However, some tools (including older Python
          releases) do not support these compression methods, and may
          either refuse to process the ZIP file altogether, or fail to
          extract individual files.

See also
........

PKZIP Application Note(3)

     Documentation on the ZIP file format by Phil Katz, the creator of
     the format and algorithms used.

Info-ZIP Home Page(4)

     Information about the Info-ZIP project’s ZIP archive programs and
     development libraries.

* Menu:

* ZipFile Objects:: 
* PyZipFile Objects:: 
* ZipInfo Objects:: 

   ---------- Footnotes ----------

   (1) https://hg.python.org/cpython/file/default/Lib/zipfile.py

   (2) https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT

   (3) https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT

   (4) http://www.info-zip.org/


File: python.info,  Node: ZipFile Objects,  Next: PyZipFile Objects,  Up: zipfile --- Work with ZIP archives

5.13.5.1 ZipFile Objects
........................

 -- Class: zipfile.ZipFile (file, mode='r', compression=ZIP_STORED,
          allowZip64=True)

     Open a ZIP file, where `file' can be either a path to a file (a
     string) or a file-like object.  The `mode' parameter should be
     ‘'r'’ to read an existing file, ‘'w'’ to truncate and write a new
     file, ‘'a'’ to append to an existing file, or ‘'x'’ to exclusively
     create and write a new file.  If `mode' is ‘'x'’ and `file' refers
     to an existing file, a *note FileExistsError: 56f. will be raised.
     If `mode' is ‘'a'’ and `file' refers to an existing ZIP file, then
     additional files are added to it.  If `file' does not refer to a
     ZIP file, then a new ZIP archive is appended to the file.  This is
     meant for adding a ZIP archive to another file (such as
     ‘python.exe’).  If `mode' is ‘a’ and the file does not exist at
     all, it is created.  If `mode' is ‘r’ or ‘a’, the file should be
     seekable.  `compression' is the ZIP compression method to use when
     writing the archive, and should be *note ZIP_STORED: 1727, *note
     ZIP_DEFLATED: 1728, *note ZIP_BZIP2: 1729. or *note ZIP_LZMA:
     172a.; unrecognized values will cause *note RuntimeError: 193. to
     be raised.  If *note ZIP_DEFLATED: 1728, *note ZIP_BZIP2: 1729. or
     *note ZIP_LZMA: 172a. is specified but the corresponding module
     (*note zlib: 141, *note bz2: 14. or *note lzma: ab.) is not
     available, *note RuntimeError: 193. is also raised.  The default is
     *note ZIP_STORED: 1727.  If `allowZip64' is ‘True’ (the default)
     zipfile will create ZIP files that use the ZIP64 extensions when
     the zipfile is larger than 2 GiB. If it is false *note zipfile:
     13f. will raise an exception when the ZIP file would require ZIP64
     extensions.

     If the file is created with mode ‘'w'’, ‘'x'’ or ‘'a'’ and then
     *note closed: 172c. without adding any files to the archive, the
     appropriate ZIP structures for an empty archive will be written to
     the file.

     ZipFile is also a context manager and therefore supports the *note
     with: 29d. statement.  In the example, `myzip' is closed after the
     *note with: 29d. statement’s suite is finished—even if an exception
     occurs:

          with ZipFile('spam.zip', 'w') as myzip:
              myzip.write('eggs.txt')

     New in version 3.2: Added the ability to use *note ZipFile: 521. as
     a context manager.

     Changed in version 3.3: Added support for *note bzip2: 14. and
     *note lzma: ab. compression.

     Changed in version 3.4: ZIP64 extensions are enabled by default.

     Changed in version 3.5: Added support for writing to unseekable
     streams.  Added support for the ‘'x'’ mode.

 -- Method: ZipFile.close ()

     Close the archive file.  You must call *note close(): 172c. before
     exiting your program or essential records will not be written.

 -- Method: ZipFile.getinfo (name)

     Return a *note ZipInfo: 17b. object with information about the
     archive member `name'.  Calling *note getinfo(): 1724. for a name
     not currently contained in the archive will raise a *note KeyError:
     1a7.

 -- Method: ZipFile.infolist ()

     Return a list containing a *note ZipInfo: 17b. object for each
     member of the archive.  The objects are in the same order as their
     entries in the actual ZIP file on disk if an existing archive was
     opened.

 -- Method: ZipFile.namelist ()

     Return a list of archive members by name.

 -- Method: ZipFile.open (name, mode='r', pwd=None)

     Extract a member from the archive as a file-like object
     (ZipExtFile).  `name' is the name of the file in the archive, or a
     *note ZipInfo: 17b. object.  The `mode' parameter, if included,
     must be one of the following: ‘'r'’ (the default), ‘'U'’, or
     ‘'rU'’.  Choosing ‘'U'’ or ‘'rU'’ will enable *note universal
     newlines: 994. support in the read-only object.  `pwd' is the
     password used for encrypted files.  Calling *note open(): 369. on a
     closed ZipFile will raise a *note RuntimeError: 193.

     *note open(): 369. is also a context manager and therefore supports
     the *note with: 29d. statement:

          with ZipFile('spam.zip') as myzip:
              with myzip.open('eggs.txt') as myfile:
                  print(myfile.read())

          Note: The file-like object is read-only and provides the
          following methods: *note read(): 16f8, *note readline(): 10b8,
          *note readlines(): 113f, *note __iter__(): 99b, *note
          __next__(): 8cf.

          Note: Objects returned by *note open(): 369. can operate
          independently of the ZipFile.

          Note: The *note open(): 369, *note read(): 92f. and *note
          extract(): 172e. methods can take a filename or a *note
          ZipInfo: 17b. object.  You will appreciate this when trying to
          read a ZIP file that contains members with duplicate names.

     Deprecated since version 3.4, will be removed in version 3.6: The
     ‘'U'’ or ‘'rU'’ mode.  Use *note io.TextIOWrapper: 557. for reading
     compressed text files in *note universal newlines: 994. mode.

 -- Method: ZipFile.extract (member, path=None, pwd=None)

     Extract a member from the archive to the current working directory;
     `member' must be its full name or a *note ZipInfo: 17b. object).
     Its file information is extracted as accurately as possible.
     `path' specifies a different directory to extract to.  `member' can
     be a filename or a *note ZipInfo: 17b. object.  `pwd' is the
     password used for encrypted files.

     Returns the normalized path created (a directory or new file).

          Note: If a member filename is an absolute path, a drive/UNC
          sharepoint and leading (back)slashes will be stripped, e.g.:
          ‘///foo/bar’ becomes ‘foo/bar’ on Unix, and ‘C:\foo\bar’
          becomes ‘foo\bar’ on Windows.  And all ‘".."’ components in a
          member filename will be removed, e.g.: ‘../../foo../../ba..r’
          becomes ‘foo../ba..r’.  On Windows illegal characters (‘:’,
          ‘<’, ‘>’, ‘|’, ‘"’, ‘?’, and ‘*’) replaced by underscore
          (‘_’).

 -- Method: ZipFile.extractall (path=None, members=None, pwd=None)

     Extract all members from the archive to the current working
     directory.  `path' specifies a different directory to extract to.
     `members' is optional and must be a subset of the list returned by
     *note namelist(): 172d.  `pwd' is the password used for encrypted
     files.

          Warning: Never extract archives from untrusted sources without
          prior inspection.  It is possible that files are created
          outside of `path', e.g.  members that have absolute filenames
          starting with ‘"/"’ or filenames with two dots ‘".."’.  This
          module attempts to prevent that.  See *note extract(): 172e.
          note.

 -- Method: ZipFile.printdir ()

     Print a table of contents for the archive to ‘sys.stdout’.

 -- Method: ZipFile.setpassword (pwd)

     Set `pwd' as default password to extract encrypted files.

 -- Method: ZipFile.read (name, pwd=None)

     Return the bytes of the file `name' in the archive.  `name' is the
     name of the file in the archive, or a *note ZipInfo: 17b. object.
     The archive must be open for read or append.  `pwd' is the password
     used for encrypted files and, if specified, it will override the
     default password set with *note setpassword(): 1731.  Calling *note
     read(): 92f. on a closed ZipFile will raise a *note RuntimeError:
     193.  Calling *note read(): 92f. on a ZipFile that uses a
     compression method other than *note ZIP_STORED: 1727, *note
     ZIP_DEFLATED: 1728, *note ZIP_BZIP2: 1729. or *note ZIP_LZMA: 172a.
     will raise a *note NotImplementedError: 569.  An error will also be
     raised if the corresponding compression module is not available.

 -- Method: ZipFile.testzip ()

     Read all the files in the archive and check their CRC’s and file
     headers.  Return the name of the first bad file, or else return
     ‘None’.  Calling *note testzip(): 1732. on a closed ZipFile will
     raise a *note RuntimeError: 193.

 -- Method: ZipFile.write (filename, arcname=None, compress_type=None)

     Write the file named `filename' to the archive, giving it the
     archive name `arcname' (by default, this will be the same as
     `filename', but without a drive letter and with leading path
     separators removed).  If given, `compress_type' overrides the value
     given for the `compression' parameter to the constructor for the
     new entry.  The archive must be open with mode ‘'w'’, ‘'x'’ or
     ‘'a'’ – calling *note write(): 1733. on a ZipFile created with mode
     ‘'r'’ will raise a *note RuntimeError: 193.  Calling *note write():
     1733. on a closed ZipFile will raise a *note RuntimeError: 193.

          Note: There is no official file name encoding for ZIP files.
          If you have unicode file names, you must convert them to byte
          strings in your desired encoding before passing them to *note
          write(): 1733.  WinZip interprets all file names as encoded in
          CP437, also known as DOS Latin.

          Note: Archive names should be relative to the archive root,
          that is, they should not start with a path separator.

          Note: If ‘arcname’ (or ‘filename’, if ‘arcname’ is not given)
          contains a null byte, the name of the file in the archive will
          be truncated at the null byte.

 -- Method: ZipFile.writestr (zinfo_or_arcname, bytes[, compress_type])

     Write the string `bytes' to the archive; `zinfo_or_arcname' is
     either the file name it will be given in the archive, or a *note
     ZipInfo: 17b. instance.  If it’s an instance, at least the
     filename, date, and time must be given.  If it’s a name, the date
     and time is set to the current date and time.  The archive must be
     opened with mode ‘'w'’, ‘'x'’ or ‘'a'’ – calling *note writestr():
     931. on a ZipFile created with mode ‘'r'’ will raise a *note
     RuntimeError: 193.  Calling *note writestr(): 931. on a closed
     ZipFile will raise a *note RuntimeError: 193.

     If given, `compress_type' overrides the value given for the
     `compression' parameter to the constructor for the new entry, or in
     the `zinfo_or_arcname' (if that is a *note ZipInfo: 17b. instance).

          Note: When passing a *note ZipInfo: 17b. instance as the
          `zinfo_or_arcname' parameter, the compression method used will
          be that specified in the `compress_type' member of the given
          *note ZipInfo: 17b. instance.  By default, the *note ZipInfo:
          17b. constructor sets this member to *note ZIP_STORED: 1727.

     Changed in version 3.2: The `compress_type' argument.

The following data attributes are also available:

 -- Attribute: ZipFile.debug

     The level of debug output to use.  This may be set from ‘0’ (the
     default, no output) to ‘3’ (the most output).  Debugging
     information is written to ‘sys.stdout’.

 -- Attribute: ZipFile.comment

     The comment text associated with the ZIP file.  If assigning a
     comment to a *note ZipFile: 521. instance created with mode ‘'w'’,
     ‘'x'’ or ‘'a'’, this should be a string no longer than 65535 bytes.
     Comments longer than this will be truncated in the written archive
     when *note close(): 172c. is called.

